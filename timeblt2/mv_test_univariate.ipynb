{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d729fdbd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d73164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # must be BEFORE torch/TF import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from bytelatent.model.blt import ByteLatentTransformerArgs, ByteLatentTransformer\n",
    "from chronos import MeanScaleUniformBins, ChronosConfig\n",
    "from utils.train_utils import *\n",
    "# from TLLM_data_provider.data_factory import data_provider\n",
    "from data_provider.data_factory import data_provider\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "torch.cuda.set_device(0)   # 0 here means “the first visible GPU”, i.e. physical #3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from bytelatent.tokenizers.constants import PAD_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe78d9",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b7c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args\n",
    "vocab_size = 4096\n",
    "batch_size = 64\n",
    "seq_len = 96\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-2\n",
    "epochs = 50\n",
    "grad_accumulation_steps = 1\n",
    "clip_grad = 1.0\n",
    "seed = 42\n",
    "warmup_steps = 0\n",
    "min_lr_factor = 0.1\n",
    "decay_lr = True\n",
    "compile = True\n",
    "output_dir = \"output\"\n",
    "save_every = 5\n",
    "compile = False\n",
    "dataset_name = 'ETTh2'\n",
    "features = 'S'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6e5f2",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e949019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8544\n",
      "val 2880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8544, 2880)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': dataset_name,\n",
    "    'data' : dataset_name,\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': f'{dataset_name}.csv',\n",
    "    'features': features,\n",
    "    'seq_len': seq_len,\n",
    "    'label_len': seq_len - 1,\n",
    "    'pred_len': 1\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=batch_size,\n",
    "    freq='h' if 'h' in dataset_name else 'm',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=True,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "train_dataset, train_loader = data_provider(args, config, flag='train')\n",
    "validate_dataset, validate_loader = data_provider(args, config, flag='val')\n",
    "# test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "len(train_dataset), len(validate_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d471e",
   "metadata": {},
   "source": [
    "## Data Loader Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35869fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    batch_x_mark = batch_x_mark.to(device)\n",
    "    batch_y_mark = batch_y_mark.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "x.shape, y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_all_channels(x, sample_idx=0, channel_names=None, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot all 7 channels for a single sample in the same plot\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor/array of shape [batch_size, seq_len, n_channels]\n",
    "        sample_idx: Which sample to plot (default: 0)\n",
    "        channel_names: List of channel names (optional)\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract single sample: [seq_len, n_channels]\n",
    "    sample_data = x[sample_idx]  # Shape: [96, 7]\n",
    "    seq_len, n_channels = sample_data.shape\n",
    "    \n",
    "    # Default channel names if not provided\n",
    "    if channel_names is None:\n",
    "        channel_names = [f'Channel {i+1}' for i in range(n_channels)]\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_channels))\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(3, 2, figure=fig, height_ratios=[2, 1, 1], hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Main plot: All channels together\n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    time_steps = np.arange(seq_len)\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        channel_data = sample_data[:, i]\n",
    "        ax_main.plot(time_steps, channel_data, \n",
    "                    color=colors[i], \n",
    "                    label=channel_names[i],\n",
    "                    linewidth=1.5,\n",
    "                    alpha=0.8)\n",
    "    \n",
    "    ax_main.set_title(f'All Channels - Sample {sample_idx}', fontsize=14, fontweight='bold')\n",
    "    ax_main.set_xlabel('Time Steps')\n",
    "    ax_main.set_ylabel('Values')\n",
    "    ax_main.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Statistics subplot\n",
    "    ax_stats = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Calculate statistics for each channel\n",
    "    stats_data = []\n",
    "    for i in range(n_channels):\n",
    "        channel_data = sample_data[:, i]\n",
    "        stats_data.append([\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data),\n",
    "            np.min(channel_data),\n",
    "            np.max(channel_data)\n",
    "        ])\n",
    "    \n",
    "    stats_array = np.array(stats_data)\n",
    "    stat_names = ['Mean', 'Std', 'Min', 'Max']\n",
    "    \n",
    "    # Heatmap of statistics\n",
    "    im = ax_stats.imshow(stats_array.T, cmap='RdYlBu_r', aspect='auto')\n",
    "    ax_stats.set_xticks(range(n_channels))\n",
    "    ax_stats.set_xticklabels([f'Ch{i+1}' for i in range(n_channels)])\n",
    "    ax_stats.set_yticks(range(len(stat_names)))\n",
    "    ax_stats.set_yticklabels(stat_names)\n",
    "    ax_stats.set_title('Channel Statistics')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(n_channels):\n",
    "        for j in range(len(stat_names)):\n",
    "            text = ax_stats.text(i, j, f'{stats_array[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    ax_corr = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(sample_data.T)\n",
    "    \n",
    "    im_corr = ax_corr.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "    ax_corr.set_xticks(range(n_channels))\n",
    "    ax_corr.set_xticklabels([f'Ch{i+1}' for i in range(n_channels)])\n",
    "    ax_corr.set_yticks(range(n_channels))\n",
    "    ax_corr.set_yticklabels([f'Ch{i+1}' for i in range(n_channels)])\n",
    "    ax_corr.set_title('Channel Correlations')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            text = ax_corr.text(j, i, f'{corr_matrix[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", \n",
    "                               color=\"white\" if abs(corr_matrix[i, j]) > 0.5 else \"black\", \n",
    "                               fontsize=8)\n",
    "    \n",
    "    # Individual channel plots (smaller)\n",
    "    ax_individual = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Create small multiples for each channel\n",
    "    for i in range(n_channels):\n",
    "        offset = i * (np.max(sample_data) - np.min(sample_data)) * 1.2\n",
    "        channel_data = sample_data[:, i] + offset\n",
    "        ax_individual.plot(time_steps, channel_data, \n",
    "                          color=colors[i], \n",
    "                          label=channel_names[i],\n",
    "                          linewidth=1)\n",
    "        \n",
    "        # Add channel label\n",
    "        ax_individual.text(-2, offset + np.mean(sample_data[:, i]), \n",
    "                          f'Ch{i+1}', \n",
    "                          va='center', ha='right', \n",
    "                          color=colors[i], fontweight='bold')\n",
    "    \n",
    "    ax_individual.set_title('Individual Channels (Stacked View)')\n",
    "    ax_individual.set_xlabel('Time Steps')\n",
    "    ax_individual.set_ylabel('Values (with offset)')\n",
    "    ax_individual.grid(True, alpha=0.3)\n",
    "    ax_individual.set_yticks([])  # Remove y-ticks since they're offset\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_channel_distributions(x, sample_idx=0, channel_names=None, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot distribution analysis for all channels\n",
    "    \"\"\"\n",
    "    sample_data = x[sample_idx]\n",
    "    seq_len, n_channels = sample_data.shape\n",
    "    \n",
    "    if channel_names is None:\n",
    "        channel_names = [f'Channel {i+1}' for i in range(n_channels)]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_channels))\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        ax = axes[i]\n",
    "        channel_data = sample_data[:, i]\n",
    "        \n",
    "        # Histogram\n",
    "        ax.hist(channel_data, bins=20, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "        ax.set_title(f'{channel_names[i]}')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Add statistics text\n",
    "        mean_val = np.mean(channel_data)\n",
    "        std_val = np.std(channel_data)\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_val:.2f}')\n",
    "        ax.axvline(mean_val + std_val, color='orange', linestyle=':', alpha=0.6, label=f'+1σ: {mean_val + std_val:.2f}')\n",
    "        ax.axvline(mean_val - std_val, color='orange', linestyle=':', alpha=0.6, label=f'-1σ: {mean_val - std_val:.2f}')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if n_channels < len(axes):\n",
    "        fig.delaxes(axes[-1])\n",
    "    \n",
    "    plt.suptitle(f'Channel Value Distributions - Sample {sample_idx}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_channel_independence(x, sample_idx=0, channel_names=None):\n",
    "    \"\"\"\n",
    "    Analyze channel independence for univariate assumption validation\n",
    "    \"\"\"\n",
    "    sample_data = x[sample_idx]\n",
    "    seq_len, n_channels = sample_data.shape\n",
    "    \n",
    "    if channel_names is None:\n",
    "        channel_names = [f'Channel {i+1}' for i in range(n_channels)]\n",
    "    \n",
    "    print(f\"Channel Independence Analysis for Sample {sample_idx}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Correlation analysis\n",
    "    corr_matrix = np.corrcoef(sample_data.T)\n",
    "    \n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(\"-\" * 30)\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i != j:\n",
    "                corr_val = corr_matrix[i, j]\n",
    "                print(f\"{channel_names[i]} vs {channel_names[j]}: {corr_val:.3f}\")\n",
    "    \n",
    "    # High correlation pairs\n",
    "    high_corr_pairs = []\n",
    "    threshold = 0.7\n",
    "    for i in range(n_channels):\n",
    "        for j in range(i+1, n_channels):\n",
    "            if abs(corr_matrix[i, j]) > threshold:\n",
    "                high_corr_pairs.append((i, j, corr_matrix[i, j]))\n",
    "    \n",
    "    print(f\"\\nHigh Correlation Pairs (|r| > {threshold}):\")\n",
    "    print(\"-\" * 40)\n",
    "    if high_corr_pairs:\n",
    "        for i, j, corr_val in high_corr_pairs:\n",
    "            print(f\"{channel_names[i]} - {channel_names[j]}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"No high correlation pairs found.\")\n",
    "    \n",
    "    # Statistics comparison\n",
    "    print(\"\\nChannel Statistics:\")\n",
    "    print(\"-\" * 20)\n",
    "    for i in range(n_channels):\n",
    "        channel_data = sample_data[:, i]\n",
    "        print(f\"{channel_names[i]}:\")\n",
    "        print(f\"  Mean: {np.mean(channel_data):.3f}\")\n",
    "        print(f\"  Std:  {np.std(channel_data):.3f}\")\n",
    "        print(f\"  Range: [{np.min(channel_data):.3f}, {np.max(channel_data):.3f}]\")\n",
    "        print()\n",
    "    \n",
    "    return corr_matrix, high_corr_pairs\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example of how to use the plotting functions\n",
    "    \"\"\"\n",
    "    # Simulated data - replace with your actual x[0, :, i] data\n",
    "    np.random.seed(42)\n",
    "    batch_size, seq_len, n_channels = 32, 96, 7\n",
    "    \n",
    "    # Create synthetic data that mimics real time series\n",
    "    x = np.random.randn(batch_size, seq_len, n_channels)\n",
    "    \n",
    "    # Add some trend and seasonality to make it more realistic\n",
    "    for i in range(n_channels):\n",
    "        trend = np.linspace(0, i*0.5, seq_len)\n",
    "        seasonal = np.sin(2 * np.pi * np.arange(seq_len) / (20 + i*5)) * (0.5 + i*0.1)\n",
    "        x[:, :, i] += trend + seasonal\n",
    "    \n",
    "    # ETT dataset channel names (common for ETT datasets)\n",
    "    ett_channel_names = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "    \n",
    "    # Plot all channels\n",
    "    fig1 = plot_all_channels(x, sample_idx=0, channel_names=ett_channel_names)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig2 = plot_channel_distributions(x, sample_idx=0, channel_names=ett_channel_names)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze independence\n",
    "    corr_matrix, high_corr_pairs = analyze_channel_independence(x, sample_idx=0, channel_names=ett_channel_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(validate_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75796d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_ett_splits(seq_len=96):\n",
    "    \"\"\"\n",
    "    Analyze the difference between pretrain=True and pretrain=False splits\n",
    "    for ETT minute dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # ETT minute dataset constants\n",
    "    # 12 months = 12 * 30 * 24 * 4 = 34,560 time points (15-min intervals)\n",
    "    # 4 months = 4 * 30 * 24 * 4 = 11,520 time points  \n",
    "    # 8 months = 8 * 30 * 24 * 4 = 23,040 time points\n",
    "    \n",
    "    month_minutes = 30 * 24 * 4  # 30 days * 24 hours * 4 (15-min intervals)\n",
    "    \n",
    "    # Calculate boundaries\n",
    "    twelve_months = 12 * month_minutes  # 34,560\n",
    "    four_months = 4 * month_minutes     # 11,520  \n",
    "    eight_months = 8 * month_minutes    # 23,040\n",
    "    \n",
    "    print(\"ETT Minute Dataset Split Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total data points per month: {month_minutes:,}\")\n",
    "    print(f\"12 months: {twelve_months:,} points\")\n",
    "    print(f\"4 months: {four_months:,} points\") \n",
    "    print(f\"8 months: {eight_months:,} points\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print()\n",
    "    \n",
    "    # PRETRAIN = True splits\n",
    "    print(\"PRETRAIN = True (Self-supervised pretraining)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    pretrain_border1s = [\n",
    "        0,  # Train start\n",
    "        twelve_months + four_months - seq_len,  # Val start  \n",
    "        twelve_months + four_months - seq_len   # Test start (same as val)\n",
    "    ]\n",
    "    pretrain_border2s = [\n",
    "        twelve_months + four_months,  # Train end (16 months)\n",
    "        twelve_months + eight_months, # Val end (20 months) \n",
    "        twelve_months + eight_months  # Test end (20 months)\n",
    "    ]\n",
    "    \n",
    "    print(\"Train: [0 : 16 months]\")\n",
    "    print(f\"  Start: 0, End: {pretrain_border2s[0]:,}\")\n",
    "    print(f\"  Duration: {pretrain_border2s[0]:,} points ({pretrain_border2s[0]/month_minutes:.1f} months)\")\n",
    "    \n",
    "    print(\"Val: [16 months - seq_len : 20 months]\") \n",
    "    print(f\"  Start: {pretrain_border1s[1]:,}, End: {pretrain_border2s[1]:,}\")\n",
    "    print(f\"  Duration: {pretrain_border2s[1] - pretrain_border1s[1]:,} points ({(pretrain_border2s[1] - pretrain_border1s[1])/month_minutes:.1f} months)\")\n",
    "    \n",
    "    print(\"Test: [16 months - seq_len : 20 months] (same as val)\")\n",
    "    print(f\"  Start: {pretrain_border1s[2]:,}, End: {pretrain_border2s[2]:,}\")\n",
    "    print(f\"  Duration: {pretrain_border2s[2] - pretrain_border1s[2]:,} points ({(pretrain_border2s[2] - pretrain_border1s[2])/month_minutes:.1f} months)\")\n",
    "    print()\n",
    "    \n",
    "    # PRETRAIN = False splits  \n",
    "    print(\"PRETRAIN = False (Standard supervised learning)\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    normal_border1s = [\n",
    "        0,  # Train start\n",
    "        twelve_months - seq_len,  # Val start\n",
    "        twelve_months + four_months - seq_len  # Test start\n",
    "    ]\n",
    "    normal_border2s = [\n",
    "        twelve_months,  # Train end (12 months)\n",
    "        twelve_months + four_months,  # Val end (16 months)\n",
    "        twelve_months + eight_months  # Test end (20 months) \n",
    "    ]\n",
    "    \n",
    "    print(\"Train: [0 : 12 months]\")\n",
    "    print(f\"  Start: 0, End: {normal_border2s[0]:,}\")\n",
    "    print(f\"  Duration: {normal_border2s[0]:,} points ({normal_border2s[0]/month_minutes:.1f} months)\")\n",
    "    \n",
    "    print(\"Val: [12 months - seq_len : 16 months]\")\n",
    "    print(f\"  Start: {normal_border1s[1]:,}, End: {normal_border2s[1]:,}\")\n",
    "    print(f\"  Duration: {normal_border2s[1] - normal_border1s[1]:,} points ({(normal_border2s[1] - normal_border1s[1])/month_minutes:.1f} months)\")\n",
    "    \n",
    "    print(\"Test: [16 months - seq_len : 20 months]\")\n",
    "    print(f\"  Start: {normal_border1s[2]:,}, End: {normal_border2s[2]:,}\")\n",
    "    print(f\"  Duration: {normal_border2s[2] - normal_border1s[2]:,} points ({(normal_border2s[2] - normal_border1s[2])/month_minutes:.1f} months)\")\n",
    "    print()\n",
    "    \n",
    "    # Key differences\n",
    "    print(\"KEY DIFFERENCES:\")\n",
    "    print(\"-\" * 16)\n",
    "    train_diff = pretrain_border2s[0] - normal_border2s[0]\n",
    "    print(f\"• Training data: Pretrain uses {train_diff:,} MORE points ({train_diff/month_minutes:.1f} more months)\")\n",
    "    print(f\"• Val/Test overlap: Pretrain has OVERLAPPING val/test sets\")\n",
    "    print(f\"• Purpose: Pretrain optimized for self-supervised learning\")\n",
    "    print()\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n",
    "    \n",
    "    # Timeline in months\n",
    "    total_months = 20\n",
    "    months = np.arange(total_months + 1)\n",
    "    \n",
    "    # Pretrain splits\n",
    "    ax1.barh(0, 16, left=0, height=0.3, color='blue', alpha=0.7, label='Train (16 months)')\n",
    "    ax1.barh(0, 4, left=16-seq_len/month_minutes, height=0.15, color='orange', alpha=0.7, label='Val (4+ months)')  \n",
    "    ax1.barh(-0.15, 4, left=16-seq_len/month_minutes, height=0.15, color='red', alpha=0.7, label='Test (4+ months)')\n",
    "    ax1.set_xlim(0, 20)\n",
    "    ax1.set_ylim(-0.4, 0.4)\n",
    "    ax1.set_xlabel('Months')\n",
    "    ax1.set_title('PRETRAIN = True (Self-supervised pretraining)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Normal splits\n",
    "    ax2.barh(0, 12, left=0, height=0.3, color='blue', alpha=0.7, label='Train (12 months)')\n",
    "    ax2.barh(0, 4, left=12-seq_len/month_minutes, height=0.15, color='orange', alpha=0.7, label='Val (4+ months)')\n",
    "    ax2.barh(-0.15, 4, left=16-seq_len/month_minutes, height=0.15, color='red', alpha=0.7, label='Test (4+ months)')\n",
    "    ax2.set_xlim(0, 20)\n",
    "    ax2.set_ylim(-0.4, 0.4) \n",
    "    ax2.set_xlabel('Months')\n",
    "    ax2.set_title('PRETRAIN = False (Standard supervised learning)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'pretrain_splits': {\n",
    "            'train': (pretrain_border1s[0], pretrain_border2s[0]),\n",
    "            'val': (pretrain_border1s[1], pretrain_border2s[1]), \n",
    "            'test': (pretrain_border1s[2], pretrain_border2s[2])\n",
    "        },\n",
    "        'normal_splits': {\n",
    "            'train': (normal_border1s[0], normal_border2s[0]),\n",
    "            'val': (normal_border1s[1], normal_border2s[1]),\n",
    "            'test': (normal_border1s[2], normal_border2s[2])\n",
    "        }\n",
    "    }\n",
    "\n",
    "def why_pretrain_overlap():\n",
    "    \"\"\"\n",
    "    Explain why pretrain has overlapping val/test sets\n",
    "    \"\"\"\n",
    "    print(\"\\nWhy does PRETRAIN have overlapping val/test sets?\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\"\"\n",
    "    In SELF-SUPERVISED PRETRAINING:\n",
    "    \n",
    "    1. GOAL: Learn general time series representations\n",
    "       - Model learns to predict masked/future values\n",
    "       - No specific downstream task yet\n",
    "       \n",
    "    2. VAL/TEST OVERLAP is acceptable because:\n",
    "       - You're not doing task-specific evaluation\n",
    "       - Focus is on representation learning quality\n",
    "       - Real evaluation happens during fine-tuning\n",
    "       \n",
    "    3. MORE TRAINING DATA (16 vs 12 months):\n",
    "       - Self-supervised learning benefits from more data\n",
    "       - Better representations with longer sequences\n",
    "       - Can afford to use more data for pretraining\n",
    "    \n",
    "    In STANDARD SUPERVISED LEARNING:\n",
    "    \n",
    "    1. GOAL: Optimize for specific forecasting task\n",
    "       - Model trained end-to-end for prediction\n",
    "       - Need proper train/val/test separation\n",
    "       \n",
    "    2. STRICT SEPARATION required:\n",
    "       - Prevent data leakage\n",
    "       - Honest evaluation of generalization\n",
    "       - Standard ML best practices\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze with default sequence length\n",
    "    splits = analyze_ett_splits(seq_len=96)\n",
    "    \n",
    "    # Explain the rationale\n",
    "    why_pretrain_overlap()\n",
    "    \n",
    "    print(\"\\nSUMMARY:\")\n",
    "    print(\"• pretrain=True: More training data, overlapping val/test (for self-supervised pretraining)\")\n",
    "    print(\"• pretrain=False: Standard splits with proper separation (for supervised learning)\")\n",
    "    print(\"• Your ByteLatent training likely uses pretrain=True for representation learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataloader_iteration():\n",
    "    \"\"\"\n",
    "    Analyze how the ETT dataloader iterates through channels and time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example parameters\n",
    "    seq_len = 96\n",
    "    pred_len = 1\n",
    "    total_time_points = 1000\n",
    "    n_channels = 7\n",
    "    \n",
    "    # Calculate dataset parameters (from ETT code)\n",
    "    tot_len = total_time_points - seq_len - pred_len + 1  # Valid starting positions per channel\n",
    "    dataset_len = tot_len * n_channels  # Total dataset length\n",
    "    \n",
    "    print(\"ETT DataLoader Iteration Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Total time points: {total_time_points}\")\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "    print(f\"Valid positions per channel: {tot_len}\")\n",
    "    print(f\"Total dataset length: {dataset_len}\")\n",
    "    print()\n",
    "    \n",
    "    # Show how index mapping works\n",
    "    print(\"Index Mapping (from __getitem__):\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    sample_indices = [0, 1, 2, tot_len-1, tot_len, tot_len+1, dataset_len-1]\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        feat_id = idx // tot_len  # Which channel\n",
    "        s_begin = idx % tot_len   # Starting time position within that channel\n",
    "        s_end = s_begin + seq_len\n",
    "        \n",
    "        print(f\"Index {idx:4d}: Channel {feat_id}, Time [{s_begin:3d}:{s_end:3d}]\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Detailed channel-by-channel breakdown\n",
    "    print(\"Channel-by-Channel Breakdown:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for channel in range(n_channels):\n",
    "        start_idx = channel * tot_len\n",
    "        end_idx = (channel + 1) * tot_len - 1\n",
    "        \n",
    "        # First and last sequence for this channel\n",
    "        first_time_start = 0\n",
    "        first_time_end = seq_len\n",
    "        last_time_start = tot_len - 1\n",
    "        last_time_end = last_time_start + seq_len\n",
    "        \n",
    "        print(f\"Channel {channel}:\")\n",
    "        print(f\"  Dataset indices: [{start_idx:4d} : {end_idx:4d}]\")\n",
    "        print(f\"  First sequence:  Time [{first_time_start:3d} : {first_time_end:3d}]\")\n",
    "        print(f\"  Last sequence:   Time [{last_time_start:3d} : {last_time_end:3d}]\")\n",
    "        print()\n",
    "    \n",
    "    return tot_len, dataset_len\n",
    "\n",
    "def visualize_channel_iteration():\n",
    "    \"\"\"\n",
    "    Visualize how sequences are sampled from each channel\n",
    "    \"\"\"\n",
    "    seq_len = 20  # Smaller for visualization\n",
    "    total_time_points = 50\n",
    "    n_channels = 3  # Fewer channels for clarity\n",
    "    \n",
    "    tot_len = total_time_points - seq_len - 1 + 1  # 30 valid positions\n",
    "    \n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(12, 8))\n",
    "    \n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    \n",
    "    for channel in range(n_channels):\n",
    "        ax = axes[channel]\n",
    "        \n",
    "        # Draw the full time series for this channel\n",
    "        time_points = np.arange(total_time_points)\n",
    "        channel_data = np.sin(2 * np.pi * time_points / 10) + channel * 2\n",
    "        ax.plot(time_points, channel_data, color='lightgray', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # Show first few sequences from this channel\n",
    "        for seq_idx in range(0, min(5, tot_len), max(1, tot_len//10)):\n",
    "            start_time = seq_idx\n",
    "            end_time = start_time + seq_len\n",
    "            \n",
    "            # Highlight the sequence\n",
    "            seq_times = np.arange(start_time, end_time)\n",
    "            seq_values = channel_data[start_time:end_time]\n",
    "            \n",
    "            dataset_idx = channel * tot_len + seq_idx\n",
    "            \n",
    "            ax.plot(seq_times, seq_values, color=colors[channel], linewidth=3, alpha=0.7,\n",
    "                   label=f'Dataset idx {dataset_idx}' if seq_idx < 2 else '')\n",
    "            \n",
    "            # Mark start and end\n",
    "            ax.scatter([start_time, end_time-1], [seq_values[0], seq_values[-1]], \n",
    "                      color=colors[channel], s=50, zorder=5)\n",
    "        \n",
    "        ax.set_title(f'Channel {channel} - Sequences never cross channel boundaries')\n",
    "        ax.set_xlabel('Time Points')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('ETT DataLoader: Each sequence stays within ONE channel', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def demonstrate_no_mixing():\n",
    "    \"\"\"\n",
    "    Demonstrate that channels never mix\n",
    "    \"\"\"\n",
    "    print(\"PROOF: No Channel Mixing\")\n",
    "    print(\"=\" * 25)\n",
    "    print(\"\"\"\n",
    "    From the __getitem__ method:\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feat_id = index // self.tot_len     # Which channel (0, 1, 2, ..., 6)\n",
    "        s_begin = index % self.tot_len      # Time start within that channel\n",
    "        \n",
    "        s_end = s_begin + self.seq_len      # Time end within SAME channel\n",
    "        \n",
    "        # Key: Only extract from ONE channel\n",
    "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id + 1]\n",
    "                           ^^^^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^\n",
    "                           Time slice       Single channel only\n",
    "    \n",
    "    IMPORTANT BOUNDARIES:\n",
    "    \"\"\")\n",
    "    \n",
    "    # Example with real numbers\n",
    "    seq_len = 96\n",
    "    tot_len = 1000  # Example\n",
    "    n_channels = 7\n",
    "    \n",
    "    for channel in range(min(3, n_channels)):  # Show first 3 channels\n",
    "        channel_start_idx = channel * tot_len\n",
    "        channel_end_idx = (channel + 1) * tot_len - 1\n",
    "        \n",
    "        # Last valid sequence for this channel\n",
    "        last_seq_start_time = tot_len - 1  # 999\n",
    "        last_seq_end_time = last_seq_start_time + seq_len  # 1095\n",
    "        \n",
    "        print(f\"Channel {channel}:\")\n",
    "        print(f\"  Dataset indices: [{channel_start_idx} : {channel_end_idx}]\")\n",
    "        print(f\"  Last sequence: Time [{last_seq_start_time} : {last_seq_end_time}] (within channel {channel})\")\n",
    "        print(f\"  Max time index: {tot_len + seq_len - 2} (still within channel {channel} data)\")\n",
    "        print()\n",
    "    \n",
    "    print(\"CONCLUSION:\")\n",
    "    print(\"• Each sequence is ENTIRELY from one channel\")\n",
    "    print(\"• Time boundaries are respected within each channel\") \n",
    "    print(\"• NO mixing between channels occurs\")\n",
    "    print(\"• Each channel is treated as independent time series\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze the iteration pattern\n",
    "    tot_len, dataset_len = analyze_dataloader_iteration()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Demonstrate no mixing\n",
    "    demonstrate_no_mixing()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Visualize the pattern\n",
    "    visualize_channel_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_batch_channel_distribution(batch_size=64, seq_len=96, total_time_points=10000, n_channels=7):\n",
    "    \"\"\"\n",
    "    Analyze how channels are distributed across batches in ETT dataloader\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate ETT dataset parameters\n",
    "    tot_len = total_time_points - seq_len - 1 + 1  # Valid starting positions per channel\n",
    "    dataset_len = tot_len * n_channels  # Total dataset length\n",
    "    \n",
    "    print(\"Batch-Channel Distribution Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Total time points: {total_time_points}\")\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "    print(f\"Valid positions per channel (tot_len): {tot_len}\")\n",
    "    print(f\"Total dataset length: {dataset_len}\")\n",
    "    print(f\"Number of batches: {dataset_len // batch_size}\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze first few batches in detail\n",
    "    print(\"DETAILED BATCH ANALYSIS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    for batch_idx in range(min(10, dataset_len // batch_size)):\n",
    "        batch_start_idx = batch_idx * batch_size\n",
    "        batch_end_idx = batch_start_idx + batch_size\n",
    "        \n",
    "        channels_in_batch = []\n",
    "        time_ranges = []\n",
    "        \n",
    "        print(f\"Batch {batch_idx}: Dataset indices [{batch_start_idx}:{batch_end_idx-1}]\")\n",
    "        \n",
    "        for sample_idx in range(batch_start_idx, min(batch_end_idx, dataset_len)):\n",
    "            # ETT's index mapping\n",
    "            feat_id = sample_idx // tot_len  # Which channel\n",
    "            s_begin = sample_idx % tot_len   # Time position within channel\n",
    "            s_end = s_begin + seq_len\n",
    "            \n",
    "            channels_in_batch.append(feat_id)\n",
    "            time_ranges.append((s_begin, s_end))\n",
    "        \n",
    "        # Count channels in this batch\n",
    "        channel_counts = Counter(channels_in_batch)\n",
    "        \n",
    "        print(f\"  Channels present: {sorted(channel_counts.keys())}\")\n",
    "        print(f\"  Channel distribution: {dict(channel_counts)}\")\n",
    "        \n",
    "        # Show time ranges for first few samples\n",
    "        print(f\"  Sample details (first 5):\")\n",
    "        for i, (channel, (start, end)) in enumerate(zip(channels_in_batch[:5], time_ranges[:5])):\n",
    "            dataset_idx = batch_start_idx + i\n",
    "            print(f\"    Sample {i}: Dataset[{dataset_idx}] = Channel {channel}, Time [{start}:{end}]\")\n",
    "        \n",
    "        if len(channels_in_batch) > 5:\n",
    "            print(f\"    ... and {len(channels_in_batch) - 5} more samples\")\n",
    "        print()\n",
    "    \n",
    "    return tot_len, dataset_len\n",
    "\n",
    "def calculate_channel_transition_points(batch_size=64, seq_len=96, total_time_points=10000, n_channels=7):\n",
    "    \"\"\"\n",
    "    Calculate exactly where channel transitions occur in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    tot_len = total_time_points - seq_len - 1 + 1\n",
    "    \n",
    "    print(\"CHANNEL TRANSITION ANALYSIS:\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    # Find transition points\n",
    "    transition_batches = []\n",
    "    \n",
    "    for channel in range(n_channels - 1):  # 0 to 5 (transition from channel i to i+1)\n",
    "        # Last index of current channel\n",
    "        last_idx_current = (channel + 1) * tot_len - 1\n",
    "        # First index of next channel  \n",
    "        first_idx_next = (channel + 1) * tot_len\n",
    "        \n",
    "        # Which batches do these fall into?\n",
    "        batch_last_current = last_idx_current // batch_size\n",
    "        batch_first_next = first_idx_next // batch_size\n",
    "        \n",
    "        print(f\"Channel {channel} → Channel {channel + 1}:\")\n",
    "        print(f\"  Last index of Channel {channel}: {last_idx_current} (Batch {batch_last_current})\")\n",
    "        print(f\"  First index of Channel {channel + 1}: {first_idx_next} (Batch {batch_first_next})\")\n",
    "        \n",
    "        if batch_last_current == batch_first_next:\n",
    "            print(f\"  ⚠️  MIXED BATCH: Batch {batch_last_current} contains BOTH channels {channel} and {channel + 1}\")\n",
    "            \n",
    "            # Calculate exact positions in the mixed batch\n",
    "            pos_in_batch_last = last_idx_current % batch_size\n",
    "            pos_in_batch_first = first_idx_next % batch_size\n",
    "            \n",
    "            print(f\"     Channel {channel} samples: positions 0 to {pos_in_batch_last}\")\n",
    "            print(f\"     Channel {channel + 1} samples: positions {pos_in_batch_first} to {batch_size - 1}\")\n",
    "            \n",
    "            transition_batches.append({\n",
    "                'batch_idx': batch_last_current,\n",
    "                'from_channel': channel,\n",
    "                'to_channel': channel + 1,\n",
    "                'split_position': pos_in_batch_first\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  ✅ CLEAN: Channels {channel} and {channel + 1} in separate batches\")\n",
    "        print()\n",
    "    \n",
    "    return transition_batches\n",
    "\n",
    "def visualize_batch_channel_distribution(batch_size=64, seq_len=96, total_time_points=1000, n_channels=7):\n",
    "    \"\"\"\n",
    "    Visualize how channels are distributed across batches\n",
    "    \"\"\"\n",
    "    \n",
    "    tot_len = total_time_points - seq_len - 1 + 1\n",
    "    dataset_len = tot_len * n_channels\n",
    "    n_batches = min(20, dataset_len // batch_size)  # Show first 20 batches\n",
    "    \n",
    "    # Create batch-channel matrix\n",
    "    batch_channel_matrix = np.zeros((n_batches, n_channels))\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_start_idx = batch_idx * batch_size\n",
    "        batch_end_idx = min(batch_start_idx + batch_size, dataset_len)\n",
    "        \n",
    "        for sample_idx in range(batch_start_idx, batch_end_idx):\n",
    "            feat_id = sample_idx // tot_len\n",
    "            if feat_id < n_channels:\n",
    "                batch_channel_matrix[batch_idx, feat_id] += 1\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Heatmap of channel counts per batch\n",
    "    im1 = ax1.imshow(batch_channel_matrix.T, cmap='YlOrRd', aspect='auto')\n",
    "    ax1.set_xlabel('Batch Index')\n",
    "    ax1.set_ylabel('Channel ID')\n",
    "    ax1.set_title(f'Channel Distribution Across Batches (Batch Size = {batch_size})')\n",
    "    ax1.set_yticks(range(n_channels))\n",
    "    ax1.set_yticklabels([f'Ch {i}' for i in range(n_channels)])\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(n_batches):\n",
    "        for j in range(n_channels):\n",
    "            if batch_channel_matrix[i, j] > 0:\n",
    "                text = ax1.text(i, j, f'{int(batch_channel_matrix[i, j])}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im1, ax=ax1, label='Number of samples')\n",
    "    \n",
    "    # Line plot showing channel transitions\n",
    "    channel_ratios = batch_channel_matrix / batch_size * 100  # Convert to percentages\n",
    "    \n",
    "    for channel in range(n_channels):\n",
    "        ax2.plot(range(n_batches), channel_ratios[:, channel], \n",
    "                marker='o', label=f'Channel {channel}', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Batch Index')\n",
    "    ax2.set_ylabel('Percentage of Batch (%)')\n",
    "    ax2.set_title('Channel Percentage Distribution Across Batches')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return batch_channel_matrix\n",
    "\n",
    "def practical_implications(batch_size=64, seq_len=96, total_time_points=10000, n_channels=7):\n",
    "    \"\"\"\n",
    "    Explain practical implications for training and evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    tot_len = total_time_points - seq_len - 1 + 1\n",
    "    \n",
    "    print(\"PRACTICAL IMPLICATIONS:\")\n",
    "    print(\"=\" * 23)\n",
    "    \n",
    "    print(\"1. TRAINING IMPLICATIONS:\")\n",
    "    print(\"   • Each batch may contain MIXED channels\")\n",
    "    print(\"   • Model sees different channels within same batch\")\n",
    "    print(\"   • Gradient updates aggregate across multiple channels\")\n",
    "    print(\"   • This is actually GOOD for learning diverse patterns\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. EVALUATION IMPLICATIONS:\")\n",
    "    print(\"   • Need to track which samples belong to which channel\")\n",
    "    print(\"   • Can't assume batch = single channel\")\n",
    "    print(\"   • Must use feat_id calculation for channel identification\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. BATCH SIZE EFFECTS:\")\n",
    "    smaller_batch = 32\n",
    "    larger_batch = 128\n",
    "    \n",
    "    # Calculate transitions for different batch sizes\n",
    "    for bs in [smaller_batch, batch_size, larger_batch]:\n",
    "        mixed_batches = 0\n",
    "        for channel in range(n_channels - 1):\n",
    "            last_idx = (channel + 1) * tot_len - 1\n",
    "            first_idx = (channel + 1) * tot_len\n",
    "            if last_idx // bs == first_idx // bs:\n",
    "                mixed_batches += 1\n",
    "        \n",
    "        print(f\"   Batch size {bs}: {mixed_batches}/{n_channels-1} transitions create mixed batches\")\n",
    "    \n",
    "    print()\n",
    "    print(\"4. RECOMMENDATIONS:\")\n",
    "    print(\"   ✅ Use the provided evaluation code to handle mixed batches\")\n",
    "    print(\"   ✅ Track channel_id for each sample during inference\")\n",
    "    print(\"   ✅ Consider batch_size impact on channel mixing\")\n",
    "    print(\"   ⚠️  Don't assume batch contains only one channel\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with your parameters\n",
    "    batch_size = 64\n",
    "    seq_len = 96\n",
    "    total_time_points = 10000  # Example ETT dataset size\n",
    "    n_channels = 7\n",
    "    \n",
    "    print(\"ANALYSIS FOR YOUR PARAMETERS:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Detailed analysis\n",
    "    tot_len, dataset_len = analyze_batch_channel_distribution(batch_size, seq_len, total_time_points, n_channels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Transition analysis\n",
    "    transitions = calculate_channel_transition_points(batch_size, seq_len, total_time_points, n_channels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Practical implications\n",
    "    practical_implications(batch_size, seq_len, total_time_points, n_channels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Visualization\n",
    "    print(\"GENERATING VISUALIZATION...\")\n",
    "    batch_channel_matrix = visualize_batch_channel_distribution(batch_size, seq_len, 2000, n_channels)  # Smaller dataset for vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bc002",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe8e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ByteLatentTransformerArgs(\n",
    "    seed=42,\n",
    "    vocab_size=4096,                       # Small byte-level vocab\n",
    "    max_length=seq_len,                        # Max full sequence length\n",
    "    max_seqlen=seq_len,\n",
    "    max_encoder_seq_length=seq_len,\n",
    "    local_attention_window_len=seq_len,        # Local window, 128 is sufficient for small models\n",
    "\n",
    "    dim_global=128,                        # Lower than default 512\n",
    "    dim_local_encoder=64,\n",
    "    dim_local_decoder=64,\n",
    "\n",
    "    n_layers_global=2,\n",
    "    n_layers_local_encoder=2,\n",
    "    n_layers_local_decoder=2,\n",
    "\n",
    "    n_heads_global=2,                      # Reduce heads\n",
    "    n_heads_local_encoder=2,\n",
    "    n_heads_local_decoder=2,\n",
    "\n",
    "    patch_size=1,\n",
    "    patch_in_forward=False,                # Patch in forward pass\n",
    "    patching_batch_size=256,\n",
    "    patching_device=\"cuda\",               # Use CPU for patching in small model\n",
    "    patching_mode=\"entropy\",\n",
    "    patching_threshold=3.0,\n",
    "    max_patch_length=16,\n",
    "    monotonicity=True,            # Monotonic patching\n",
    "    pad_to_max_length=True,\n",
    "\n",
    "    cross_attn_encoder=True,\n",
    "    cross_attn_decoder=True,\n",
    "    cross_attn_k=2,\n",
    "    cross_attn_nheads=2,\n",
    "    cross_attn_all_layers_encoder=True,\n",
    "    cross_attn_all_layers_decoder=True,\n",
    "    cross_attn_use_flex_attention=False,\n",
    "    cross_attn_init_by_pooling=True,\n",
    "\n",
    "    encoder_hash_byte_group_size=[4,5,6],   # Fewer hash sizes\n",
    "    encoder_hash_byte_group_vocab=2**12,\n",
    "    encoder_hash_byte_group_nb_functions=1,\n",
    "    encoder_enable_byte_ngrams=False,\n",
    "\n",
    "    non_linearity=\"swiglu\",\n",
    "    use_rope=True,\n",
    "    attn_impl=\"sdpa\",                      # Efficient PyTorch attention\n",
    "    attn_bias_type=\"causal\",\n",
    "\n",
    "    dropout=0.0,\n",
    "    layer_ckpt=\"none\",                     # No checkpointing in small model\n",
    "    init_use_gaussian=True,\n",
    "    init_use_depth=\"current\",\n",
    "    alpha_depth=\"disabled\",\n",
    "    log_patch_lengths=True,\n",
    "\n",
    "    downsampling_by_pooling=\"max\",         # Efficient downsampling\n",
    "    use_local_encoder_transformer=True,\n",
    "    share_encoder_decoder_emb=True         # Save memory if possible\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23457017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = ByteLatentTransformerArgs(\n",
    "#     seed=42,\n",
    "#     vocab_size=4096,                       # Small byte-level vocab\n",
    "#     max_length=seq_len,                        # Max full sequence length\n",
    "#     max_seqlen=seq_len,\n",
    "#     max_encoder_seq_length=seq_len,\n",
    "#     local_attention_window_len=seq_len,        # Local window, 128 is sufficient for small models\n",
    "#     dim = 128,                            # Lower than default 512\n",
    "#     # dim_global=512,                        # Lower than default 512\n",
    "#     dim_local_encoder=128,\n",
    "#     dim_local_decoder=128,\n",
    "\n",
    "#     n_layers_global=3,\n",
    "#     n_layers_local_encoder=2,\n",
    "#     n_layers_local_decoder=2,\n",
    "\n",
    "#     n_heads_global=2,                      # Reduce heads\n",
    "#     n_heads_local_encoder=2,\n",
    "#     n_heads_local_decoder=2,\n",
    "\n",
    "#     patch_size=8,\n",
    "#     patch_in_forward=False,                # Patch in forward pass\n",
    "#     patching_batch_size=256,\n",
    "#     patching_device=\"cuda\",               # Use CPU for patching in small model\n",
    "#     patching_mode=\"static\",\n",
    "#     patching_threshold=3.0,\n",
    "#     max_patch_length=16,\n",
    "#     monotonicity=True,            # Monotonic patching\n",
    "#     pad_to_max_length=True,\n",
    "\n",
    "#     cross_attn_encoder=True,\n",
    "#     cross_attn_decoder=True,\n",
    "#     cross_attn_k=8,\n",
    "#     cross_attn_nheads=2,\n",
    "#     cross_attn_all_layers_encoder=True,\n",
    "#     cross_attn_all_layers_decoder=True,\n",
    "#     cross_attn_use_flex_attention=False,\n",
    "#     cross_attn_init_by_pooling=True,\n",
    "\n",
    "#     encoder_hash_byte_group_size=[4,5,6,7,8],   # Fewer hash sizes\n",
    "#     encoder_hash_byte_group_vocab=2**5,\n",
    "#     encoder_hash_byte_group_nb_functions=1,\n",
    "#     encoder_enable_byte_ngrams=False,\n",
    "\n",
    "#     non_linearity=\"swiglu\",\n",
    "#     use_rope=True,\n",
    "#     attn_impl=\"sdpa\",                      # Efficient PyTorch attention\n",
    "#     attn_bias_type=\"causal\",\n",
    "\n",
    "#     dropout=0.0,\n",
    "#     layer_ckpt=\"none\",                     # No checkpointing in small model\n",
    "#     init_use_gaussian=True,\n",
    "#     init_use_depth=\"current\",\n",
    "#     alpha_depth=\"disabled\",\n",
    "#     log_patch_lengths=True,\n",
    "\n",
    "#     downsampling_by_pooling=\"max\",         # Efficient downsampling\n",
    "#     use_local_encoder_transformer=True,\n",
    "#     # share_encoder_decoder_emb=True         # Save memory if possible\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fea30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 2.19M\n"
     ]
    }
   ],
   "source": [
    "model = ByteLatentTransformer(model_args)\n",
    "model = model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# n of params in model in millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in model: {count_parameters(model) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8fad9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 13]), 96)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_static_patch_lengths(batch_size, seq_len, patch_length=8):\n",
    "    if seq_len == 336:\n",
    "        n_patches = 43\n",
    "    elif seq_len == 96:\n",
    "        n_patches = 13\n",
    "    elif seq_len == 512:\n",
    "        n_patches = 65\n",
    "    elif seq_len == 256:\n",
    "        n_patches = 33\n",
    "    elif seq_len == 192:\n",
    "        n_patches = 13\n",
    "    elif seq_len == 720:\n",
    "        n_patches = 91\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported seq_len: {seq_len}\")\n",
    "\n",
    "    l = torch.full((batch_size,n_patches), patch_length).to('cuda')\n",
    "    l[:,0] = 1\n",
    "    l[:,1] = patch_length - 1\n",
    "    patch_lengths = l\n",
    "    return patch_lengths\n",
    "patch_lengths = create_static_patch_lengths(batch_size=batch_size, seq_len=seq_len)\n",
    "# patch_lengths = torch.ones((batch_size, seq_len), dtype=torch.int32, device='cuda') * 1\n",
    "patch_lengths.shape, patch_lengths[0].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5892003",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=5e-4, \n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.95)  # Use better beta values from first code\n",
    "    )\n",
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dbaa018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precision: bfloat16\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(model_args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(model_args.seed)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "print(f\"Using precision: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "203095fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteLatentTransformer(\n",
       "  (local_encoder): LocalEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w3): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w2): Linear(in_features=256, out_features=64, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=64, out_features=128, bias=False)\n",
       "    (tok_embeddings): Embedding(4096, 64)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-1): 2 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_transformer): GlobalTransformer(\n",
       "    (rope_embeddings): RotaryEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (wk): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (wv): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (wo): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (w3): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=128, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (local_decoder): LocalDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w3): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w2): Linear(in_features=256, out_features=64, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-1): 2 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (output): Linear(in_features=64, out_features=4096, bias=False)\n",
       "  )\n",
       "  (encoder_hash_tok_embedding): ModuleList(\n",
       "    (0-2): 3 x Embedding(4096, 64)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd368a",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d76d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new config with prediction_length=1\n",
    "train_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=seq_len,\n",
    "    prediction_length=seq_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, train_tokenizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fc971",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fa7ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 133/133 [00:04<00:00, 27.46it/s, loss=3.7682, avg_loss=4.0276, lr=0.000500]\n",
      "Epoch 1 Validation: 100%|██████████| 45/45 [00:01<00:00, 39.31it/s, loss=4.3921, avg_loss=4.3431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 Results:\n",
      "Training Loss: 4.0276 (Time: 4.90s)\n",
      "Validation Loss (Avg. MSE): 0.1785,  (Time: 1.20s)\n",
      "New best model saved with validation loss: 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 133/133 [00:04<00:00, 27.66it/s, loss=3.5240, avg_loss=3.7244, lr=0.000498]\n",
      "Epoch 2 Validation: 100%|██████████| 45/45 [00:01<00:00, 32.45it/s, loss=4.2238, avg_loss=4.2963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 Results:\n",
      "Training Loss: 3.7244 (Time: 4.85s)\n",
      "Validation Loss (Avg. MSE): 0.1685,  (Time: 1.43s)\n",
      "New best model saved with validation loss: 0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 133/133 [00:04<00:00, 27.77it/s, loss=3.5067, avg_loss=3.6197, lr=0.000496]\n",
      "Epoch 3 Validation: 100%|██████████| 45/45 [00:01<00:00, 34.80it/s, loss=4.3384, avg_loss=4.2312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 Results:\n",
      "Training Loss: 3.6197 (Time: 4.83s)\n",
      "Validation Loss (Avg. MSE): 0.1558,  (Time: 1.34s)\n",
      "New best model saved with validation loss: 0.1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 133/133 [00:04<00:00, 28.09it/s, loss=3.6012, avg_loss=3.5417, lr=0.000493]\n",
      "Epoch 4 Validation: 100%|██████████| 45/45 [00:01<00:00, 35.32it/s, loss=4.3441, avg_loss=4.2203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 Results:\n",
      "Training Loss: 3.5417 (Time: 4.78s)\n",
      "Validation Loss (Avg. MSE): 0.1517,  (Time: 1.32s)\n",
      "New best model saved with validation loss: 0.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 133/133 [00:04<00:00, 27.40it/s, loss=3.4281, avg_loss=3.4751, lr=0.000489]\n",
      "Epoch 5 Validation: 100%|██████████| 45/45 [00:01<00:00, 34.96it/s, loss=4.1880, avg_loss=4.2006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 Results:\n",
      "Training Loss: 3.4751 (Time: 4.90s)\n",
      "Validation Loss (Avg. MSE): 0.1467,  (Time: 1.33s)\n",
      "New best model saved with validation loss: 0.1467\n",
      "Checkpoint saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 133/133 [00:04<00:00, 27.77it/s, loss=3.2253, avg_loss=3.4168, lr=0.000484]\n",
      "Epoch 6 Validation: 100%|██████████| 45/45 [00:01<00:00, 32.45it/s, loss=4.3247, avg_loss=4.2003]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 Results:\n",
      "Training Loss: 3.4168 (Time: 4.83s)\n",
      "Validation Loss (Avg. MSE): 0.1493,  (Time: 1.43s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Training: 100%|██████████| 133/133 [00:04<00:00, 28.05it/s, loss=3.2741, avg_loss=3.3641, lr=0.000479]\n",
      "Epoch 7 Validation: 100%|██████████| 45/45 [00:01<00:00, 36.22it/s, loss=4.0743, avg_loss=4.2098]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 Results:\n",
      "Training Loss: 3.3641 (Time: 4.78s)\n",
      "Validation Loss (Avg. MSE): 0.1473,  (Time: 1.29s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Training: 100%|██████████| 133/133 [00:04<00:00, 27.81it/s, loss=3.3766, avg_loss=3.3135, lr=0.000472]\n",
      "Epoch 8 Validation: 100%|██████████| 45/45 [00:01<00:00, 35.89it/s, loss=4.3115, avg_loss=4.2136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 Results:\n",
      "Training Loss: 3.3135 (Time: 4.82s)\n",
      "Validation Loss (Avg. MSE): 0.1459,  (Time: 1.30s)\n",
      "New best model saved with validation loss: 0.1459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 133/133 [00:04<00:00, 27.67it/s, loss=3.2154, avg_loss=3.2666, lr=0.000465]\n",
      "Epoch 9 Validation: 100%|██████████| 45/45 [00:01<00:00, 33.89it/s, loss=4.3333, avg_loss=4.1969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 Results:\n",
      "Training Loss: 3.2666 (Time: 4.85s)\n",
      "Validation Loss (Avg. MSE): 0.1413,  (Time: 1.39s)\n",
      "New best model saved with validation loss: 0.1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 133/133 [00:04<00:00, 28.03it/s, loss=3.2445, avg_loss=3.2233, lr=0.000457]\n",
      "Epoch 10 Validation: 100%|██████████| 45/45 [00:01<00:00, 35.63it/s, loss=4.2605, avg_loss=4.2382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 Results:\n",
      "Training Loss: 3.2233 (Time: 4.79s)\n",
      "Validation Loss (Avg. MSE): 0.1429,  (Time: 1.31s)\n",
      "Checkpoint saved at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 133/133 [00:04<00:00, 28.43it/s, loss=3.1116, avg_loss=3.1809, lr=0.000448]\n",
      "Epoch 11 Validation: 100%|██████████| 45/45 [00:01<00:00, 37.44it/s, loss=4.2447, avg_loss=4.2542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 Results:\n",
      "Training Loss: 3.1809 (Time: 4.72s)\n",
      "Validation Loss (Avg. MSE): 0.1450,  (Time: 1.25s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 133/133 [00:04<00:00, 28.20it/s, loss=3.1255, avg_loss=3.1398, lr=0.000439]\n",
      "Epoch 12 Validation: 100%|██████████| 45/45 [00:01<00:00, 38.76it/s, loss=3.9655, avg_loss=4.2506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 Results:\n",
      "Training Loss: 3.1398 (Time: 4.76s)\n",
      "Validation Loss (Avg. MSE): 0.1400,  (Time: 1.20s)\n",
      "New best model saved with validation loss: 0.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 133/133 [00:04<00:00, 28.20it/s, loss=3.0381, avg_loss=3.1025, lr=0.000429]\n",
      "Epoch 13 Validation: 100%|██████████| 45/45 [00:01<00:00, 33.72it/s, loss=4.1804, avg_loss=4.2829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 Results:\n",
      "Training Loss: 3.1025 (Time: 4.76s)\n",
      "Validation Loss (Avg. MSE): 0.1416,  (Time: 1.38s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Training: 100%|██████████| 133/133 [00:04<00:00, 28.28it/s, loss=3.0933, avg_loss=3.0640, lr=0.000419]\n",
      "Epoch 14 Validation: 100%|██████████| 45/45 [00:01<00:00, 35.80it/s, loss=4.2143, avg_loss=4.2746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 Results:\n",
      "Training Loss: 3.0640 (Time: 4.74s)\n",
      "Validation Loss (Avg. MSE): 0.1416,  (Time: 1.30s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Training: 100%|██████████| 133/133 [00:04<00:00, 28.07it/s, loss=2.8603, avg_loss=3.0288, lr=0.000407]\n",
      "Epoch 15 Validation: 100%|██████████| 45/45 [00:01<00:00, 37.04it/s, loss=4.4085, avg_loss=4.3239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 Results:\n",
      "Training Loss: 3.0288 (Time: 4.78s)\n",
      "Validation Loss (Avg. MSE): 0.1447,  (Time: 1.26s)\n",
      "Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training:  51%|█████     | 68/133 [00:02<00:02, 26.91it/s, loss=3.0848, avg_loss=2.9817, lr=0.000401]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     loss = ce_loss / grad_accumulation_steps\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     total_loss += loss.item() * grad_accumulation_steps\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "total_steps = epochs * num_batches\n",
    "min_lr = learning_rate * min_lr_factor\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Training\", position=0, leave=True)\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in progress_bar:\n",
    "\n",
    "        iteration = epoch * num_batches + i\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = get_lr(\n",
    "            iteration,\n",
    "            total_steps,\n",
    "            warmup_steps,\n",
    "            learning_rate,\n",
    "            min_lr,\n",
    "            decay_lr\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for micro_step in range(grad_accumulation_steps):\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to(device), patch_lengths)\n",
    "            predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "            valid_mask = (target_token_ids != PAD_ID)\n",
    "            pred_values = predicted_tokens.float().cuda()\n",
    "            target_values = target_token_ids.float().cuda()\n",
    "            \n",
    "            # Calculate loss\n",
    "            # NEW:\n",
    "            valid_mask = (target_token_ids != PAD_ID)\n",
    "            loss, ce_loss, mse_loss = improved_loss_with_gradients(\n",
    "                logits, target_token_ids.cuda(), valid_mask\n",
    "            )\n",
    "            loss = ce_loss / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "                \n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "        # Update weights with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update training metrics\n",
    "        epoch_loss += total_loss\n",
    "        avg_epoch_loss = epoch_loss / (i + 1)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{total_loss:.4f}\",\n",
    "            'avg_loss': f\"{avg_epoch_loss:.4f}\",\n",
    "            'lr': f\"{lr:.6f}\"\n",
    "        })\n",
    "    \n",
    "    # Calculate training time and average loss\n",
    "    train_time = time.time() - t1\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    t1 = time.time()\n",
    "    model.eval()\n",
    "    val_loss = validate(model, validate_loader, tokenizer, patch_lengths, device, desc=f\"Epoch {epoch+1} Validation\")\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"Training Loss: {train_avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
    "    print(f\"Validation Loss (Avg. MSE): {val_loss:.4f},  (Time: {val_time:.2f}s)\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss\n",
    "        }, os.path.join(output_dir, f'best_model_{features}_{dataset_name}_{seq_len}.pth'))\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if save_every > 0 and (epoch + 1) % save_every == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss\n",
    "        }, os.path.join(output_dir, f'checkpoint_{seq_len}_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb41f52",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model on test set...\n",
      "Saved Val loss: 0.13998858630657196\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate best model\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "checkpoint = torch.load(os.path.join(output_dir, f\"best_model_{features}_{dataset_name}_{seq_len}.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Saved Val loss: {checkpoint['val_loss'].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da249b2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410bb0c",
   "metadata": {},
   "source": [
    "### Input 92 >> Predict 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee533f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "input_len = 96\n",
    "pred_len = 96\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = pred_len \n",
    "\n",
    "_, num_patches = patch_lengths.shape\n",
    "\n",
    "# Create a new config with prediction_length=1\n",
    "test_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=input_len,\n",
    "    prediction_length=pred_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, test_tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8e46a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 96]), torch.Size([64, 96]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': dataset_name,\n",
    "    'data' : dataset_name,\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': f'{dataset_name}.csv',\n",
    "    'features': features,\n",
    "    'seq_len': input_len,\n",
    "    'label_len': 0,\n",
    "    'pred_len': pred_len\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=eval_batch_size,\n",
    "    freq='h' if 'h' in dataset_name else 'm',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "    # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "    target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "    break\n",
    "\n",
    "# patch_lengths = create_static_patch_lengths(batch_size=eval_batch_size, seq_len=input_len)\n",
    "assert patch_lengths[0].sum().item() == token_ids.shape[1]\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    if i == 0:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e5a9927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9059a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - MSE: 0.2983, MAE: 0.4167\n",
      "Batch 1 - MSE: 0.4019, MAE: 0.5307\n",
      "Batch 2 - MSE: 0.5645, MAE: 0.6076\n",
      "Batch 3 - MSE: 0.3845, MAE: 0.4771\n",
      "Batch 4 - MSE: 0.3125, MAE: 0.4172\n",
      "Batch 5 - MSE: 0.2308, MAE: 0.3541\n",
      "Batch 6 - MSE: 0.1333, MAE: 0.2682\n",
      "Batch 7 - MSE: 0.1472, MAE: 0.2744\n",
      "Batch 8 - MSE: 0.4938, MAE: 0.5509\n",
      "Batch 9 - MSE: 1.2583, MAE: 0.9177\n",
      "Batch 10 - MSE: 0.6931, MAE: 0.5686\n",
      "Batch 11 - MSE: 0.7439, MAE: 0.6083\n",
      "Batch 12 - MSE: 0.4563, MAE: 0.4563\n",
      "Batch 13 - MSE: 0.6926, MAE: 0.5999\n",
      "Batch 14 - MSE: 0.9255, MAE: 0.6862\n",
      "Batch 15 - MSE: 1.0834, MAE: 0.7147\n",
      "Batch 16 - MSE: 1.1563, MAE: 0.7262\n",
      "Batch 17 - MSE: 1.0209, MAE: 0.6803\n",
      "Batch 18 - MSE: 1.1145, MAE: 0.7099\n",
      "Batch 19 - MSE: 0.9185, MAE: 0.6117\n",
      "Batch 20 - MSE: 1.3136, MAE: 0.7350\n",
      "Batch 21 - MSE: 1.3730, MAE: 0.7704\n",
      "Batch 22 - MSE: 1.3461, MAE: 0.7631\n",
      "Batch 23 - MSE: 1.1259, MAE: 0.6807\n",
      "Batch 24 - MSE: 0.9917, MAE: 0.5886\n",
      "Batch 25 - MSE: 0.8000, MAE: 0.6388\n",
      "Batch 26 - MSE: 1.5002, MAE: 0.9952\n",
      "Batch 27 - MSE: 1.3612, MAE: 0.7503\n",
      "Batch 28 - MSE: 0.8653, MAE: 0.5337\n",
      "Batch 29 - MSE: 1.0394, MAE: 0.6878\n",
      "Batch 30 - MSE: 1.2049, MAE: 0.7151\n",
      "Batch 31 - MSE: 1.1057, MAE: 0.6761\n",
      "Batch 32 - MSE: 0.8967, MAE: 0.6066\n",
      "Batch 33 - MSE: 0.8628, MAE: 0.7174\n",
      "Batch 34 - MSE: 1.7457, MAE: 1.0720\n",
      "Batch 35 - MSE: 1.0740, MAE: 0.7001\n",
      "Batch 36 - MSE: 0.8060, MAE: 0.5448\n",
      "Batch 37 - MSE: 1.8532, MAE: 0.8287\n",
      "Batch 38 - MSE: 2.0782, MAE: 0.9390\n",
      "Batch 39 - MSE: 0.9507, MAE: 0.6379\n",
      "Batch 40 - MSE: 1.3040, MAE: 0.7094\n",
      "Batch 41 - MSE: 1.0672, MAE: 0.6920\n",
      "Batch 42 - MSE: 1.2084, MAE: 0.7064\n",
      "\n",
      "Overall Results (96) (96):\n",
      "Average MSE: 0.9513\n",
      "Average MAE: 0.6481\n"
     ]
    }
   ],
   "source": [
    "top_k = 2  # Top-k filtering\n",
    "temperature = 1.0 # Temperature for sampling\n",
    "B, L = token_ids.shape\n",
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    try:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids_start\n",
    "            forecast = torch.zeros((B, max_new_tokens), dtype=torch.long).to(device)\n",
    "            for _ in range(max_new_tokens):\n",
    "                # print(_)\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids.to(device), patch_lengths)\n",
    "                #pred_tokens = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "                #next_token = pred_tokens.unsqueeze(1) \n",
    "                next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # # Apply top-k filtering if specified\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, __ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                    next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # # Apply softmax to convert to probabilities\n",
    "                probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # print(next_token.shape, forecast.shape, _)\n",
    "                forecast[:, _] = next_token.squeeze(-1)\n",
    "                # Update token sequence and patch lengths\n",
    "                all_tokens = torch.cat([token_ids.to(device), next_token], dim=1)\n",
    "                look_back = max(pred_len, all_tokens.shape[1])\n",
    "                token_ids = all_tokens[:, -1*input_len:]  # Keep last 512 tokens\n",
    "\n",
    "            \n",
    "            # Extract the forecasted and actual tokens\n",
    "            actual = target_token_ids\n",
    "\n",
    "            \n",
    "            # Convert tokens back to values using inverse transform\n",
    "            actual_values = tokenizer.output_transform(actual.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            forecast_values = tokenizer.output_transform(forecast.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            mse = torch.mean((actual_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(actual_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print for the current batch\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results ({seq_len}) ({pred_len}):\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(f\"forecast_metrics_{dataset_name}_{features}_{seq_len}_{pred_len}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40d925bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1887, 1883, 1879,  ..., 1939, 1929, 1929],\n",
       "        [1883, 1883, 1879,  ..., 2025, 1992, 1982],\n",
       "        [1879, 1879, 1879,  ..., 1759, 1760, 1783],\n",
       "        ...,\n",
       "        [1909, 1909, 1909,  ..., 1894, 1890, 1883],\n",
       "        [1923, 1923, 1923,  ..., 1952, 1940, 1927],\n",
       "        [1919, 1919, 1919,  ..., 2097, 2097, 2065]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e701ac6",
   "metadata": {},
   "source": [
    "### Input 96 >> Predict 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ff7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 96\n",
    "pred_len = 192\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = pred_len \n",
    "B, L = token_ids.shape\n",
    "_, num_patches = patch_lengths.shape\n",
    "\n",
    "# Create a new config with prediction_length=1\n",
    "test_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=input_len,\n",
    "    prediction_length=pred_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, test_tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': dataset_name,\n",
    "    'data' : dataset_name,\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': f'{dataset_name}.csv',\n",
    "    'features': features,\n",
    "    'seq_len': input_len,\n",
    "    'label_len': 0,\n",
    "    'pred_len': pred_len\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=eval_batch_size,\n",
    "    freq='h' if 'h' in dataset_name else 'm',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "    # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "    target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "    break\n",
    "\n",
    "patch_lengths = create_static_patch_lengths(batch_size=eval_batch_size, seq_len=input_len)\n",
    "assert patch_lengths[0].sum().item() == token_ids.shape[1]\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    if i == 0:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    try:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids_start\n",
    "            forecast = torch.zeros((B, max_new_tokens), dtype=torch.long).to(device)\n",
    "            for _ in range(max_new_tokens):\n",
    "                # print(_)\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids.to(device), patch_lengths)\n",
    "                pred_tokens = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "                next_token = pred_tokens.unsqueeze(1) \n",
    "                # next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # # Apply top-k filtering if specified\n",
    "                # if top_k is not None and top_k > 0:\n",
    "                #     v, __ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                #     next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # # Apply softmax to convert to probabilities\n",
    "                # probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # print(next_token.shape, forecast.shape, _)\n",
    "                forecast[:, _] = next_token.squeeze(-1)\n",
    "                # Update token sequence and patch lengths\n",
    "                all_tokens = torch.cat([token_ids.to(device), next_token], dim=1)\n",
    "                look_back = max(pred_len, all_tokens.shape[1])\n",
    "                token_ids = all_tokens[:, -1*input_len:]  # Keep last 512 tokens\n",
    "\n",
    "            \n",
    "            # Extract the forecasted and actual tokens\n",
    "            actual = target_token_ids\n",
    "\n",
    "            \n",
    "            # Convert tokens back to values using inverse transform\n",
    "            actual_values = tokenizer.output_transform(actual.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            forecast_values = tokenizer.output_transform(forecast.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            mse = torch.mean((actual_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(actual_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print for the current batch\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results ({seq_len}) ({pred_len}):\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(f\"forecast_metrics_{dataset_name}_{features}_{input_len}_{pred_len}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38ec7b",
   "metadata": {},
   "source": [
    "### Input 96 >> Predict 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 96\n",
    "pred_len = 336\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = pred_len \n",
    "B, L = token_ids.shape\n",
    "_, num_patches = patch_lengths.shape\n",
    "\n",
    "# Create a new config with prediction_length=1\n",
    "test_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=input_len,\n",
    "    prediction_length=pred_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, test_tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f456bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': dataset_name,\n",
    "    'data' : dataset_name,\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': f'{dataset_name}.csv',\n",
    "    'features': features,\n",
    "    'seq_len': input_len,\n",
    "    'label_len': 0,\n",
    "    'pred_len': pred_len\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=eval_batch_size,\n",
    "    freq='h' if 'h' in dataset_name else 'm',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "    # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "    target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "    break\n",
    "\n",
    "patch_lengths = create_static_patch_lengths(batch_size=eval_batch_size, seq_len=input_len)\n",
    "assert patch_lengths[0].sum().item() == token_ids.shape[1]\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    if i == 0:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    try:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids_start\n",
    "            forecast = torch.zeros((B, max_new_tokens), dtype=torch.long).to(device)\n",
    "            for _ in range(max_new_tokens):\n",
    "                # print(_)\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids.to(device), patch_lengths)\n",
    "                pred_tokens = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "                next_token = pred_tokens.unsqueeze(1) \n",
    "                # next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # # Apply top-k filtering if specified\n",
    "                # if top_k is not None and top_k > 0:\n",
    "                #     v, __ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                #     next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # # Apply softmax to convert to probabilities\n",
    "                # probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # print(next_token.shape, forecast.shape, _)\n",
    "                forecast[:, _] = next_token.squeeze(-1)\n",
    "                # Update token sequence and patch lengths\n",
    "                all_tokens = torch.cat([token_ids.to(device), next_token], dim=1)\n",
    "                look_back = max(pred_len, all_tokens.shape[1])\n",
    "                token_ids = all_tokens[:, -1*input_len:]  # Keep last 512 tokens\n",
    "\n",
    "            \n",
    "            # Extract the forecasted and actual tokens\n",
    "            actual = target_token_ids\n",
    "\n",
    "            \n",
    "            # Convert tokens back to values using inverse transform\n",
    "            actual_values = tokenizer.output_transform(actual.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            forecast_values = tokenizer.output_transform(forecast.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            mse = torch.mean((actual_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(actual_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print for the current batch\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results ({seq_len}) ({pred_len}):\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(f\"forecast_metrics_{dataset_name}_{features}_{input_len}_{pred_len}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb60817",
   "metadata": {},
   "source": [
    "### Input 96 >> Predict 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 96\n",
    "pred_len = 720\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = pred_len \n",
    "B, L = token_ids.shape\n",
    "_, num_patches = patch_lengths.shape\n",
    "\n",
    "# Create a new config with prediction_length=1\n",
    "test_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=input_len,\n",
    "    prediction_length=pred_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, test_tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7410b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': dataset_name,\n",
    "    'data' : dataset_name,\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': f'{dataset_name}.csv',\n",
    "    'features': features,\n",
    "    'seq_len': input_len,\n",
    "    'label_len': 0,\n",
    "    'pred_len': pred_len\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=eval_batch_size,\n",
    "    freq='h' if 'h' in dataset_name else 'm',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "    # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "    target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "    break\n",
    "\n",
    "patch_lengths = create_static_patch_lengths(batch_size=eval_batch_size, seq_len=input_len)\n",
    "assert patch_lengths[0].sum().item() == token_ids.shape[1]\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    if i == 0:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f267b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    try:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids_start\n",
    "            forecast = torch.zeros((B, max_new_tokens), dtype=torch.long).to(device)\n",
    "            for _ in range(max_new_tokens):\n",
    "                # print(_)\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids.to(device), patch_lengths)\n",
    "                pred_tokens = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "                next_token = pred_tokens.unsqueeze(1) \n",
    "                # next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # # Apply top-k filtering if specified\n",
    "                # if top_k is not None and top_k > 0:\n",
    "                #     v, __ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                #     next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # # Apply softmax to convert to probabilities\n",
    "                # probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # print(next_token.shape, forecast.shape, _)\n",
    "                forecast[:, _] = next_token.squeeze(-1)\n",
    "                # Update token sequence and patch lengths\n",
    "                all_tokens = torch.cat([token_ids.to(device), next_token], dim=1)\n",
    "                look_back = max(pred_len, all_tokens.shape[1])\n",
    "                token_ids = all_tokens[:, -1*input_len:]  # Keep last 512 tokens\n",
    "\n",
    "            \n",
    "            # Extract the forecasted and actual tokens\n",
    "            actual = target_token_ids\n",
    "\n",
    "            \n",
    "            # Convert tokens back to values using inverse transform\n",
    "            actual_values = tokenizer.output_transform(actual.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            forecast_values = tokenizer.output_transform(forecast.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            mse = torch.mean((actual_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(actual_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print for the current batch\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results ({seq_len}) ({pred_len}):\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(f\"forecast_metrics_{dataset_name}_{features}_{input_len}_{pred_len}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b58330",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
