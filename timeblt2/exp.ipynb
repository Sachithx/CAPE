{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1febc3",
   "metadata": {},
   "source": [
    "## Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c795ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # must be BEFORE torch/TF import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from bytelatent.model.blt import ByteLatentTransformerArgs, ByteLatentTransformer\n",
    "from utils.train_utils import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from bytelatent.tokenizers.constants import PAD_ID\n",
    "from utils.eval_utils import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)   # 0 here means \"the first visible GPU\", i.e. physical #3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f33e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args\n",
    "vocab_size = 2048\n",
    "quant_range = 10\n",
    "batch_size = 512\n",
    "seq_len = 96\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-2\n",
    "epochs = 500  # Increased for early stopping\n",
    "grad_accumulation_steps = 1\n",
    "clip_grad = 1.0\n",
    "seed = 42\n",
    "warmup_steps = 0\n",
    "min_lr_factor = 0.1\n",
    "decay_lr = True\n",
    "compile = True\n",
    "output_dir = \"output\"\n",
    "save_every = 10\n",
    "# eval_every = 100  # Evaluate every 5 epochs\n",
    "patience = 6   # Early stopping patience\n",
    "compile = True\n",
    "dataset_name = 'ETTm1'\n",
    "features = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patcher_backbone import patcher_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb36a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features, \n",
    "    seq_len=seq_len, \n",
    "    label_len=0, \n",
    "    pred_len=96, \n",
    "    flag='train', \n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    "    )\n",
    "\n",
    "validate_dataset, validate_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features, \n",
    "    seq_len=seq_len, \n",
    "    label_len=0, \n",
    "    pred_len=96, \n",
    "    flag='val', \n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    "    )\n",
    "\n",
    "print(f\"Dataset: {dataset_name}, Features: {features}, Batch Size: {batch_size}, Seq Len: {seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af17dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set model args\n",
    "model_args = ByteLatentTransformerArgs(\n",
    "    seed=42,\n",
    "    vocab_size=vocab_size,                       # Small byte-level vocab\n",
    "    max_length=seq_len,                        # Max full sequence length\n",
    "    max_seqlen=seq_len,\n",
    "    max_encoder_seq_length=seq_len,\n",
    "    local_attention_window_len=seq_len,        # Local window, 128 is sufficient for small models\n",
    "\n",
    "    dim_global=64,                        # Lower than default 512\n",
    "    dim_local_encoder=32,\n",
    "    dim_local_decoder=32,\n",
    "\n",
    "    n_layers_global=3,\n",
    "    n_layers_local_encoder=3,\n",
    "    n_layers_local_decoder=3,\n",
    "\n",
    "    n_heads_global=8,                      # Reduce heads\n",
    "    n_heads_local_encoder=4,\n",
    "    n_heads_local_decoder=4,\n",
    "\n",
    "    patch_size=8,\n",
    "    patch_in_forward=False,                # Patch in forward pass\n",
    "    patching_batch_size=256,\n",
    "    patching_device=\"cuda\",               # Use CPU for patching in small model\n",
    "    patching_mode=\"entropy\",\n",
    "    patching_threshold=3.0,\n",
    "    max_patch_length=16,\n",
    "    monotonicity=True,            # Monotonic patching\n",
    "    pad_to_max_length=True,\n",
    "\n",
    "    cross_attn_encoder=True,\n",
    "    cross_attn_decoder=True,\n",
    "    cross_attn_k=2,\n",
    "    cross_attn_nheads=2,\n",
    "    cross_attn_all_layers_encoder=True,\n",
    "    cross_attn_all_layers_decoder=True,\n",
    "    cross_attn_use_flex_attention=False,\n",
    "    cross_attn_init_by_pooling=True,\n",
    "\n",
    "    encoder_hash_byte_group_size=[4,5,6],   # Fewer hash sizes\n",
    "    encoder_hash_byte_group_vocab=2**6,\n",
    "    encoder_hash_byte_group_nb_functions=1,\n",
    "    encoder_enable_byte_ngrams=False,\n",
    "\n",
    "    non_linearity=\"swiglu\",\n",
    "    use_rope=True,\n",
    "    attn_impl=\"sdpa\",                      # Efficient PyTorch attention\n",
    "    attn_bias_type=\"causal\",\n",
    "\n",
    "    dropout=0.0,\n",
    "    layer_ckpt=\"none\",                     # No checkpointing in small model\n",
    "    init_use_gaussian=True,\n",
    "    init_use_depth=\"current\",\n",
    "    alpha_depth=\"disabled\",\n",
    "    log_patch_lengths=True,\n",
    "\n",
    "    downsampling_by_pooling=\"max\",         # Efficient downsampling\n",
    "    use_local_encoder_transformer=True,\n",
    "    share_encoder_decoder_emb=True         # Save memory if possible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb5d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 0.83M\n"
     ]
    }
   ],
   "source": [
    "# model = ByteLatentTransformer(model_args)\n",
    "# model = model.to(device)\n",
    "# if compile:\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "model = patcher_backbone\n",
    "\n",
    "# n of params in model in millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model_param_count = count_parameters(model)\n",
    "print(f\"Number of parameters in model: {model_param_count / 1e6:.2f}M\")\n",
    "\n",
    "patch_lengths = torch.full((batch_size,12), 8).to('cuda')\n",
    "#create_static_patch_lengths(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=5e-4, \n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.95)  # Use better beta values from first code\n",
    ")\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# torch.manual_seed(model_args.seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(model_args.seed)\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "# scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# print(f\"Using precision: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0138d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteLatentTransformer(\n",
       "  (local_encoder): LocalEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wo): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=32, out_features=256, bias=False)\n",
       "          (w3): Linear(in_features=32, out_features=256, bias=False)\n",
       "          (w2): Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=32, out_features=64, bias=False)\n",
       "    (tok_embeddings): Embedding(2048, 32)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-2): 3 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=32, out_features=32, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_transformer): GlobalTransformer(\n",
       "    (rope_embeddings): RotaryEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wk): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wv): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (wo): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w3): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (w2): Linear(in_features=256, out_features=64, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (local_decoder): LocalDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (wo): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=32, out_features=256, bias=False)\n",
       "          (w3): Linear(in_features=32, out_features=256, bias=False)\n",
       "          (w2): Linear(in_features=256, out_features=32, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=64, out_features=64, bias=False)\n",
       "    (norm): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-2): 3 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=32, out_features=32, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (output): Linear(in_features=32, out_features=2048, bias=False)\n",
       "    (prediction_head): Linear(in_features=3072, out_features=96, bias=True)\n",
       "  )\n",
       "  (encoder_hash_tok_embedding): ModuleList(\n",
       "    (0-2): 3 x Embedding(64, 32)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fceeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(96, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_lengths[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55758c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training with early stopping...\n",
      "üìù Configuration:\n",
      "   Max epochs: 500\n",
      "   Early stopping patience: 6\n",
      "   Save every: 10 epochs\n",
      "   WandB logging: Disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèÉ Epoch 1/500:   0%|          | 0/627 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (96) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m logits = model(token_ids.to(device)) \u001b[38;5;66;03m#, patch_lengths)\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# shape: [bs, pred_len]\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# target: [bs, pred_len]\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     74\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# loss = F.cross_entropy(\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m#     logits.reshape(-1, logits.size(-1)),\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m#     target_token_ids.reshape(-1).to(device),\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m#     ignore_index=PAD_ID\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     80\u001b[39m loss = loss / grad_accumulation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/nn/functional.py:3791\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3789\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3792\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.mse_loss(\n\u001b[32m   3793\u001b[39m     expanded_input, expanded_target, _Reduction.get_enum(reduction)\n\u001b[32m   3794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/functional.py:76\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (32) must match the size of tensor b (96) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training function with early stopping, periodic evaluation, and WandB logging\n",
    "\"\"\"\n",
    "\n",
    "# Initialize components\n",
    "tokenizer = build_tokenizer(\n",
    "    quant_range=quant_range,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=seq_len,\n",
    "    prediction_length=seq_len\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=1e-6)\n",
    "logger = TrainingLogger(output_dir, dataset_name, enable_wandb=ENABLE_WANDB)\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "total_steps = epochs * num_batches\n",
    "min_lr = learning_rate * min_lr_factor\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüöÄ Starting training with early stopping...\")\n",
    "print(f\"üìù Configuration:\")\n",
    "print(f\"   Max epochs: {epochs}\")\n",
    "print(f\"   Early stopping patience: {patience}\")\n",
    "print(f\"   Save every: {save_every} epochs\")\n",
    "print(f\"   WandB logging: {'Enabled' if ENABLE_WANDB else 'Disabled'}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    epoch_loss = 0\n",
    "    current_lr = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader), \n",
    "        desc=f\"üèÉ Epoch {epoch+1}/{epochs}\", \n",
    "        position=0, \n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for i, (batch_x, batch_y, _, _) in progress_bar:\n",
    "        iteration = epoch * num_batches + i\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = get_lr(iteration, total_steps, warmup_steps, learning_rate, min_lr, decay_lr)\n",
    "        current_lr = lr\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for micro_step in range(grad_accumulation_steps):\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            # target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to(device)) #, patch_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.mse_loss(\n",
    "                logits,                     # shape: [bs, pred_len]\n",
    "                y.to(device),               # target: [bs, pred_len]\n",
    "                reduction='mean'\n",
    "            )\n",
    "            # loss = F.cross_entropy(\n",
    "            #     logits.reshape(-1, logits.size(-1)),\n",
    "            #     target_token_ids.reshape(-1).to(device),\n",
    "            #     ignore_index=PAD_ID\n",
    "            # )\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "            # Log gradient norm to wandb periodically\n",
    "            if ENABLE_WANDB and i % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'train/grad_norm': grad_norm,\n",
    "                    'train/step': iteration,\n",
    "                    'train/batch_loss': total_loss\n",
    "                })\n",
    "        else:\n",
    "            grad_norm = 0\n",
    "            \n",
    "        # Update weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += total_loss\n",
    "        batch_losses.append(total_loss)\n",
    "        avg_epoch_loss = epoch_loss / (i + 1)\n",
    "        # run_val(patch_lengths)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{total_loss:.4f}\",\n",
    "            'avg_loss': f\"{avg_epoch_loss:.4f}\",\n",
    "            'lr': f\"{lr:.6f}\",\n",
    "            'patience': f\"{early_stopping.counter}/{patience}\"\n",
    "        })\n",
    "        # run_val(patch_lengths)\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_time = time.time() - t1\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    train_std_loss = np.std(batch_losses) if len(batch_losses) > 1 else 0\n",
    "    \n",
    "    # Validation phase\n",
    "    print(f\"\\nüîç Running validation for epoch {epoch+1}...\")\n",
    "    t1 = time.time()\n",
    "    model.eval()\n",
    "    # val_loss = validate(model, validate_loader, tokenizer, patch_lengths, device, \n",
    "    #                     desc=f\"Epoch {epoch+1} Validation\")\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"   Training Loss: {train_avg_loss:.6f} ¬± {train_std_loss:.6f} (Time: {train_time:.2f}s)\")\n",
    "    # print(f\"   Validation Loss: {val_loss:.6f} (Time: {val_time:.2f}s)\")\n",
    "    print(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd82fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.permute(0, 2, 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val(patch_lengths):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    with torch.no_grad():  # üö´ No gradients during evaluation\n",
    "        progress_bar = tqdm(\n",
    "            enumerate(validate_loader), \n",
    "            total=len(validate_loader), \n",
    "            desc=f\"üîç Validation Epoch {epoch+1}/{epochs}\", \n",
    "            position=0, \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for i, (batch_x, batch_y, _, _) in progress_bar:\n",
    "            x = batch_x.float().squeeze(-1).to(device)\n",
    "            y = batch_y.float().squeeze(-1).to(device)\n",
    "\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "            token_ids = token_ids.to(device)\n",
    "            if isinstance(patch_lengths, torch.Tensor):\n",
    "                patch_lengths = patch_lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to('cuda'), patch_lengths.to('cuda'))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(logits, y, reduction='mean')\n",
    "            epoch_loss += loss.item()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'val_loss': f\"{loss.item():.4f}\",\n",
    "                'avg_val_loss': f\"{epoch_loss / (i + 1):.4f}\",\n",
    "            })\n",
    "\n",
    "    val_avg_loss = epoch_loss / len(validate_loader)\n",
    "    val_std_loss = np.std(batch_losses) if len(batch_losses) > 1 else 0\n",
    "\n",
    "    print(f\"\\nüìä Validation Results for Epoch {epoch+1}:\")\n",
    "    print(f\"   Avg Loss: {val_avg_loss:.6f} ¬± {val_std_loss:.6f}\")  \n",
    "    return val_avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7408fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_val(patch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate best model\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "checkpoint = torch.load(os.path.join(output_dir, f\"best_model_{dataset_name}_{features}_{seq_len}.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Saved Val loss: {checkpoint['val_loss'].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periodic evaluation\n",
    "eval_results = None\n",
    "\n",
    "print(f\"\\nüéØ Running full evaluation at ...\")\n",
    "try:\n",
    "    eval_results = evaluation(\n",
    "        model, \n",
    "        dataset_name, \n",
    "        features,\n",
    "        quant_range,\n",
    "        vocab_size,\n",
    "        input_len=96,\n",
    "        pred_len=96,\n",
    "        eval_batch_size=batch_size,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"   üìà Evaluation completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Evaluation failed: {e}\")\n",
    "    eval_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff8d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blt_250513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
