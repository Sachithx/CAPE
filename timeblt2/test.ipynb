{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7c1300",
   "metadata": {},
   "source": [
    "## Patch TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ae366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0a0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9825720f",
   "metadata": {},
   "source": [
    "## Nano-GPT Entropy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fc660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f0677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54787a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bb42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35030fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4d1a7a",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e71c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # must be BEFORE torch/TF import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from bytelatent.model.blt import ByteLatentTransformerArgs, ByteLatentTransformer\n",
    "from chronos import MeanScaleUniformBins, ChronosConfig\n",
    "from utils.train_utils import *\n",
    "from TLLM_data_provider.data_factory import data_provider\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "torch.cuda.set_device(0)   # 0 here means “the first visible GPU”, i.e. physical #3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from bytelatent.tokenizers.constants import PAD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb35a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args\n",
    "vocab_size = 4096\n",
    "batch_size = 64\n",
    "seq_len = 96\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-2\n",
    "epochs = 10\n",
    "grad_accumulation_steps = 1\n",
    "clip_grad = 1.0\n",
    "seed = 42\n",
    "warmup_steps = 0\n",
    "min_lr_factor = 0.1\n",
    "decay_lr = True\n",
    "compile = True\n",
    "output_dir = \"output\"\n",
    "save_every = 1\n",
    "compile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f391d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Args\n",
    "dim = 256 # [64, 128, 256, 512]\n",
    "vocab_size=vocab_size\n",
    "max_length=seq_len\n",
    "local_attention_window_len=seq_len\n",
    "max_seqlen=seq_len\n",
    "max_encoder_seq_length=seq_len\n",
    "\n",
    "dim_local_encoder=256 # [64, 128, 256, 512]\n",
    "dim_local_decoder=256 # [64, 128, 256, 512]\n",
    "n_layers_global = 2 # [2, 4, 6, 8]\n",
    "n_layers_local_decoder=2 # [2, 4, 6, 8]\n",
    "n_layers_local_encoder=2 # [2, 4, 6, 8]\n",
    "cross_attn_k=2 # [2, 4, 6]\n",
    "cross_attn_nheads=2 # [2, 4, 6, 8]\n",
    "patch_size=8\n",
    "\n",
    "patching_mode=\"static\"\n",
    "tie_local_encoder_decoder_logits=False\n",
    "patch_in_forward=False\n",
    "pad_to_max_length=True\n",
    "patching_threshold=3.1439168453216553\n",
    "\n",
    "encoder_hash_byte_group_size=[4,5,6,7,8] \n",
    "encoder_hash_byte_group_vocab=2**15 # [2**12, 2**13, 2**14, 2**15, 2**16]\n",
    "encoder_hash_byte_group_nb_functions=1\n",
    "encoder_enable_byte_ngrams=False\n",
    "\n",
    "cross_attn_encoder=True\n",
    "cross_attn_decoder=True\n",
    "cross_attn_window_encoder=None\n",
    "cross_attn_window_decoder=None\n",
    "cross_attn_all_layers_decoder=True\n",
    "cross_attn_all_layers_encoder=True\n",
    "cross_attn_use_flex_attention=False\n",
    "cross_attn_init_by_pooling=True\n",
    "\n",
    "log_patch_lengths=True\n",
    "non_linearity=\"swiglu\"\n",
    "use_rope=True\n",
    "recompute_fc1_out=False\n",
    "recompute_fc3_out=False\n",
    "recompute_attn=False\n",
    "custom_bwd=False\n",
    "layer_ckpt=\"none\"\n",
    "use_local_encoder_transformer=True\n",
    "init_use_gaussian=True\n",
    "init_use_depth=\"current\"\n",
    "attn_impl=\"sdpa\"\n",
    "attn_bias_type=\"causal\"\n",
    "alpha_depth=\"disabled\"\n",
    "\n",
    "downsampling_by_pooling=\"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe1114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ByteLatentTransformerArgs(dim=512, n_layers=8, head_dim=None, n_heads=8, n_kv_heads=None, ffn_dim_multiplier=1.0, multiple_of=256, norm_eps=1e-05, rope_theta=10000.0, \n",
    "#                           rope_use_fp32_in_outer_product=False, init_base_std=None, init_std_factor=<InitStdFactor.DISABLED: 'disabled'>, max_seqlen=1024, attn_impl='sdpa', attn_bias_type='causal', \n",
    "#                           eos_id=2, seed=42, vocab_size=-1, weight_tying=False, patch_in_forward=False, dim_token=None, dim_global=512, dim_local_decoder=512, dim_local_encoder=512,\n",
    "#                             n_layers_global=8, n_layers_local_decoder=8, n_layers_local_encoder=8, patch_size=None, patching_mode=None, patching_threshold=None, patching_threshold_add=None, \n",
    "#                             monotonicity=False, patching_batch_size=None, patching_device='cuda', max_patch_length=None, tie_local_encoder_decoder_logits=False, use_local_encoder_transformer=False, \n",
    "#                             encoder_lm_loss=False, max_encoder_seq_length=None, pad_to_max_length=False, encoder_enable_byte_ngrams=False, encoder_enable_byte_group_hash=False, ngram_vocab_sizes=None, \n",
    "#                             cross_attn_encoder=False, cross_attn_decoder=False, cross_attn_window_encoder=None, cross_attn_window_decoder=None, cross_attn_k=None, cross_attn_nheads=None,\n",
    "#                               cross_attn_all_layers_decoder=False, cross_attn_all_layers_encoder=False, cross_attn_use_flex_attention=True, cross_attn_init_by_pooling=False, encoder_hash_byte_group_size=None,\n",
    "#                               encoder_hash_byte_group_vocab=30000, encoder_hash_byte_group_nb_functions=3, log_patch_lengths=False, non_linearity='swiglu', use_rope=True, recompute_fc1_out=False, recompute_fc3_out=False,\n",
    "#                                 recompute_attn=True, custom_bwd=False, layer_ckpt='all', init_use_gaussian=True, init_use_depth='current', alpha_depth='disabled', max_length=2048, norm_affine=True, pre_norm=True,\n",
    "#                                   norm_type='rmsnorm', dropout=0, output_size=-1, architecture='vanilla', share_encoder_decoder_emb=True, global_local_decoder_residual_layer=None, tokenize_with_bpe_delimiter=False,\n",
    "#                                     patching_thresholds_str=None, tie_local_encoder_decoder=False, encoder_preds_low_entropy_toks=None, encoder_preds_random_toks=None, dim_token_emb=None, dim_patch_emb=None,\n",
    "#                                       encoder_ngram_table_dir=None, encoder_ngram_to_size_str=None, entropy_model_checkpoint_dir=None, entropy_model_is_ngram_model=False, downsampling_by_pooling=None, n_heads_global=8, \n",
    "#                                       n_heads_local_decoder=8, n_heads_local_encoder=8, n_kv_heads_global=None, conv_kernel_size=None, local_attention_window_len=None, \n",
    "#                           sequence_parallel=False, loss_parallel=False, fuse_sequence_parallel=False, use_fsdp=True, attn_to_keep='all', pm_size=0, full_logging_n_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ByteLatentTransformerArgs(\n",
    "    seed=seed,\n",
    "    vocab_size=vocab_size,\n",
    "    dim=dim, \n",
    "    n_layers_global=n_layers_global,  \n",
    "    n_layers_local_decoder=n_layers_local_decoder, \n",
    "    n_layers_local_encoder=n_layers_local_encoder,  \n",
    "    patch_size=patch_size,\n",
    "    patching_mode=patching_mode,\n",
    "    tie_local_encoder_decoder_logits=tie_local_encoder_decoder_logits,\n",
    "    patch_in_forward=patch_in_forward,\n",
    "    max_encoder_seq_length=max_encoder_seq_length,\n",
    "    pad_to_max_length=pad_to_max_length,\n",
    "    patching_threshold=patching_threshold,\n",
    "    encoder_hash_byte_group_size=encoder_hash_byte_group_size,\n",
    "    encoder_hash_byte_group_vocab=encoder_hash_byte_group_vocab,\n",
    "    encoder_hash_byte_group_nb_functions=encoder_hash_byte_group_nb_functions,\n",
    "    encoder_enable_byte_ngrams=encoder_enable_byte_ngrams,\n",
    "    cross_attn_encoder=cross_attn_encoder,\n",
    "    cross_attn_decoder=cross_attn_decoder,\n",
    "    cross_attn_window_encoder=cross_attn_window_encoder,\n",
    "    cross_attn_window_decoder=cross_attn_window_decoder,\n",
    "    dim_local_encoder=dim_local_encoder,\n",
    "    dim_local_decoder=dim_local_decoder,\n",
    "    cross_attn_k=cross_attn_k,   \n",
    "    cross_attn_nheads=cross_attn_nheads,  \n",
    "    cross_attn_all_layers_decoder=cross_attn_all_layers_decoder,\n",
    "    cross_attn_all_layers_encoder=cross_attn_all_layers_encoder,\n",
    "    cross_attn_use_flex_attention=cross_attn_use_flex_attention,\n",
    "    cross_attn_init_by_pooling=cross_attn_init_by_pooling,\n",
    "    log_patch_lengths=log_patch_lengths,\n",
    "    non_linearity=non_linearity,\n",
    "    use_rope=use_rope,\n",
    "    recompute_fc1_out=recompute_fc1_out,\n",
    "    recompute_fc3_out=recompute_fc3_out,\n",
    "    recompute_attn=recompute_attn,\n",
    "    custom_bwd=custom_bwd,\n",
    "    layer_ckpt=layer_ckpt,\n",
    "    use_local_encoder_transformer=use_local_encoder_transformer,\n",
    "    init_use_gaussian=init_use_gaussian,\n",
    "    init_use_depth=init_use_depth,\n",
    "    attn_impl=attn_impl,\n",
    "    attn_bias_type=attn_bias_type,\n",
    "    alpha_depth=alpha_depth,\n",
    "    max_length=max_length,\n",
    "    local_attention_window_len=local_attention_window_len,\n",
    "    max_seqlen=max_seqlen,\n",
    "    downsampling_by_pooling=downsampling_by_pooling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c457a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 55.71M\n"
     ]
    }
   ],
   "source": [
    "model = ByteLatentTransformer(model_args)\n",
    "model = model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# n of params in model in millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in model: {count_parameters(model) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa25940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 336 > 42, 8\n",
    "# 96 > 12, 8\n",
    "# 512 > 64, 8\n",
    "\n",
    "l = torch.full((batch_size,12), 8).to('cuda')\n",
    "l[:,0] = 1\n",
    "l[:,-1] = 10\n",
    "l[:,2] = 10\n",
    "l[:,1] = 11\n",
    "patch_lengths = l\n",
    "patch_lengths.shape, patch_lengths[0].sum().item()\n",
    "assert  patch_lengths[0].sum().item() == seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5925f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_lengths[0].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82badd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=5e-4, \n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.95)  # Use better beta values from first code\n",
    "    )\n",
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafa6cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precision: bfloat16\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(model_args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(model_args.seed)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "print(f\"Using precision: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288efad9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d755fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new config with prediction_length=1\n",
    "train_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=96,\n",
    "    prediction_length=96,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, train_tokenizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b10975",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262251a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': 'ETTm2',\n",
    "    'data' : 'ETTm2',\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': 'ETTm2.csv',\n",
    "    'features': 'S',\n",
    "    'seq_len': seq_len,\n",
    "    'label_len': seq_len - 1,\n",
    "    'pred_len': 1\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=batch_size,\n",
    "    freq='m',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=True,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "train_dataset, train_loader = data_provider(args, config, flag='train')\n",
    "validate_dataset, validate_loader = data_provider(args, config, flag='val')\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d693cba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 96, 1])\n",
      "torch.Size([64, 96, 1])\n",
      "torch.Size([64, 96, 1])\n",
      "torch.Size([64, 96, 1])\n"
     ]
    }
   ],
   "source": [
    "# sample a batch\n",
    "for i, (x, y, x_mark, y_mark) in enumerate(train_loader):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x_mark.shape)\n",
    "    print(y_mark.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7f725e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 96]), torch.Size([64, 96]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    batch_x_mark = batch_x_mark.to(device)\n",
    "    batch_y_mark = batch_y_mark.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "x.shape, y.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d172fd",
   "metadata": {},
   "source": [
    "## Entropy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0aec65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def calculate_entropies_optimized(\n",
    "    tokens: torch.tensor,\n",
    "    entropy_model,\n",
    "    patching_batch_size,\n",
    "    device: str | None = None,\n",
    "    enable_grad: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized version for T5/encoder-decoder models using vectorized computation.\n",
    "    tokens: 2D tensor of shape [batch_size, seq_len]\n",
    "    Return 2D tensor of shape [batch_size, seq_len] with entropies for each token.\n",
    "    \"\"\"\n",
    "    grad_context = torch.no_grad() if not enable_grad else torch.nullcontext()\n",
    "    \n",
    "    with grad_context:\n",
    "        entropies = []\n",
    "        max_length = getattr(entropy_model, \"max_length\", 512)\n",
    "        batch_numel = max_length * patching_batch_size\n",
    "        splits = torch.split(tokens.flatten(), batch_numel)\n",
    "        vocab_size = entropy_model.config.n_tokens\n",
    "        decoder_start_token_id = 0\n",
    "        \n",
    "        for split in splits:\n",
    "            pad_size = (max_length - (split.numel() % max_length)) % max_length\n",
    "            pad = torch.zeros(\n",
    "                pad_size, dtype=split.dtype, device=split.device, requires_grad=False\n",
    "            )\n",
    "            split = torch.cat((split, pad), dim=0)\n",
    "            split = split.reshape(-1, max_length)  # shape [B, L]\n",
    "            \n",
    "            if device is not None:\n",
    "                split = split.to(device)\n",
    "            \n",
    "            B, L = split.shape\n",
    "            \n",
    "            # Vectorized approach: create all contexts at once\n",
    "            # Instead of processing each position sequentially, we'll batch multiple positions\n",
    "            chunk_entropies = []\n",
    "            \n",
    "            # Process in smaller chunks to manage memory\n",
    "            position_chunk_size = min(16, L-1)  # Process multiple positions at once\n",
    "            \n",
    "            for pos_start in range(1, L, position_chunk_size):\n",
    "                pos_end = min(pos_start + position_chunk_size, L)\n",
    "                \n",
    "                # Create batched contexts for positions pos_start to pos_end\n",
    "                contexts = []\n",
    "                attention_masks = []\n",
    "                decoder_inputs = []\n",
    "                \n",
    "                for pos in range(pos_start, pos_end):\n",
    "                    context = split[:, :pos]  # tokens 0 to pos-1\n",
    "                    attention_mask = torch.ones_like(context, dtype=torch.bool, device=device)\n",
    "                    decoder_input_ids = torch.full(\n",
    "                        (B, 1), decoder_start_token_id, dtype=torch.long, device=device\n",
    "                    )\n",
    "                    \n",
    "                    contexts.append(context)\n",
    "                    attention_masks.append(attention_mask)\n",
    "                    decoder_inputs.append(decoder_input_ids)\n",
    "                \n",
    "                # Batch the contexts by expanding batch dimension\n",
    "                # We'll process all positions for all sequences together\n",
    "                all_logits = []\n",
    "                \n",
    "                for i, (context, attention_mask, decoder_input_ids) in enumerate(\n",
    "                    zip(contexts, attention_masks, decoder_inputs)\n",
    "                ):\n",
    "                    with torch.no_grad():\n",
    "                        outputs = entropy_model.model(\n",
    "                            input_ids=context.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            decoder_input_ids=decoder_input_ids,\n",
    "                            output_hidden_states=False,\n",
    "                            output_attentions=False,\n",
    "                            return_dict=True,\n",
    "                        )\n",
    "                        logits = outputs.logits[:, -1, :]  # shape: [B, vocab]\n",
    "                        all_logits.append(logits)\n",
    "                \n",
    "                # Stack logits for this chunk\n",
    "                if all_logits:\n",
    "                    chunk_logits = torch.stack(all_logits, dim=1)  # [B, chunk_size, vocab]\n",
    "                    chunk_entropy = entropy(chunk_logits)  # [B, chunk_size]\n",
    "                    chunk_entropies.append(chunk_entropy)\n",
    "            \n",
    "            # Concatenate all position chunks\n",
    "            if chunk_entropies:\n",
    "                split_entropies = torch.cat(chunk_entropies, dim=1)  # [B, L-1]\n",
    "                \n",
    "                # Add zero entropy for first position\n",
    "                first_pos_entropy = torch.zeros(B, 1, device=device)\n",
    "                split_entropies = torch.cat([first_pos_entropy, split_entropies], dim=1)\n",
    "            else:\n",
    "                split_entropies = torch.zeros(B, L, device=device)\n",
    "            \n",
    "            # Flatten and remove padding\n",
    "            split_entropies = split_entropies.flatten()[:split.numel() - pad_size]\n",
    "            entropies.append(split_entropies)\n",
    "        \n",
    "        # Concatenate all splits and reshape to original shape\n",
    "        concat_entropies = torch.cat(entropies, dim=0)\n",
    "        concat_entropies = concat_entropies.reshape(tokens.shape)\n",
    "        \n",
    "    return concat_entropies\n",
    "\n",
    "def calculate_entropies_fully_vectorized(\n",
    "    tokens: torch.tensor,\n",
    "    entropy_model,\n",
    "    patching_batch_size,\n",
    "    device: str | None = None,\n",
    "    enable_grad: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fully vectorized version that processes multiple positions simultaneously.\n",
    "    Most aggressive optimization for T5/encoder-decoder models.\n",
    "    \"\"\"\n",
    "    grad_context = torch.no_grad() if not enable_grad else torch.nullcontext()\n",
    "    \n",
    "    with grad_context:\n",
    "        entropies = []\n",
    "        max_length = getattr(entropy_model, \"max_length\", 512)\n",
    "        batch_numel = max_length * patching_batch_size\n",
    "        splits = torch.split(tokens.flatten(), batch_numel)\n",
    "        vocab_size = entropy_model.config.n_tokens\n",
    "        decoder_start_token_id = 0\n",
    "        \n",
    "        for split in splits:\n",
    "            pad_size = (max_length - (split.numel() % max_length)) % max_length\n",
    "            pad = torch.zeros(\n",
    "                pad_size, dtype=split.dtype, device=split.device, requires_grad=False\n",
    "            )\n",
    "            split = torch.cat((split, pad), dim=0)\n",
    "            split = split.reshape(-1, max_length)  # shape [B, L]\n",
    "            \n",
    "            if device is not None:\n",
    "                split = split.to(device)\n",
    "            \n",
    "            B, L = split.shape\n",
    "            \n",
    "            # Create all contexts at once by stacking them\n",
    "            max_context_len = L - 1\n",
    "            \n",
    "            # Pre-allocate tensors for all contexts\n",
    "            all_contexts = []\n",
    "            all_attention_masks = []\n",
    "            position_mapping = []\n",
    "            \n",
    "            for seq_idx in range(B):\n",
    "                for pos in range(1, L):\n",
    "                    context = split[seq_idx:seq_idx+1, :pos]  # [1, pos]\n",
    "                    attention_mask = torch.ones_like(context, dtype=torch.bool, device=device)\n",
    "                    \n",
    "                    all_contexts.append(context)\n",
    "                    all_attention_masks.append(attention_mask)\n",
    "                    position_mapping.append((seq_idx, pos))\n",
    "            \n",
    "            if not all_contexts:\n",
    "                split_entropies = torch.zeros(B, L, device=device)\n",
    "            else:\n",
    "                # Pad all contexts to the same length for efficient batching\n",
    "                padded_contexts = torch.nn.utils.rnn.pad_sequence(\n",
    "                    [ctx.squeeze(0) for ctx in all_contexts], \n",
    "                    batch_first=True, \n",
    "                    padding_value=0\n",
    "                )\n",
    "                \n",
    "                padded_attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "                    [mask.squeeze(0) for mask in all_attention_masks],\n",
    "                    batch_first=True,\n",
    "                    padding_value=False\n",
    "                )\n",
    "                \n",
    "                # Create decoder input ids for all contexts\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (len(all_contexts), 1), \n",
    "                    decoder_start_token_id, \n",
    "                    dtype=torch.long, \n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                # Process all contexts in one forward pass\n",
    "                with torch.no_grad():\n",
    "                    outputs = entropy_model.model(\n",
    "                        input_ids=padded_contexts,\n",
    "                        attention_mask=padded_attention_masks,\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        output_hidden_states=False,\n",
    "                        output_attentions=False,\n",
    "                        return_dict=True,\n",
    "                    )\n",
    "                    \n",
    "                    logits = outputs.logits[:, -1, :]  # [num_contexts, vocab]\n",
    "                    context_entropies = entropy(logits)  # [num_contexts]\n",
    "                \n",
    "                # Map back to original positions\n",
    "                split_entropies = torch.zeros(B, L, device=device)\n",
    "                for i, (seq_idx, pos) in enumerate(position_mapping):\n",
    "                    split_entropies[seq_idx, pos] = context_entropies[i]\n",
    "            \n",
    "            # Flatten and remove padding\n",
    "            split_entropies = split_entropies.flatten()[:split.numel() - pad_size]\n",
    "            entropies.append(split_entropies)\n",
    "        \n",
    "        # Concatenate all splits and reshape to original shape\n",
    "        concat_entropies = torch.cat(entropies, dim=0)\n",
    "        concat_entropies = concat_entropies.reshape(tokens.shape)\n",
    "        \n",
    "    return concat_entropies\n",
    "\n",
    "def calculate_entropies_kv_cache(\n",
    "    tokens: torch.tensor,\n",
    "    entropy_model,\n",
    "    patching_batch_size,\n",
    "    device: str | None = None,\n",
    "    enable_grad: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Version using KV caching for incremental computation.\n",
    "    Most efficient for very long sequences.\n",
    "    \"\"\"\n",
    "    if not hasattr(entropy_model.model, 'generate'):\n",
    "        # Fallback to batch method if KV cache not supported\n",
    "        return calculate_entropies_optimized(tokens, entropy_model, patching_batch_size, device, enable_grad)\n",
    "    \n",
    "    grad_context = torch.no_grad() if not enable_grad else torch.nullcontext()\n",
    "    \n",
    "    with grad_context:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        vocab_size = entropy_model.config.n_tokens\n",
    "        \n",
    "        if device is not None:\n",
    "            tokens = tokens.to(device)\n",
    "        \n",
    "        entropies = torch.zeros_like(tokens, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Process each sequence in the batch\n",
    "        for batch_idx in range(batch_size):\n",
    "            sequence = tokens[batch_idx:batch_idx+1]  # Keep batch dimension\n",
    "            past_key_values = None\n",
    "            \n",
    "            for pos in range(1, seq_len):\n",
    "                # Use only the current token for input (with KV cache)\n",
    "                if pos == 1:\n",
    "                    input_ids = sequence[:, :pos]\n",
    "                else:\n",
    "                    input_ids = sequence[:, pos-1:pos]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = entropy_model.model(\n",
    "                        input_ids=input_ids,\n",
    "                        past_key_values=past_key_values,\n",
    "                        use_cache=True,\n",
    "                        return_dict=True,\n",
    "                    )\n",
    "                    \n",
    "                    logits = outputs.logits[:, -1, :]  # Last position logits\n",
    "                    past_key_values = outputs.past_key_values\n",
    "                    \n",
    "                    # Calculate entropy for current position\n",
    "                    token_entropy = entropy(logits.unsqueeze(1)).squeeze(1)\n",
    "                    entropies[batch_idx, pos] = token_entropy\n",
    "        \n",
    "    return entropies\n",
    "\n",
    "def calculate_entropies_parallel_chunks(\n",
    "    tokens: torch.tensor,\n",
    "    entropy_model,\n",
    "    patching_batch_size,\n",
    "    device: str | None = None,\n",
    "    enable_grad: bool = False,\n",
    "    num_chunks: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process multiple chunks in parallel using different GPU streams.\n",
    "    Requires multiple GPUs or careful memory management.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n",
    "        # Fallback to optimized version\n",
    "        return calculate_entropies_optimized(tokens, entropy_model, patching_batch_size, device, enable_grad)\n",
    "    \n",
    "    grad_context = torch.no_grad() if not enable_grad else torch.nullcontext()\n",
    "    \n",
    "    with grad_context:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        chunk_size = seq_len // num_chunks\n",
    "        \n",
    "        streams = [torch.cuda.Stream() for _ in range(num_chunks)]\n",
    "        chunk_results = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size if i < num_chunks - 1 else seq_len\n",
    "            \n",
    "            with torch.cuda.stream(streams[i]):\n",
    "                chunk_tokens = tokens[:, start_idx:end_idx].to(f'cuda:{i % torch.cuda.device_count()}')\n",
    "                \n",
    "                # Process chunk (similar to optimized version)\n",
    "                outputs = entropy_model.model(\n",
    "                    input_ids=chunk_tokens,\n",
    "                    attention_mask=torch.ones_like(chunk_tokens, dtype=torch.bool),\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits[:, :-1, :]\n",
    "                chunk_entropies = entropy(logits)\n",
    "                \n",
    "                # Add padding for first position\n",
    "                first_pos = torch.zeros(batch_size, 1, device=chunk_tokens.device)\n",
    "                chunk_entropies = torch.cat([first_pos, chunk_entropies], dim=1)\n",
    "                \n",
    "                chunk_results.append(chunk_entropies)\n",
    "        \n",
    "        # Synchronize all streams\n",
    "        for stream in streams:\n",
    "            stream.synchronize()\n",
    "        \n",
    "        # Concatenate results\n",
    "        final_entropies = torch.cat([r.to(device) for r in chunk_results], dim=1)\n",
    "        \n",
    "    return final_entropies\n",
    "\n",
    "# Additional utility function for entropy calculation\n",
    "def entropy(scores):\n",
    "    \"\"\"\n",
    "    Vectorized entropy calculation with numerical stability.\n",
    "    scores: [..., vocab_size]\n",
    "    returns [...] (same shape as input except last dimension)\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(scores, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    # Numerical stability: handle log(0) cases\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db7b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from typing import Optional, List, Tuple, Dict, Union\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "from chronos import ChronosModel, ChronosConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from bytelatent.transformer import LMTransformer, LMTransformerArgs\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def load_entropy_model(entropy_model_checkpoint_dir=\"amazon/chronos-t5-tiny\", state_dict_path=\"no_need\", device=\"cuda\"):\n",
    "\n",
    "    model_path = entropy_model_checkpoint_dir\n",
    "    torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        device = device\n",
    "\n",
    "    # Load Chronos model\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    chronos_config = ChronosConfig(**config.chronos_config)\n",
    "    pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    entropy_model = ChronosModel(config=chronos_config, model=pretrained_model)\n",
    "    entropy_model.to(device)\n",
    "    entropy_model = entropy_model.eval()\n",
    "\n",
    "    return entropy_model\n",
    "\n",
    "def entropy(scores):\n",
    "    \"\"\"\n",
    "    scores: [bs, seq_len, vocab]\n",
    "    returns [bs, seq_len]\n",
    "\n",
    "    Computes the entropy for each token in the batch.\n",
    "    Note: uses natural log.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(scores, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    p_log_p = log_probs * probs\n",
    "    entropy = -p_log_p.sum(dim=-1)\n",
    "    return entropy\n",
    "\n",
    "def calculate_entropies(\n",
    "    tokens: torch.tensor,\n",
    "    entropy_model,\n",
    "    patching_batch_size,\n",
    "    device: str | None = None,\n",
    "    enable_grad: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    tokens: 2D tensor of shape [batch_size, seq_len]\n",
    "    Return 2D tensor of shape [batch_size, seq_len] with entropies for each token.\n",
    "\n",
    "    Splits the tokens into chunks of size max_length and calculates entropies for each chunk.\n",
    "    Entropy model can be executed on cpu or gpu, specify either 'cuda' or 'cpu' in the device argument.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_context = nullcontext() if enable_grad else torch.no_grad()\n",
    "\n",
    "    with grad_context:\n",
    "        entropies = []\n",
    "        preds = []\n",
    "        max_length = getattr(entropy_model, \"max_length\", 256)\n",
    "        batch_numel = max_length * patching_batch_size\n",
    "        splits = torch.split(tokens.flatten(), batch_numel)\n",
    "        vocab_size = entropy_model.config.n_tokens\n",
    "        decoder_start_token_id = 0\n",
    "        # Save original shape\n",
    "        # original_shape = tokens.shape\n",
    "        # vocab_size = entropy_model.config.n_tokens\n",
    "\n",
    "        # Flatten the tokens for uniform processing\n",
    "        # flat_tokens = tokens.flatten()\n",
    "        # splits = torch.split(tokens.flatten(), batch_numel)\n",
    "\n",
    "        for split in splits:\n",
    "            pad_size = (max_length - (split.numel() % max_length)) % max_length\n",
    "            pad = torch.zeros(\n",
    "                    pad_size, dtype=split.dtype, device=split.device, requires_grad=False\n",
    "                )\n",
    "            split = torch.cat((split, pad), dim=0)\n",
    "            split = split.reshape(-1, max_length) # shape [B, L]\n",
    "            if device is not None:\n",
    "                split = split.to(device)\n",
    "            # if pad_size > 0:\n",
    "            #     split = torch.cat([split, torch.zeros(pad_size, dtype=split.dtype, device=split.device, requires_grad=False)])\n",
    "\n",
    "            \n",
    "            B, L = split.shape\n",
    "\n",
    "            # Allocate per-split outputs\n",
    "            # entropy_per_split = torch.zeros((B, L), device=device)\n",
    "            pred = torch.zeros((B, L, vocab_size), device=device)\n",
    "\n",
    "            for i in range(1, L):\n",
    "                context = split[:, :i]  # tokens 0 to i-1\n",
    "                attention_mask = torch.ones_like(context, dtype=torch.bool, device=device)\n",
    "\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (B, 1),\n",
    "                    decoder_start_token_id,\n",
    "                    dtype=torch.long,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = entropy_model.model(\n",
    "                        input_ids=context.to(device),\n",
    "                        attention_mask=attention_mask.to(device),\n",
    "                        decoder_input_ids=decoder_input_ids,\n",
    "                        output_hidden_states=False,\n",
    "                        output_attentions=False,\n",
    "                        return_dict=True,\n",
    "                    )\n",
    "                    logits = outputs.logits[:, -1, :]  # shape: [B, vocab]\n",
    "                    pred[:, i, :] = logits\n",
    "\n",
    "            pred = pred.reshape(-1, pred.shape[-1])[\n",
    "                        : split.numel() - pad_size, :\n",
    "                    ]  # [batch_size * seq_len, vocab]\n",
    "\n",
    "            pred_entropies = entropy(pred)\n",
    "            entropies.append(pred_entropies)\n",
    "            preds.append(pred)\n",
    "        concat_entropies = torch.cat(entropies, dim=0)\n",
    "        concat_entropies = concat_entropies.reshape(tokens.shape)\n",
    "        concat_preds = torch.cat(preds, dim=0)\n",
    "        concat_preds = concat_preds.reshape(tokens.shape[0], -1)\n",
    "    return concat_entropies, concat_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "504d34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randint(3, 100, (128, 96), device='cuda')\n",
    "entropy_model = load_entropy_model()\n",
    "patching_batch_size = 32\n",
    "device = 'cuda'\n",
    "enable_grad = False\n",
    "\n",
    "vocab_size = entropy_model.config.n_tokens\n",
    "entropy_model = load_entropy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebf8f755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8.3750, 7.0000, 6.8125,  ..., 6.7812, 6.7812, 6.7812],\n",
       "         [6.8125, 6.8438, 6.8125,  ..., 6.8125, 6.8125, 6.8438],\n",
       "         [6.7500, 6.7812, 6.8125,  ..., 6.6875, 6.7188, 6.6875],\n",
       "         ...,\n",
       "         [6.7500, 6.8438, 6.8438,  ..., 6.8125, 6.7500, 6.8125],\n",
       "         [6.7812, 6.8125, 6.7188,  ..., 6.8125, 6.7812, 6.8438],\n",
       "         [6.8750, 6.8125, 6.8125,  ..., 6.7812, 6.8125, 6.8438]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0.0000, 0.0000, 0.0000,  ..., 2.2812, 2.7969, 0.5039],\n",
       "         [2.8906, 0.5234, 3.3594,  ..., 2.1250, 2.6250, 0.4297],\n",
       "         [2.8750, 0.5742, 3.2812,  ..., 1.7656, 2.2656, 0.2793],\n",
       "         ...,\n",
       "         [2.8281, 0.6172, 3.1094,  ..., 2.0781, 2.5938, 0.4512],\n",
       "         [2.8594, 0.6641, 3.0938,  ..., 2.1875, 2.6875, 0.4941],\n",
       "         [2.8594, 0.6328, 3.2969,  ..., 2.1094, 2.6094, 0.4492]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "e  = calculate_entropies(\n",
    "    tokens=tokens,\n",
    "    entropy_model=entropy_model,\n",
    "    patching_batch_size=patching_batch_size,\n",
    "    device=device,\n",
    "    enable_grad=False\n",
    ")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "264ea1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 6.8125, 6.2812,  ..., 6.7812, 6.7812, 6.8125],\n",
       "        [6.7500, 6.7188, 6.7500,  ..., 6.7188, 6.7812, 6.7812],\n",
       "        [6.7812, 6.7500, 6.7812,  ..., 6.7812, 6.7812, 6.7188],\n",
       "        ...,\n",
       "        [6.8125, 6.8438, 6.7812,  ..., 6.7812, 6.7812, 6.8125],\n",
       "        [6.7500, 6.7500, 6.8125,  ..., 6.8125, 6.7500, 6.8125],\n",
       "        [6.8125, 6.7812, 6.7500,  ..., 6.7812, 6.7812, 6.8125]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159dbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\n",
    "#     \"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\n",
    "# )\n",
    "# context=torch.tensor(df[\"#Passengers\"])\n",
    "# tokens,_,_ = tokenizer.context_input_transform(context.unsqueeze(0).cpu())\n",
    "# tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_length = getattr(entropy_model, \"max_length\", 512)\n",
    "batch_numel = max_length * patching_batch_size\n",
    "device = 'cuda'\n",
    "\n",
    "epsilon = 1e-10\n",
    "\n",
    "decoder_start_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab3ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens, entropy_model, patching_batch_size, device, enable_grad\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cacc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d9054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 336])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4742b081",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m entropy = \u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m entropy.shape\n",
      "\u001b[31mTypeError\u001b[39m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "entropy = entropy(pred)\n",
    "entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000,  ..., 2.7031, 3.1875, 0.5352], device='cuda:0')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_valid_tokens = split.numel() - pad_len\n",
    "pred = pred.view(-1)[:total_valid_tokens] \n",
    "pred_per_split = pred_per_split.view(-1, vocab_size)[:total_valid_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8515e78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3750, 6.8125, 6.6562,  ..., 6.8125, 6.7500, 6.7188],\n",
       "        [8.3750, 6.4062, 6.8438,  ..., 6.7812, 6.7500, 6.7500],\n",
       "        [8.3750, 6.6562, 7.0000,  ..., 6.7812, 6.7812, 6.7812],\n",
       "        ...,\n",
       "        [8.3750, 6.9375, 6.9062,  ..., 6.7500, 6.8125, 6.7812],\n",
       "        [8.3750, 6.9688, 6.8750,  ..., 6.8125, 6.7812, 6.7500],\n",
       "        [8.3750, 6.9375, 6.9688,  ..., 6.7500, 6.7812, 6.8125]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_entropy(pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3fe172f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.7500e+00,  5.2344e-01,  3.0781e+00,  ...,  1.8438e+00,\n",
       "           2.3750e+00,  4.6631e-02],\n",
       "         [ 2.6406e+00,  2.9297e-01,  2.9688e+00,  ...,  1.7891e+00,\n",
       "           2.2812e+00,  2.6172e-01],\n",
       "         ...,\n",
       "         [ 2.8438e+00,  6.3281e-01,  3.2656e+00,  ...,  2.0781e+00,\n",
       "           2.5781e+00,  3.5547e-01],\n",
       "         [ 2.8281e+00,  6.5625e-01,  3.2344e+00,  ...,  2.0469e+00,\n",
       "           2.5469e+00,  3.6523e-01],\n",
       "         [ 2.8438e+00,  6.2109e-01,  3.2031e+00,  ...,  2.0312e+00,\n",
       "           2.5312e+00,  3.5547e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.4531e+00,  5.1172e-01,  1.3047e+00,  ...,  7.3828e-01,\n",
       "           1.2422e+00, -2.6001e-02],\n",
       "         [ 2.8906e+00,  4.2969e-01,  3.6406e+00,  ...,  2.5312e+00,\n",
       "           3.0000e+00,  6.6016e-01],\n",
       "         ...,\n",
       "         [ 2.8594e+00,  6.9141e-01,  3.1562e+00,  ...,  2.0469e+00,\n",
       "           2.5469e+00,  4.2383e-01],\n",
       "         [ 2.8594e+00,  6.8359e-01,  3.2188e+00,  ...,  2.0781e+00,\n",
       "           2.5781e+00,  4.1016e-01],\n",
       "         [ 2.8594e+00,  6.6406e-01,  3.1562e+00,  ...,  2.0312e+00,\n",
       "           2.5312e+00,  3.9062e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.7969e+00,  4.8828e-01,  2.8906e+00,  ...,  1.3281e+00,\n",
       "           1.8750e+00, -6.8359e-02],\n",
       "         [ 2.7344e+00,  3.3569e-03,  3.9219e+00,  ...,  2.7188e+00,\n",
       "           3.1875e+00,  1.2031e+00],\n",
       "         ...,\n",
       "         [ 2.8281e+00,  6.4062e-01,  3.1250e+00,  ...,  2.0156e+00,\n",
       "           2.5156e+00,  4.4727e-01],\n",
       "         [ 2.8281e+00,  6.7188e-01,  3.0781e+00,  ...,  1.9922e+00,\n",
       "           2.4844e+00,  4.3164e-01],\n",
       "         [ 2.8438e+00,  6.7578e-01,  3.1406e+00,  ...,  2.0469e+00,\n",
       "           2.5469e+00,  4.4922e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.5625e+00,  1.1250e+00,  5.7031e-01,  ...,  6.2500e-01,\n",
       "           1.1016e+00,  7.7734e-01],\n",
       "         [ 2.7969e+00,  6.7969e-01,  3.1250e+00,  ...,  2.0312e+00,\n",
       "           2.5312e+00,  4.3555e-01],\n",
       "         ...,\n",
       "         [ 2.8750e+00,  6.4062e-01,  3.2656e+00,  ...,  2.1250e+00,\n",
       "           2.6094e+00,  4.1797e-01],\n",
       "         [ 2.8594e+00,  6.3672e-01,  3.2344e+00,  ...,  2.0625e+00,\n",
       "           2.5781e+00,  3.9648e-01],\n",
       "         [ 2.8750e+00,  6.5234e-01,  3.2500e+00,  ...,  2.0938e+00,\n",
       "           2.5938e+00,  4.0039e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.6094e+00,  7.6172e-01,  2.2656e+00,  ...,  1.4375e+00,\n",
       "           1.9141e+00,  1.0547e-01],\n",
       "         [ 2.5781e+00,  4.0625e-01,  2.5625e+00,  ...,  1.5156e+00,\n",
       "           2.0469e+00, -5.5420e-02],\n",
       "         ...,\n",
       "         [ 2.8281e+00,  6.4062e-01,  3.1875e+00,  ...,  2.0312e+00,\n",
       "           2.5312e+00,  3.6523e-01],\n",
       "         [ 2.8438e+00,  6.4844e-01,  3.1875e+00,  ...,  2.0312e+00,\n",
       "           2.5156e+00,  3.5938e-01],\n",
       "         [ 2.8438e+00,  6.3281e-01,  3.1875e+00,  ...,  2.0156e+00,\n",
       "           2.5156e+00,  3.3398e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 3.0312e+00,  6.6797e-01,  3.9062e+00,  ...,  2.8906e+00,\n",
       "           3.3281e+00,  1.0547e+00],\n",
       "         [ 2.8750e+00,  3.1055e-01,  3.6250e+00,  ...,  2.6094e+00,\n",
       "           3.0625e+00,  8.4375e-01],\n",
       "         ...,\n",
       "         [ 2.8281e+00,  6.5234e-01,  3.0312e+00,  ...,  1.9609e+00,\n",
       "           2.4531e+00,  4.1016e-01],\n",
       "         [ 2.8438e+00,  6.5234e-01,  3.0938e+00,  ...,  1.9922e+00,\n",
       "           2.5000e+00,  3.8281e-01],\n",
       "         [ 2.8281e+00,  6.5234e-01,  3.0781e+00,  ...,  1.9844e+00,\n",
       "           2.4844e+00,  4.1016e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "580fc3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 720])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_entropies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd89565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAkm5JREFUeJzs3XeYE9X+x/HPJNuBpXdWehMQFRGRi2BBFBVRsStg9ycIiF7FQrfivYgVG4JXQUXEXlEQFVERRXpRQXpnWWBh2U3O748h2Q3bkiWTbJb363nm2c3kZOY7Z+ZM+ebMxDLGGAEAAAAAAAAR5Ip2AAAAAAAAADj2kJQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAGHx7bffyrIsffvtt9EOBWWEZVkaOXJktMPIp0ePHrrllluiHUaRfO1x+vTpjs4nOztbaWlpeuGFFxydDwCgbCIpBQAoMyZPnizLsgodfvrpp5Cn+dlnn5XKi+Ky4oUXXpBlWerQoUNIn+vatWuh67lFixYliuXRRx/VBx98UKLPIjjFtVHf0KBBg2iHWqi5c+fqq6++0n333ecf50sA5R2qVKmi0047TVOmTCnxvF544QVNnjw5DFGXTFZWlu677z7VqVNHycnJ6tChg2bOnBlQJj4+XkOGDNEjjzyigwcPRilSAECsiot2AAAAhNvo0aPVsGHDfOObNGkS8rQ+++wzPf/88ySmHDJlyhQlJCTol19+0Z9//hnSOqpXr54ee+yxfOMrVqxYolgeffRR9e7dW7169SrR51G8M844Q2+88UbAuJtvvlmnnnqqbr31Vv+48uXLS5IOHDiguLjSdbr65JNP6uyzzy5wWx04cKDat28vSdq5c6feeecdXXfddUpPT1f//v1DntcLL7ygatWqqV+/fkcbdon069dP06dP1+DBg9W0aVNNnjxZPXr00OzZs/Wvf/3LX+6GG27Q0KFDNXXqVN14441RiRUAEJtK11EeAIAwOP/883XKKadEfL45OTnyer1KSEiI+Lxj0Zo1a/Tjjz9q9OjRGjNmjKZMmaIRI0YE/fmKFSvquuuuczDCwu3fv1/lypWLyrxjgdfr1aFDh5SUlBQwvlGjRmrUqFHAuNtvv12NGjUqcF0e+flo27Ztmz799FO9+OKLBb7fuXNn9e7d2//6//7v/9SoUSNNnTq1REmpaPrll1/09ttv68knn9Q999wjSerTp49at26te++9Vz/++KO/bKVKlXTuuedq8uTJJKUAACHh9j0AwDFn7dq1sixL//nPf/Tyyy+rcePGSkxMVPv27TV//nx/uX79+un555+XpIDbco6cxvjx4/3TWLZsmSRp1qxZ6ty5s8qVK6dKlSrp4osv1vLlywPiGDlypCzL0ooVK3TFFVcoNTVVVatW1aBBgwJug+nSpYvatm1b4LI0b95c3bt3L3J5P/zwQ11wwQWqU6eOEhMT1bhxY40ZM0YejyegXNeuXdW6dWstW7ZMZ555plJSUlS3bl2NHTs23zQ3bNigXr16qVy5cqpRo4buuusuZWVlFRnHkaZMmSK3261bb71V3bp1O6rbnArjq+M///xT/fr1U6VKlVSxYkXdcMMNyszM9JezLEv79+/X66+/7l/Pvt4pvmksW7ZM11xzjSpXruzvJZKTk6MxY8b413+DBg30wAMP5KuLBg0a6MILL9RXX32lE088UUlJSTr++OM1Y8YMf5m///5blmXpqaeeyrccP/74oyzL0ltvvVXk8m7btk033XSTatasqaSkJLVt21avv/66//3s7GxVqVJFN9xwQ77PZmRkKCkpyZ+AkOzbt0aMGKEmTZooMTFRaWlpuvfee/Mtn2VZGjBggKZMmaJWrVopMTFRX3zxRZGxBuPIZ0r51sWqVat03XXXqWLFiqpevbqGDRsmY4zWr1+viy++WKmpqapVq5b++9//5ptmsMtUkE8//VQ5OTk655xzgoo/ISFBlStXztfba9KkSTrrrLNUo0YNJSYm6vjjj9eECRMCyjRo0EBLly7VnDlz/Ntk165d/e+np6frrrvuUoMGDZSYmKh69eqpT58+2rFjR8B0vF6vHnnkEdWrV09JSUk6++yz9eeffxYb+/Tp0/3t0ycpKUk33XST5s2bp/Xr1weU79atm3744Qft2rUrqLoBAECipxQAoAzas2dPvgszy7JUtWrVgHFTp07V3r17ddttt8myLI0dO1aXXnqp/v77b8XHx+u2227Tpk2bNHPmzHy3HPlMmjRJBw8e1K233qrExERVqVJFX3/9tc4//3w1atRII0eO1IEDB/Tss8+qU6dO+u233/I9L+eKK65QgwYN9Nhjj+mnn37SM888o927d+t///ufJOn666/XLbfcoiVLlqh169b+z82fP1+rVq3SQw89VGR9TJ48WeXLl9eQIUNUvnx5zZo1S8OHD1dGRoaefPLJgLK7d+/Weeedp0svvVRXXHGFpk+frvvuu09t2rTR+eefL8m+perss8/WunXrNHDgQNWpU0dvvPGGZs2aVWQcR5oyZYrOOOMM1axZU1dccYX69eun+fPn+29/Ko7H48m3niUpOTk5Xy+mK664Qg0bNtRjjz2m3377Ta+++qpq1KihJ554QpL0xhtv5LuNrHHjxgHTuPzyy9W0aVM9+uijMsZIsm89e/3119W7d2/dfffd+vnnn/XYY49p+fLlev/99wM+v3r1al155ZW6/fbb1bdvX02aNEmXX365vvjiC3Xr1k2NGjVSp06dNGXKFN1111356qpChQq6+OKLC62PAwcOqGvXrvrzzz81YMAANWzYUO+++6769eun9PR0DRo0SPHx8brkkks0Y8YMvfTSSwG9+j744ANlZWXpqquukmQnM3r27KkffvhBt956q1q2bKnFixfrqaee0qpVq/I9f2vWrFmaNm2aBgwYoGrVqjn6XKgrr7xSLVu21OOPP65PP/1UDz/8sKpUqaKXXnpJZ511lp544glNmTJF99xzj9q3b68zzjijRMt0pB9//FFVq1ZV/fr1C3x/7969/m1y165dmjp1qpYsWaKJEycGlJswYYJatWqlnj17Ki4uTh9//LHuuOMOeb1ef4+q8ePH684771T58uX14IMPSpJq1qwpSdq3b586d+6s5cuX68Ybb9TJJ5+sHTt26KOPPtKGDRtUrVo1/7wef/xxuVwu3XPPPdqzZ4/Gjh2ra6+9Vj///HORy/r777+rWbNmSk1NDRh/6qmnSpIWLlyotLQ0//h27drJGKMff/xRF154YZHTBgDAzwAAUEZMmjTJSCpwSExM9Jdbs2aNkWSqVq1qdu3a5R//4YcfGknm448/9o/r37+/Kehw6ZtGamqq2bZtW8B7J554oqlRo4bZuXOnf9wff/xhXC6X6dOnj3/ciBEjjCTTs2fPgM/fcccdRpL5448/jDHGpKenm6SkJHPfffcFlBs4cKApV66c2bdvX5H1kpmZmW/cbbfdZlJSUszBgwf947p06WIkmf/973/+cVlZWaZWrVrmsssu848bP368kWSmTZvmH7d//37TpEkTI8nMnj27yHiMMebXX381ksyLL77oX8aEhAQzaNCgYj+bN9aChttuu81fzlfHN954Y8DnL7nkElO1atWAceXKlTN9+/bNNy/fNK6++uqA8QsXLjSSzM033xww/p577jGSzKxZs/zj6tevbySZ9957zz9uz549pnbt2uakk07yj3vppZeMJLN8+XL/uEOHDplq1aoVGFtevvXy5ptvBny2Y8eOpnz58iYjI8MYY8yXX36Zbzs3xpgePXqYRo0a+V+/8cYbxuVyme+//z6g3Isvvmgkmblz5/rHSTIul8ssXbq0yBgLUli9+6Y7YsQI/2vfurj11lv943Jycky9evWMZVnm8ccf94/fvXu3SU5ODph2KMtUkH/961+mXbt2+cbPnj27wG3R5XKZRx55JF/5gtpk9+7dA+rfGGNatWplunTpkq/s8OHDjSQzY8aMfO95vd6AmFq2bGmysrL87z/99NNGklm8eHGRy9qqVStz1lln5Ru/dOnSgLbrs2nTJiPJPPHEE0VOFwCAvLh9DwBQ5jz//POaOXNmwPD555/nK3fllVeqcuXK/tedO3eWZN9GFazLLrtM1atX97/evHmzFi5cqH79+qlKlSr+8SeccIK6deumzz77LN80jnzWzJ133ilJ/rIVK1bUxRdfrLfeesvfQ8fj8eidd97x30JXlOTkZP//vp4cnTt3VmZmplasWBFQtnz58gHP9klISNCpp54aUCefffaZateuHfDsnJSUlIDbfIozZcoUxcXF6bLLLvMv43nnnae33347322FhWnQoEG+9Txz5kwNHjw4X9nbb7894HXnzp21c+dOZWRkBB3zkdPwrZ8hQ4YEjL/77rsl2bd65VWnTh1dcskl/tepqanq06ePfv/9d23ZskWS3aMrKSkp4FbGL7/8Ujt27Cj2+VmfffaZatWqpauvvto/Lj4+XgMHDtS+ffs0Z84cSdJZZ52latWq6Z133vGX2717t2bOnKkrr7zSP+7dd99Vy5Yt1aJFC+3YscM/nHXWWZKk2bNnB8y/S5cuOv7444uMMVxuvvlm//9ut1unnHKKjDG66aab/OMrVaqk5s2bB2y7oS7TkXbu3BmwzzjS8OHD/dvhO++8o6uvvloPPvignn766YByedukr2dnly5d9Pfff2vPnj3FLv97772ntm3bBmxPPr5bjH1uuOGGgB5xwe7nDhw4oMTExHzjfc/5OnDgQMB4X70U1HsRAIDCcPseAKDMOfXUU4N60Plxxx0X8Np3UbV79+6g53Xkr/z9888/kuxnPR2pZcuW+vLLL/M9JLtp06YB5Ro3biyXy6W1a9f6x/Xp00fvvPOOvv/+e51xxhn6+uuvtXXrVl1//fXFxrh06VI99NBDmjVrVr4kzJEXwPXq1ct3UVu5cmUtWrQoYBmbNGmSr1xBy1wQj8ejt99+258c8bnyyiv10Ucf6ZtvvtG5555b7HTKlSsX9LN9ilrXR96eVJiC1rXL5cr3K2y1atVSpUqV/NuCT0F11qxZM0n2M8p8n7vooos0depUjRkzRpKdwKtbt64/cVKYf/75R02bNpXLFfidY8uWLf3vS/InA6dOnaqsrCwlJiZqxowZys7ODkhKrV69WsuXLw9Iuua1bdu2gNcF/eKlU45cnxUrVlRSUlLA9uQbv3PnTv/rUJepIL7EcEHatGkTsE1eccUV2rNnj4YOHaprrrnGP9+5c+dqxIgRmjdvXsCzzSS7TRb3C5J//fWXP6FbnJLu55KTkwt8zpbveXd5E2tSbr0cuY0DAFAUklIAgGOW2+0ucHxRF51HOvLCLBwKuqjr3r27atasqTfffFNnnHGG3nzzTdWqVavYpEx6erq6dOmi1NRUjR49Wo0bN1ZSUpJ+++033XffffJ6vQHlw1EnxZk1a5Y2b96shx9+OGB8z549lZycrClTpgSVlAqFk+s63Bfhffr00bvvvqsff/xRbdq00UcffaQ77rgjX7LpaFx11VV66aWX9Pnnn6tXr16aNm2aWrRoEfBAfa/XqzZt2mjcuHEFTiPv84QkZ9pCYQpan8Gs41CX6UhVq1YNKWktSWeffbY++eQT/fLLL7rgggv0119/6eyzz1aLFi00btw4paWlKSEhQZ999pmeeuqpfG3yaJV0269du7Y2btyYb/zmzZsl2T3/8vLVy5GJQQAAikJSCgCAIoSacPA9AHnlypX53luxYoWqVauW73a71atXB/Qy+fPPP+X1egMeFO12u3XNNddo8uTJeuKJJ/TBBx/olltuKfSC0+fbb7/Vzp07NWPGDP/DniVpzZo1IS1XXvXr19eSJUtkjAmon4KWuSBTpkzxP3A7r/Lly6tHjx56//339eKLL0Y0ySGVbF17vV6tXr3a3xtJkrZu3ar09PR8D8P+888/89XZqlWrJClgXZ933nmqXr26pkyZog4dOigzMzOoHnH169fXokWL5PV6AxJYvls088ZzxhlnqHbt2nrnnXf0r3/9S7NmzfI/TNuncePG+uOPP3T22WeXmd4vR7tMLVq00HvvvRfSZ3JyciTZDyeXpI8//lhZWVn66KOPAnoxFXTrYGExNm7cWEuWLAkpjlCdeOKJmj17tjIyMgJ6E/oekH7iiScGlPftU/K2BQAAisMzpQAAKIIvgZSenh5U+dq1a+vEE0/U66+/HvCZJUuW6KuvvlKPHj3yfeb5558PeP3ss89Kkv/X7nyuv/567d69W7fddpv27dtX7DOGpNxeEnl7RRw6dEgvvPBCUMtTkB49emjTpk2aPn26f1xmZqZefvnlYj974MABzZgxQ926dSvw2TxXXHGF9u7dq48++qjE8ZVUuXLlgl7Pkvzrcvz48QHjfb1wLrjggoDxmzZtCvhFvoyMDP3vf//TiSeeqFq1avnHx8XF6eqrr9a0adM0efJktWnTRieccEJQ8WzZsiXgWVE5OTl69tlnVb58eXXp0sU/3uVyqXfv3vr444/1xhtvKCcnJ+DWPcleFxs3btQrr7ySb14HDhzQ/v37i42ptDnaZerYsaN2794d0nPnPvnkE0ny90IrqE3u2bNHkyZNyvfZwrbJyy67TH/88Ue+X3g8crpHo3fv3vJ4PAHtOisrS5MmTVKHDh3y9SpbsGCBLMtSx44dwzJ/AMCxgZ5SAIAy5/PPP8/3AG9JOv3009WoUaOQptWuXTtJ0sCBA9W9e3e53W5dddVVRX7mySef1Pnnn6+OHTvqpptu0oEDB/Tss8+qYsWKGjlyZL7ya9asUc+ePXXeeedp3rx5evPNN3XNNdcE3EolSSeddJJat27tf1jzySefXGz8p59+uipXrqy+fftq4MCBsixLb7zxxlFduN5yyy167rnn1KdPHy1YsEC1a9fWG2+8oZSUlGI/+9FHH2nv3r2S7J+qP5Lv+TpTpkzJlyQ50p49e/Tmm28W+F4wCbsjtWvXTl9//bXGjRunOnXqqGHDhurQoUOh5du2bau+ffvq5Zdf9t8m+csvv+j1119Xr169dOaZZwaUb9asmW666SbNnz9fNWvW1GuvvaatW7cWmIzo06ePnnnmGc2ePVtPPPFEUPHfeuuteumll9SvXz8tWLBADRo00PTp0zV37lyNHz9eFSpUCCh/5ZVX6tlnn9WIESPUpk2bfD1crr/+ek2bNk233367Zs+erU6dOsnj8WjFihWaNm2avvzyy6Ce3VaaHO0yXXDBBYqLi9PXX39d4IP9v//+e/8zl3bt2qWPPvpIc+bM0VVXXaUWLVpIks4991wlJCTooosu8ieYX3nlFdWoUcN/a5xPu3btNGHCBD388MNq0qSJatSoobPOOkv//ve/NX36dF1++eW68cYb1a5dO//8XnzxxXz7jpLo0KGDLr/8ct1///3atm2bmjRpotdff11r167VxIkT85WfOXOmOnXqpKpVqx71vAEAx5Ao/OIfAACOmDRpUoE/y+4bJk2aZIwxZs2aNUaSefLJJ/NNQ0f8BH1OTo658847TfXq1Y1lWcZ36CxqGsYY8/XXX5tOnTqZ5ORkk5qaai666CKzbNmygDK+n7dftmyZ6d27t6lQoYKpXLmyGTBggDlw4ECB0x07dqyRZB599NGg62Xu3LnmtNNOM8nJyaZOnTrm3nvvNV9++aWRZGbPnu0v16VLF9OqVat8n+/bt6+pX79+wLh//vnH9OzZ06SkpJhq1aqZQYMGmS+++CLfNI900UUXFbmOfEN8fLzZsWNHodPp0qVLkZ/38dXx9u3bAz7v21bWrFnjH7dixQpzxhlnmOTkZCPJ9O3bt8hpGGNMdna2GTVqlGnYsKGJj483aWlp5v777zcHDx4MKFe/fn1zwQUXmC+//NKccMIJJjEx0bRo0cK8++67hS5jq1atjMvlMhs2bCi0zJG2bt1qbrjhBlOtWjWTkJBg2rRp49/uj+T1ek1aWpqRZB5++OECyxw6dMg88cQTplWrViYxMdFUrlzZtGvXzowaNcrs2bPHX06S6d+/f9Bx5lWuXDl/XR/pyPZY2Lro27evKVeuXL7PF7RNB7tMhenZs6c5++yzA8bNnj073zaYkJBgWrRoYR555BFz6NChgPIfffSROeGEE0xSUpJp0KCBeeKJJ8xrr72Wb5vcsmWLueCCC0yFChWMJNOlSxf/ezt37jQDBgwwdevWNQkJCaZevXqmb9++/nbji+nIbcy37ypsu8jrwIED5p577jG1atUyiYmJpn379uaLL77IVy49Pd0kJCSYV199tdhpAgCQl2VMGJ9cCgAAgjZy5EiNGjVK27dvD/rhwE8//bTuuusurV27Nt+vaqH0atCggVq3bu2/lSsYJ510kqpUqaJvvvnGwcgQqu+//15du3bVihUr8v1y5rFq/PjxGjt2rP7666+IPwsOABDbeKYUAAAxwhijiRMnqkuXLiSkyrhff/1VCxcuVJ8+faIdCo7QuXNnnXvuuRo7dmy0QykVsrOzNW7cOD300EMkpAAAIeOZUgAAlHL79+/XRx99pNmzZ2vx4sX68MMPox0SHLJkyRItWLBA//3vf1W7du1in6uF6Pj888+jHUKpER8fr3Xr1kU7DABAjCIpBQBAKbd9+3Zdc801qlSpkh544AH17Nkz2iHBIdOnT9fo0aPVvHlzvfXWW0pKSop2SAAAAI7hmVIAAAAAAACIOJ4pBQAAAAAAgIgjKQUAAAAAAICIi+lnSnm9Xm3atEkVKlSQZVnRDgcAAAAAAOCYZ4zR3r17VadOHblchfeHiumk1KZNm5SWlhbtMAAAAAAAAHCE9evXq169eoW+H9NJqQoVKkiyFzI1NTXK0Ry97OxsffXVVzr33HMVHx8f7XCAMoX2BTiLNgY4h/YFOIs2BoRfRkaG0tLS/HmbwsR0Usp3y15qamqZSUqlpKQoNTWVnSEQZrQvwFm0McA5tC/AWbQxwDnFPWqJB50DAAAAAAAg4khKAQAAAAAAIOJISgEAAAAAACDiYvqZUgAAAAAAIHZ4PB5lZ2dHOwwcpfj4eLnd7qOeDkkpAAAAAADgKGOMtmzZovT09GiHgjCpVKmSatWqVezDzItCUgoAAAAAADjKl5CqUaOGUlJSjiqRgegyxigzM1Pbtm2TJNWuXbvE0yIpBQAAAAAAHOPxePwJqapVq0Y7HIRBcnKyJGnbtm2qUaNGiW/l40HnAAAAAADAMb5nSKWkpEQ5EoSTb30ezTPCSEoBAAAAAADHccte2RKO9UlSCgAAAAAAABFHUgoAAAAAAAARR1KqlPB4pDlzLH33XV3NmWPJ44l2RAAAAAAAlC4ej/Ttt9Jbb9l/nb527tevnyzLyjecd955QX3+22+/lWVZSk9PdzbQGMWv75UCM2ZIgwZJGzbESTpF48ZJ9epJTz8tXXpptKMDAAAAACD6cq+dc8dF4tr5vPPO06RJkwLGJSYmhnUehw4dUkJCQlinGQvoKRVlM2ZIvXsHNipJ2rjRHj9jRnTiAgAAAACgtIjmtXNiYqJq1aoVMFSuXFmS/bDvV199VZdccolSUlLUtGlTffTRR5KktWvX6swzz5QkVa5cWZZlqV+/fpKkrl27asCAARo8eLCqVaum7t27S5LmzJmjU089VYmJiapdu7aGDh2qnJwcfyy+zw0YMEAVK1ZUtWrVNGzYMBljJEmjR49W69at8y3DiSeeqGHDhjlWRyVFUiqKPB47y3t42wngGzd4sPPdEQEAAAAAiCRjpP37gxsyMqSBA4u+dh40yC5X3LQKmsbRGjVqlK644gotWrRIPXr00LXXXqtdu3YpLS1N7733niRp5cqV2rx5s55++mn/515//XUlJCRo7ty5evHFF7Vx40b16NFD7du31x9//KEJEyZo4sSJevjhhwPm9/rrrysuLk6//PKLnn76aY0bN06vvvqqJOnGG2/U8uXLNX/+fH/533//XYsWLdINN9wQ/oU/Sty+F0Xff58/y5uXMdL69Xa5rl0jFhYAAAAAAI7KzJTKlw/PtIyxr60rViy+7L59UrlyoU3/k08+Ufkjgn3ggQf0wAMPSLKfO3X11VdLkh599FE988wz+uWXX3TeeeepSpUqkqQaNWqoUqVKAdNo2rSpxo4d63/94IMPKi0tTc8995wsy1KLFi20adMm3XfffRo+fLhcLrtfUVpamp566ilZlqXmzZtr8eLFeuqpp3TLLbeoXr166t69uyZNmqT27dtLkiZNmqQuXbqoUaNGoS14BNBTKoo2bw5vOQAAAAAAEF5nnnmmFi5cGDDcfvvt/vdPOOEE///lypVTamqqtm3bVux027VrF/B6+fLl6tixoyzL8o/r1KmT9u3bpw15erScdtppAWU6duyo1atXy3P4NqtbbrlFb731lg4ePKhDhw5p6tSpuvHGG0Nf8Aigp1QU1a4d3nIAAAAAAMSClBS711IwvvtO6tGj+HKffSadcUbx8w1VuXLl1KRJk0Lfj4+PD3htWZa8Xm9Q03XCRRddpMTERL3//vtKSEhQdna2evfu7ci8jhZJqSjq3Nn+pYCNGwu+r9Wy7Pc7d458bAAAAAAAOMWygr+N7txzg7t2Pvdcye0Ob5xHy/eLep4gHhbdsmVLvffeezLG+HtCzZ07VxUqVFC9evX85X7++eeAz/30009q2rSp3IcXPi4uTn379tWkSZOUkJCgq666SsnJyeFapLDi9r0ocrvtn66U7EaUl+/1+PGlr1EBAAAAABAp0b52zsrK0pYtWwKGHTt2BPXZ+vXry7IsffLJJ9q+fbv2FdE97I477tD69et15513asWKFfrwww81YsQIDRkyxP88KUlat26dhgwZopUrV+qtt97Ss88+q0GDBgVM6+abb9asWbP0xRdflNpb9ySSUlF36aXS9OlS3bqB4+vVs8dfeml04gIAAAAAoLSI5rXzF198odq1awcM//rXv4L6bN26dTVq1CgNHTpUNWvW1IABA4os+9lnn+mXX35R27Ztdfvtt+umm27SQw89FFCuT58+OnDggE499VT1799fgwYN0q233hpQpmnTpjr99NPVokULdejQIfSFjhBu3ysFLr1Uuvhi6ZNPcnTJJS4Z49J330kNGkQ7MgAAAAAASgfftfP339s/CFa7tv24GyfvLpo8ebImT55c6PumgPsJ09PTA14PGzZMw4YNCxj37bffFji9Ll266Jdffikypvj4eI0fP14TJkwoMq5NmzbpjjvuKHJa0UZSqpRwu6UePYyaNNmj1asr64cfSEoBAAAAAJCX2y117RrtKEq37du36+2339aWLVt0ww03RDucIpGUKmVatdqh1asr69tvpeuui3Y0AAAAAAAgltSoUUPVqlXTyy+/rMqVK0c7nCKRlCplWrfeoQ8+aKpCevIBAAAAAIBjVGG3/eVV0C2FpRUPOi9ljj9+l1wuo7/+kjZsiHY0AAAAAAAAzohqUsrj8WjYsGFq2LChkpOT1bhxY40ZMyamsnrhlpKSo5NPtpd/zpwoBwMAAAAAAOCQqCalnnjiCU2YMEHPPfecli9frieeeEJjx47Vs88+G82wou6MM+ykFLfwAQAAAACAsiqqSakff/xRF198sS644AI1aNBAvXv31rnnnlvszx+Wdb6kFD2lAAAAAABAWRXVB52ffvrpevnll7Vq1So1a9ZMf/zxh3744QeNGzeuwPJZWVnKysryv87IyJAkZWdnKzs7OyIxO8m3DKeeekgul1urV1tauzZbdetGOTCgDPC1r7KwrwBKI9oY4BzaF+As2pjzsrOzZYyR1+uV1+uNdjgIE6/XK2OMsrOz5Xa7A94Ltj1ZJooPcPJ6vXrggQc0duxYud1ueTwePfLII7r//vsLLD9y5EiNGjUq3/ipU6cqJSXF6XAj6u67u+ivvyrprrt+VZcuG6MdDgAAAAAAJRIXF6datWopLS1NCQkJ0Q4HYXLo0CGtX79eW7ZsUU5OTsB7mZmZuuaaa7Rnzx6lpqYWOo2o9pSaNm2apkyZoqlTp6pVq1ZauHChBg8erDp16qhv3775yt9///0aMmSI/3VGRobS0tJ07rnnFrmQsSI7O1szZ85Ut27d1LNnop56Stq79yT16NE22qEBMS9v+4qPj492OECZQxsDnEP7ApxFG3PewYMHtX79epUvX15JSUnRDgdhcvDgQSUnJ+uMM87It159d7YVJ6pJqX//+98aOnSorrrqKklSmzZt9M8//+ixxx4rMCmVmJioxMTEfOPj4+PL1M4jPj5eZ53l1lNPSd9951Z8vLv4DwEISlnbXwClDW0McA7tC3AWbcw5Ho9HlmXJ5XLJ5Yrqo61LbN68eTr99NPVo0cPffrpp8WW79q1q+YU8KDo2267TS+++GJQ85w8ebIGDx6s9PT0UMONCJfLJcuyCmw7wbalqCalMjMz822Qbrebe0wl/etfkmVJq1ZJmzZJdepEOyIAAAAAAKJk0UjJcktthuV/b/EYyXikE0Y6NvuJEyfq6quv1owZM7Rp0ybVCeIi/ZZbbtHo0aMDxjnx6KFDhw7F7G2RUU1RXnTRRXrkkUf06aefau3atXr//fc1btw4XXLJJdEMq1SoVEk66ST7f36FDwAAAABwTLPc0uLhdgIqr8Vj7PGWc3cY7du3T++8844GDx6sM888U5MnTw7qcykpKapVq1bA4Hv00Nq1a2VZlmbMmKEzzzxTKSkpatu2rebNmydJ+vbbb3XDDTdoz549sixLlmVp5MiRkqQGDRpozJgx6tOnj1JTU3XrrbdKkt577z21atVKiYmJatCggf773/8GxOP73NVXX61y5cqpbt26ev755/3v33jjjbrwwgsDPpOdna0aNWpo4sSJJam6YkU1KfXss8+qd+/euuOOO9SyZUvdc889uu222zRmzJjiP3wM6NrV/ktSCgAAAABQphgj5ewPfmg5RGr1kJ2A+mOYPe6PYfbrVg/Z7wcznRL81tu0adNUq1YtnXrqqbr22mv12muvKVy/Gffggw/qnnvu0cKFC9WsWTNdffXVysnJ0emnn67x48crNTVVmzdv1ubNm3XPPff4P/ef//xHbdu21e+//65hw4ZpwYIFuuKKK3TVVVdp8eLFGjlypIYNG5Yvgfbkk0/6Pzd06FANGjRIM2fOlCTdfPPN+uKLL7R582Z/+U8++USZmZm68sorw7K8R4rq7XsVKlTQ+PHjNX78+GiGUWp16SKNGyd9+220IwEAAAAAIIw8mdK08iX77NKH7aGw10W5Yp8UVy6k2U2cOFHXXnutJKlXr1667bbbNGfOHHX19SQpxAsvvKBXX301YNxLL73kn5Yk3XPPPbrgggskSaNGjVKrVq30559/qkWLFqpYsaIsy1KtWrXyTfuss87S3Xff7X997bXX6uyzz9awYfbtjc2aNdOyZcv05JNPql+/fv5ynTp10tChQ/1l5s6dq6eeekrdunXT6aefrubNm+uNN97QvffeK0maNGmSLr/8cpUvX8J1VYzYfMLYMaJzZ/u5UitXSnkSlQAAAAAAIAJWrlypH3/80Z9IKl++vC6++OKgbme79tprtXDhwoChZ8+eAWVOOOEE//+1a9eWJG3btq3YaZ9yyikBr5cvX65OnToFjOvUqZNWr14tj8fjH9exY8eAMh07dtTy5cv9r2+++WZNmjRJkrR161Z9/vnnuvHGG4uNp6Si2lMKRatcWTrxROn336XvvpMc6i0HAAAAAEBkuVPsXkuhWvq43SvKlSB5D9m37rUaGtp8QzBx4kS1b99eTZs29Y+79tprdfnll+u5555TxYoVC/1sxYoV1aRJkyKnn/dX6izLkqSgfvytXLnQensFq0+fPho6dKjmzZunH3/8UQ0bNlTnzp0dmZdET6lSz9cbkFv4AAAAAABlhmXZt9GFMiwfZyek2oyWrsqy/y592B4f7DQOJ36CkZOTo//973+65pprAsafe+65SklJ0VtvvRXuWgmQkJAQ0MupKC1bttTcuXMDxs2dO1fNmjWT2537EPiffvopoMxPP/2kli1b+l9XrVpVvXr10qRJkzR58mTdcMMNR7EExaOnVCnXpYv01FMkpQAAAAAAxzDfr+y1GS21sZ+b5P+7eHjg6zD55JNPtHXrVrVu3VpLliwJeO+MM87QxIkTdfvttxf6+czMTG3ZsiVgXGJioipXrhzU/Bs0aKB9+/bpm2++Udu2bZWSkqKUlIJ7et19991q3769xowZoyuvvFLz5s3Tc889pxdeeCGg3Ny5czV27Fj16tVLM2fO1LvvvqtPP/00oMzNN9+sCy+8UB6PR3379g0q1pIiKVXK+Z4rtWKFtHWrVLNmtCMCAAAAACDCjCcwIeXje22C61EUCt9zo7p161ZomUWLFgU8FyqvV155Ra+88krAuO7du+uLL74Iav6nn366br/9dl155ZXauXOnRowYoZEjRxZY9uSTT9a0adM0fPhwjRkzRrVr19bo0aMDHnIu2cmrX3/9VaNGjVJqaqrGjRun7t27B5Q555xzVLt2bbVq1Up16tQJKtaSIilVylWpIrVtKy1cKM2ZI11xRbQjAgAAAAAgwk4YWfh7Ye4h5fPxxx+X+LPfFnO7U4MGDWSMCRhXqVKlfOMmTJigCRMmBIxbu3ZtgdO87LLLdNlllxU539TUVE2bNq3IMvv379fu3bt10003FVkuHHimVAzo0sX+yy18AAAAAADACV6vV9u2bdOYMWNUqVKlfL8U6ASSUjHA97DzOXOiGgYAAAAAACij1q1bp5o1a2rq1Kl67bXXFBfn/M113L4XA844w36u1LJl0rZtUo0a0Y4IAAAAAADEksJu+/Mp6JZCp9FTKgZUqSK1aWP/T28pAAAAAABQFpCUihG+W/h4rhQAAAAAACgLSErFCJ4rBQAAAACIZV6vN9ohIIzCsT55plSM6NzZ/rt0Kc+VAgAAAADEjoSEBLlcLm3atEnVq1dXQkKCLMuKdlgoIWOMDh06pO3bt8vlcikhIaHE0yIpFSOqVbOfK7V4sfTdd1Lv3tGOCAAAAACA4rlcLjVs2FCbN2/Wpk2boh0OwiQlJUXHHXecXK6S34RHUiqGdO1qJ6XmzCEpBQAAAACIHQkJCTruuOOUk5Mjj8cT7XBwlNxut+Li4o66xxtJqRjStav07LM87BwAAAAAEHssy1J8fLzi4+OjHQpKCR50HkPOOMP+u2SJtGNHdGMBAAAAAAA4GiSlYki1alKrVvb/jzxi95ii1yMAAAAAAIhFJKViyIwZ0j//2P+PHy+deabUoIE9HgAAAAAAIJaQlIoRM2bYDzffty9w/MaN9ngSUwAAAAAAIJaQlIoBHo80aJBkTP73fOMGD+ZWPgAAAAAAEDtISsWA77+XNmwo/H1jpPXr7XIAAAAAAACxgKRUDNi8ObzlAAAAAAAAoo2kVAyoXTu85QAAAAAAAKKNpFQM6NxZqldPsqzCy8TFSVWrRi4mAAAAAACAo0FSKga43dLTT9v/F5aYysmROnaUpk2LXFwAAAAAAAAlRVIqRlx6qTR9ulS3buD4tDTptdekM8+U9u+XrrxSuvtuO0nl8Ujffiu99Zb9l1/nAwAAAAAApUVctANA8C69VLr4YvtX9jZvtp8h1bmz3ZPq+uulhx6SnnhCGjdO+uILaffuwIef16tn97i69NLIxezxFBwvAAAAAAA4tpGUijFut9S1a/7xcXHS449Lp54qXXuttGxZ/jIbN0q9e9s9ro4mMRVsomnGDGnQIGnDhtxx0UiMAQAAAACA0ofb98qYiy+WKlUq+D1j7L+DB5f8Vr4ZM6QGDezbBa+5xv7boIE9/shyvXsHJqSk3MTYkeVROnDLZ2ioLwAAAAAoOZJSZcz330tbthT+vjHS+vV2uVAFm2jyeOweUr4k2JHzl44uMQZnBJtwhI36AgAAAICjQ1KqjMn7DKlwlPMpLtFkjHTTTfZD1rt1y5+4OrJ8SRNjoaInS3Do2RYa6gsAAAAAjh7PlCpjatcOrlx8fGjT/f77ohNNkpSebj9kPVhHJsaCfVZVaXimVbQf4B7K/IsrW1zC0bLsnm0XX5z/c7FSB+Gcbmmqr2ivg1DFWrwAAAAAnEVSqozp3NlOvGzcWPBFs8/NN0sHDkjXXWdfRBd3sbhqVXDzP+88qWFDacKE4su+8orUpInUvn3wCaRQyvXunb8OwvGw91CTXeG+EA9l/sGULS7hmLdnm+8h+07WQTBlSzL/OXMsffddXZUrZ+nMM0ueyIxEfQUj1n5IINbiJYEWmmDbWEmmy3oAACB0HEOpg5hhYtiePXuMJLNnz55ohxIWhw4dMh988IE5dOjQUU3nvfeMsSx7yL25Lndc48a543r2NObVV42pVy+wbL169nRWrjTm//7PmISEwPcLG2bPNiYnx/78kfMvbGjRouDxvnjfey9wuYorl51tTJ06hc/PsoxJS7PjzCsnx45/6tTc5SisbouLIW/5wuq2IMXFEMr8iys7YoQxo0cb07ZtcOupVy9jPvnEmMmTnauDYMo6tQ6Km+7QofbQsmVw9XXllfY6nDIltHiDEWodOMmJdhPsdEMpF4pQ220onIg32pyqr1CmG0q9OrUOYmndEqtznIo3XOeIODaUhn1iLMnJMWbmzGwzZMh8M3NmdpmoLyfPZWIFdRB9weZrSEqVIuE84SioEaal2eOzs4155BFj4uODu7j2DUWVPzLRU1xi7D//MaZPH2Pi4oqfb/Xqxnz4oTHVqhVdLjHRmEaNgl+u2bOLrq8jd1q+ZFuodRCu5Elx8/fV1Sef2ENx9eXEcDR1EExZp9ZBMHUbifrKu60VdsITah0EM80jBVvWiXYT7HRDKRfKcjmZ8HPq5CiaSRan6ivUfUc4E9+h1kFJphus0pB0jeYFWKwliJ2q21AumJ0S7WRutOfvZNlwx1oa9omhlg1WtNttaaivaH8Z6CTqoOwhKRWDwv0tWHEN6/ffg0vgXHih/fnp04tONAVzwehLjPlMmxb5RIBvaNTImCefNObFF4vfaW3caMzIkcFNt0sXYwYNMqZixeAvxIvbcb7zjjHPPx/+OjjzTGOeesqYGjUK79lmWcZUrmzM9dcHn7gJtg7q1jVm2zZ7KKp3m2RMSkrwvbratzfmhhuMSU0tulz58sZcemnwvZ/OP9+Yl182pmbNonsCVqpkTO/ewScGQ0mQzp4d/mkW125L0rNs+nS7p+V99wUX7xdfBDfdUHtOhrJcJU34BcPJ5E20eik5VV+hTDfcie9Q66Ak0w2WE+vW6d6+4RRrCWInt69of9Mf7d6Q0Z6/k2XDHWtp2CdGuw5CnWYs1Ve0vwx0EnVQNpGUikGR7podrovbIxNNeRWXGJs6NbgYgr24HzYs+GkGO7jd4Z1e3qF/fzvhVKNG+KbZoIE9BFN26tTc9RpMwjHcdRtrg1P1ddNNdmKuuBOeAQOMOfHE4KZ5xhnGfPqpvX2F+4QrmJ5lLldodetyGdOunZ0oLKyM7yQiK8uZHnPvvRdcrHn3icHs55zq3ebUt4DBTNfrtW9NdaK+gj023XmnMVWrBlev0eplWdLEnBPr1unevsHEEGxZJ3uExlLdRnMdOFlfTtdBMJxKRjhRX07sj5zcH0SzDkKZZqzVV7DlPvmk8DjzDrNmOVe3oZalDkpWNhaQlIpBkU5KBXvB7LsQ9wlnYwn24uOpp4Ir54unXr2Cdy6+HUydOsY891zwF/iWFfgsrqKG/v2NueCC4MqGMqSkBF8HTiUcg52uU3XQq1dw5e691+6pFEzZvn3tZ2tFs75821hiYvjrrLB24HsvlBOuWrWM+fxzY/797+DmHR9vTOvW4V+mKlWCK/foo8YsWBBcL7xgE7mSMffcY8zBg4VvB0d+s/b11+HZvkK5nbek3wIGk3BMTg4tkT5ihDEeT/ExrF1r915t1Ci820vt2vYQTNknnjBm0aLin0tYq5ad9L3nntDXra+ew5mQCWbdBrs/Gjs2uDo4mm+ZiyvrVI9Qp+p21qzg4n3mGWP+/NPuJVxcDKEm38O9Dpysr2CnW5I68E0/GskIJ+ormGnWrm3Md9/ZX8wGsx2Gsk988kljliwJbpuNZh04uU8Mpb6eesruJR7u+iquLUjGlCsX2jlX+fLG9OhR9F0FpWk/40QdVKhgX69Euw5KUjZWkJSKQbHQUyrcgkkg5d0RFVfuyG8ojix/ZMY72MTcxInBx+o7GQpmul27GtO0aXBl33gj+PmHEuuR6yOYE7lw1sHXXwd/0f711+Gff7CJTKfqq2JFY04+ObhYJftZbEXdPmhZdsLgjjvs2y6DmeaZZ9pDsDEEO/zvf8HX7V9/2T3Bwh2DU0PFinb7LWyZJDsxes45xiQlBTfNs8+2f0jg2WeL/sbunXeMeeut4KY5Y4bdqymYbwH37rXnH2wdhNKLtE4d++Q3nOugWbPobwfBDhMn5u4XwpWQeffd4G5hvPVWe79Rq1b4l+uzzwKPuUVtX3mXv7Cykv3IgGC/BGra1Ji777aTg8XNPzPTPo4GM92PPy6+3UjGXHWVvV2XKxf+ug32S7Ngt4NQe2bs2WPMpEnh3Q7efdeYLVvsxyaEsw5CTU4G28ZCOTaGUvbLL4Orr2nTovuYi1AHp+rgnXeCm+Z779lfgBTXbm+6yZhrrrGfwxor9RVsW3Bq+Pjj8O7rffuDTZuMmTAhNurgk0/CXwdH28MwFpCUikGRTkqV9EI83IJNIAVbLu90w9WTxXfCE2wMTiRvZs8OrQ5Cra9wr69Q6iCUsk7MvzTUV7C9AadODX6ab74Z/oNyvXr2M7ucaDfBtoWBA4Mr17Sp/S1YMGXvv9+YnTuL3mYk+5tFJy7unRzi44tPIBW1zAUNI0bYSazi6islJfh1INmJvueft7+VLq7tBpvMfu45ewimbIMGdk+wYMqmpRlz6qnBr4OLLzZm8OCiTzrvusuY7t2Dry+3O/y3mDdoEHzPXMn+hrm4Hy2pVMnucfHkk0U/a9CpIdTbiSW7DoL5MZZQhtq1nXkkQDDbQSjrINT9gW/6of6ATriG88+3kxcvv1x0+3riCWOeftqYjh2jE2feoXr18NZXtWrGtGkTXNlQ9on16wf/xUq060Cy22y421go9VW3bvTagWQfY9avL/78t149Y3780X5ebLDTLl++9O/rJfs4umFDcHUwd2506yDYspG6JncCSakYFI2f+3XqQrwkcQTzrKpgy/mEs+dPSWJ1KnkSbB2EWl/BCncdlKRsuOcf7foqSYI0XNMcMCD4Xkol7VkWTLxO9Jx0IvHs8dgXNsFM9667jPntt+LjrVrVmCFDgu92XpKL7GCGUG4TDra+Dh60b6UM93SdSnyHs5elFP7ERkm2geuvt7/prlMnvHXg1DB0qJ38LSrW2rXtnjwXXRT8dJ24RXrAAGPmzbMvRoOp22++CW66wd6uXpIEUrCDEz3ApKKfBVeSOnBqCOXYGErZUIZg12+ox2Yn9onRroNQ2kK/fvbjCILdJ4ZSX8HezhtKfQXbFpz6MtCpwbKCf25wWa2DUAYn715yCkmpGBSNpJQxzl2IhyrYZ1WF+wFwJUnMBRuDU8mT0vDAvHDXQahlnZh/KNMNlRMJ0nBOMxI9y0J52GVx03Uq6eubdnHbTKjP5Av3LcX/+19wy5WZGXwCLdTblJ2sr2Cn60TiO9xtYfp0+xlN110XXB306mU/Ny2Y5Gyw396Huh0GWwe7dtnPoAomhk6d7CHYeMPdZl55xZjs7OCWKyPDmHHjolu3wSbfDx60b/kN9zp47TVntoNQbusOpg6qVDHm9tuDv+WzQwdjRo2yL4TDfWwMtuzWrcEn6oPd15fk2BzNnu9btoS/Dg4cCL4thNpuo11foT7GJNhjaLDzT0835j//Ca5uQ9nPvPEGdRDqsTHWkJSKQdFKShnj3IV4rHAyMedU8iSWRDuJlpNjzMyZ2WbIkPlm5szsUrt9O9Fz0clbPktDTzwnkr7GFL8dRvvHBEK5ndep25SPrK+i2lhJn2EYbCIz3IlvJ9pCKIk5p9atE3UQSgzR7BHq1LfnTtZtaVkH0dzPhDs56WQywqn6cvLY7MQ+MZp14GS7jXZ9RfvLwNKwnymrdVAanvPsFJJSMSiaSSmUjsRcaYihrIqV9uVEoseJEy6faPfEi1bSN5oPxw/1dl4nb1POq6g25vQzDJ1IfIe7LTiRkHFqOwylDpzskRlMrNG+ndjJui0t6yDa+xknkpOhzN+Jsk7s6/OK9j4xWnVwNO022C8vo3mnQDS/DCwN+5myWgdOnyNFE0mpGBQrF81ALIql9uVUb7Fo3/IZbeFerpJ8Y+fEdMP5LWCo082ruDbmVH05KZzbjBMJGWOcr9dwx+BEvNH+9rykQkm+F3fB7PQ6iPZ+xonkZCjzd6KsE/t6p8VCHZS03TpxnujE9hXNLwNLw34mlHKhimYdxOI5UjBISsWgWLpoBmIN7QtOiOY3dqVhmnkF08acjqG0czKRGe16dbJHZrjnXxqmG6pwt6+yWF+xeFFXWravaCot+wPOE4NTGvYz0RbNXpaxJth8jWWMMYpRGRkZqlixovbs2aPU1NRoh3PUsrOz9dlnn6lHjx6Kj4+PdjhAmUL7glM8Hun776XNm6XataXOnSW3u3RO16lYpeDbmJMxxIIZM6RBg6QNG3LHpaVJ48dLl15a8umWhnoNJYZY2r5LQ9060b7KYn051b6cVBq2r2grDfsDzhODVxr2M9HmVB2UtfoKNl9DUqoUYWcIOIf2BTiLNha8snbSCefRvoJH+0JJ0MaA8As2XxMXwZgAAACOeW631LVrtKMAyibaFwDEFle0AwAAAAAAAMCxh6QUAAAAAAAAIo6kFAAAAAAAACKOpBQAAAAAAAAijqQUAAAAAAAAIo6kFAAAAAAAACKOpBQAAAAAAAAijqQUAAAAAAAAIo6kFAAAAAAAACKOpBQAAAAAAAAijqQUAAAAAAAAIo6kFAAAAAAAACKOpBQAAAAAAAAijqQUAAAAAAAAIo6kFAAAAAAAACKOpBQAAAAAAAAiLupJqY0bN+q6665T1apVlZycrDZt2ujXX3+NdlgAAAAAAABwUFw0Z75792516tRJZ555pj7//HNVr15dq1evVuXKlaMZFgAAAAAAABwW1aTUE088obS0NE2aNMk/rmHDhlGMCAAAAAAAAJEQ1dv3PvroI51yyim6/PLLVaNGDZ100kl65ZVXohkSAAAAAAAAIiCqPaX+/vtvTZgwQUOGDNEDDzyg+fPna+DAgUpISFDfvn3zlc/KylJWVpb/dUZGhiQpOztb2dnZEYvbKb5lKAvLApQ2tC/AWbQxwDm0L8BZtDEg/IJtT5YxxjgcS6ESEhJ0yimn6Mcff/SPGzhwoObPn6958+blKz9y5EiNGjUq3/ipU6cqJSXF0VgBAAAAAABQvMzMTF1zzTXas2ePUlNTCy0X1Z5StWvX1vHHHx8wrmXLlnrvvfcKLH///fdryJAh/tcZGRlKS0vTueeeW+RCxors7GzNnDlT3bp1U3x8fLTDAcoU2hfgLNoY4BzaF+As2hgQfr4724oT1aRUp06dtHLlyoBxq1atUv369Qssn5iYqMTExHzj4+Pjy9TOo6wtD1Ca0L4AZ9HGAOfQvgBn0caA8Am2LUX1Qed33XWXfvrpJz366KP6888/NXXqVL388svq379/NMMCAAAAAACAw6KalGrfvr3ef/99vfXWW2rdurXGjBmj8ePH69prr41mWAAAAAAAAHBYVG/fk6QLL7xQF154YbTDAAAAAAAAQARFtacUAAAAAAAAjk0kpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEkpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEkpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEkpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEkpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEkpQAAAAAAABBxJKUAAAAAAAAQcSSlAAAAAAAAEHEhJ6UaNGig0aNHa926dU7EAwAAAAAAgGNAyEmpwYMHa8aMGWrUqJG6deumt99+W1lZWU7EBgAAAAAAgDKqREmphQsX6pdfflHLli115513qnbt2howYIB+++03J2IEAAAAAABAGVPiZ0qdfPLJeuaZZ7Rp0yaNGDFCr776qtq3b68TTzxRr732mowx4YwTAAAAAAAAZUhcST+YnZ2t999/X5MmTdLMmTN12mmn6aabbtKGDRv0wAMP6Ouvv9bUqVPDGSsAAAAAAADKiJCTUr/99psmTZqkt956Sy6XS3369NFTTz2lFi1a+Mtccsklat++fVgDBQAAAAAAQNkRclKqffv26tatmyZMmKBevXopPj4+X5mGDRvqqquuCkuAAAAAAAAAKHtCTkr9/fffql+/fpFlypUrp0mTJpU4KAAAAAAAAJRtISelfAmpX3/9VcuXL5cktWzZUqecckp4IwMAAAAAAECZFXJSasOGDbr66qs1d+5cVapUSZKUnp6u008/XW+//bbq1asX7hgBAAAAAABQxrhC/cDNN9+s7OxsLV++XLt27dKuXbu0fPlyeb1e3XzzzU7ECAAAAAAAgDIm5J5Sc+bM0Y8//qjmzZv7xzVv3lzPPvusOnfuHNbgAAAAAAAAUDaF3FMqLS1N2dnZ+cZ7PB7VqVMnLEEBAAAAAACgbAs5KfXkk0/qzjvv1K+//uof9+uvv2rQoEH6z3/+E9bgAAAAAAAAUDaFfPtev379lJmZqQ4dOiguzv54Tk6O4uLidOONN+rGG2/0l921a1f4IgUAAAAAAECZEXJSavz48Q6EAQAAAAAAgGNJyEmpvn37OhEHAAAAAAAAjiEhJ6Uk+6HmH3zwgZYvXy5JatWqlXr27Cm32x3W4AAAAAAAAFA2hZyU+vPPP9WjRw9t3LhRzZs3lyQ99thjSktL06effqrGjRuHPUgAAAAAAACULSH/+t7AgQPVuHFjrV+/Xr/99pt+++03rVu3Tg0bNtTAgQOdiBEAAAAAAABlTMg9pebMmaOffvpJVapU8Y+rWrWqHn/8cXXq1CmswQEAAAAAAKBsCrmnVGJiovbu3Ztv/L59+5SQkBCWoAAAAAAAAFC2hZyUuvDCC3Xrrbfq559/ljFGxhj99NNPuv3229WzZ08nYgQAAAAAAEAZE3JS6plnnlHjxo3VsWNHJSUlKSkpSZ06dVKTJk309NNPOxEjAAAAAAAAypiQnilljFFGRobefvttbdy4UcuXL5cktWzZUk2aNHEkQAAAAAAAAJQ9ISelmjRpoqVLl6pp06YkogAAAAAAAFAiId2+53K51LRpU+3cudOpeAAAAAAAAHAMCPmZUo8//rj+/e9/a8mSJU7EAwAAAAAAgGNASLfvSVKfPn2UmZmptm3bKiEhQcnJyQHv79q1K2zBAQAAAAAAoGwKOSn11FNPybIsJ2IBAAAAAADAMSLkpFS/fv0cCAMAAAAAAADHkpCfKeV2u7Vt27Z843fu3Cm32x2WoAAAAAAAAFC2hZyUMsYUOD4rK0sJCQlHHRAAAAAAAADKvqBv33vmmWckSZZl6dVXX1X58uX973k8Hn333Xdq0aJF+CMEAAAAAABAmRN0Uuqpp56SZPeUevHFFwNu1UtISFCDBg304osvhj9CAAAAAAAAlDlBJ6XWrFkjSTrzzDM1Y8YMVa5c2bGgAAAAAAAAULaF/Ot7s2fPdiIOAAAAAAAAHENCTkp5PB5NnjxZ33zzjbZt2yav1xvw/qxZs8IWHAAAAAAAAMqmkJNSgwYN0uTJk3XBBReodevWsizLibgAAAAAAABQhoWclHr77bc1bdo09ejRw4l4AAAAAAAAcAxwhfqBhIQENWnSxIlYAAAAAAAAcIwIOSl199136+mnn5Yxxol4AAAAAAAAcAwI+fa9H374QbNnz9bnn3+uVq1aKT4+PuD9GTNmhC04AAAAAAAAlE0hJ6UqVaqkSy65xIlYAAAAAAAAcIwIOSk1adIkJ+IAAAAAAADAMSToZ0pt27atyPdzcnL0yy+/HHVAAAAAAAAAKPuCTkrVrl07IDHVpk0brV+/3v96586d6tixY3ijAwAAAAAAQJkUdFLqyF/bW7t2rbKzs4ssAwAAAAAAABQk6KRUMCzLKvFnH3/8cVmWpcGDB4cvIAAAAAAAAJRKYU1KldT8+fP10ksv6YQTToh2KAAAAAAAAIiAoJNSlmVp7969ysjI0J49e2RZlvbt26eMjAz/UBL79u3Ttddeq1deeUWVK1cu0TQAAAAAAAAQW+KCLWiMUbNmzQJen3TSSQGvS3L7Xv/+/XXBBRfonHPO0cMPP1xk2aysLGVlZflf+xJh2dnZ+Z5vFYt8y1AWlgUobWhfgLNoY4BzaF+As2hjQPgF256CTkrNnj27xMEU5u2339Zvv/2m+fPnB1X+scce06hRo/KN/+qrr5SSkhLu8KJm5syZ0Q4BKLNoX4CzaGOAc2hfgLNoY0D4ZGZmBlXOMlH6ybz169frlFNO0cyZM/3PkuratatOPPFEjR8/vsDPFNRTKi0tTTt27FBqamokwnZUdna2Zs6cqW7duik+Pj7a4QBlCu0LcBZtDHAO7QtwFm0MCL+MjAxVq1ZNe/bsKTJfE3RPqXBbsGCBtm3bppNPPtk/zuPx6LvvvtNzzz2nrKwsud3ugM8kJiYqMTEx37Ti4+PL1M6jrC0PUJrQvgBn0cYA59C+AGfRxoDwCbYtRS0pdfbZZ2vx4sUB42644Qa1aNFC9913X76EFAAAAAAAAMqOqCWlKlSooNatWweMK1eunKpWrZpvPAAAAAAAAMoWV7QDAAAAAAAAwLEnLD2ljDH64osvNHHiRE2fPr3E0/n222/DEQ4AAAAAAABKuaNKSq1Zs0avvfaaJk+erI0bN8qyrHDFBQAAAAAAgDIs5Nv3srKyNGXKFJ111llq2rSpvvzyS91999368ssvnYgPAAAAAAAAZVDQPaUWLFigiRMn6q233lK1atV07bXX6qWXXlLTpk0lSUuXLnUsSAAAAAAAAJQtQSel2rdvr2uuuUZfffWV2rdv72RMAAAAAAAAKOOCTkp169ZN06dPV3p6uq677jpdfPHFSk5OdjI2AAAAAAAAlFFBP1Pqyy+/1OrVq3Xqqafq/vvvV82aNXX99dfryy+/lNfrdTJGAAAAAAAAlDEhPeg8LS1Nw4cP15o1a/Tee+/J4/GoV69eql27tkaMGOFUjAAAAAAAAChjQv71PZ9u3bpp6tSp2rRpkx566CH99ddf4YwLAAAAAAAAZViJk1I+lStX1p133qnff/9d8+fPD0dMAAAAAAAAKOOOOimV18knnxzOyQEAAAAAAKCMCmtSCgAAAAAAAAgGSSkAAAAAAABEXNBJqczMTCfjAAAAAAAAwDEk6KRUtWrVdOGFF+rll1/Wli1bnIwJAAAAAAAAZVzQSakVK1aoe/fumjZtmho0aKAOHTrokUce0eLFi52MDwAAAAAAAGVQ0Emp4447Tnfeeae+/vprbd26VYMHD9bixYvVuXNnNWrUSIMHD9asWbPk8XicjBcAAAAAAABlQIkedF6xYkVdffXVevvtt7V9+3a99NJL8ng8uuGGG1S9enVNmTIl3HECAAAAAACgDIk72gnEx8erW7du6tatm5599ln9/vvvysnJCUdsAAAAAAAAKKOOOil1pJNOOinckwQAAAAAAEAZU6Lb9wAAAAAAAICjQVIKAAAAAAAAEUdSCgAAAAAAABEXclJqxIgR+ueff5yIBQAAAAAAAMeIkJNSH374oRo3bqyzzz5bU6dOVVZWlhNxAQAAAAAAoAwLOSm1cOFCzZ8/X61atdKgQYNUq1Yt/d///Z/mz5/vRHwAAAAAAAAog0r0TKmTTjpJzzzzjDZt2qSJEydqw4YN6tSpk0444QQ9/fTT2rNnT7jjBAAAAAAAQBlyVA86N8YoOztbhw4dkjFGlStX1nPPPae0tDS988474YoRAAAAAAAAZUyJklILFizQgAEDVLt2bd1111066aSTtHz5cs2ZM0erV6/WI488ooEDB4Y7VgAAAAAAAJQRISel2rRpo9NOO01r1qzRxIkTtX79ej3++ONq0qSJv8zVV1+t7du3hzVQAAAAAAAAlB1xoX7giiuu0I033qi6desWWqZatWryer1HFRgAAAAAAADKrpCTUsOGDfP/b4yRJFmWFb6IAAAAAAAAUOaV6JlSEydOVOvWrZWUlKSkpCS1bt1ar776arhjAwAAAAAAQBkVck+p4cOHa9y4cbrzzjvVsWNHSdK8efN01113ad26dRo9enTYgwQAAAAAAEDZEnJSasKECXrllVd09dVX+8f17NlTJ5xwgu68806SUgAAAAAAAChWyLfvZWdn65RTTsk3vl27dsrJyQlLUAAAAAAAACjbQk5KXX/99ZowYUK+8S+//LKuvfbasAQFAAAAAACAsi3k2/ck+0HnX331lU477TRJ0s8//6x169apT58+GjJkiL/cuHHjwhMlAAAAAAAAypSQk1JLlizRySefLEn666+/JEnVqlVTtWrVtGTJEn85y7LCFCIAAAAAAADKmpCTUrNnz3YiDgAAAAAAABxDQn6mVF4bNmzQhg0bwhULAAAAAAAAjhEhJ6W8Xq9Gjx6tihUrqn79+qpfv74qVaqkMWPGyOv1OhEjAAAAAAAAypiQb9978MEHNXHiRD3++OPq1KmTJOmHH37QyJEjdfDgQT3yyCNhDxIAAAAAAABlS8hJqddff12vvvqqevbs6R93wgknqG7durrjjjtISgEAAAAAAKBYId++t2vXLrVo0SLf+BYtWmjXrl1hCQoAAAAAAABlW8hJqbZt2+q5557LN/65555T27ZtwxIUAAAAAAAAyraQb98bO3asLrjgAn399dfq2LGjJGnevHlav369Pvvss7AHCAAAAAAAgLIn5J5SXbp00apVq3TJJZcoPT1d6enpuvTSS7Vy5Up17tzZiRgBAAAAAABQxoTUUyo7O1vnnXeeXnzxRR5oDgAAAAAAgBILqadUfHy8Fi1a5FQsAAAAAAAAOEaEfPveddddp4kTJzoRCwAAAAAAAI4RIT/oPCcnR6+99pq+/vprtWvXTuXKlQt4f9y4cWELDgAAAAAAAGVTyEmpJUuW6OSTT5YkrVq1KuwBAQAAAAAAoOwLOSk1e/ZsJ+IAAAAAAADAMSTkZ0rdeOON2rt3b77x+/fv14033hiWoAAAAAAAAFC2hZyUev3113XgwIF84w8cOKD//e9/YQkKAAAAAAAAZVvQt+9lZGTIGCNjjPbu3aukpCT/ex6PR5999plq1KjhSJAAAAAAAAAoW4JOSlWqVEmWZcmyLDVr1izf+5ZladSoUWENDgAAAAAAAGVT0Emp2bNnyxijs846S++9956qVKnify8hIUH169dXnTp1HAkSAAAAAAAAZUvQSakuXbpIktasWaO0tDS5XCE/jgoAAAAAAACQFEJSyqd+/fpKT0/XL7/8om3btsnr9Qa836dPn7AFBwAAAAAAgLIp5KTUxx9/rGuvvVb79u1TamqqLMvyv2dZFkkpAAAAAAAAFCvke/Duvvtu3Xjjjdq3b5/S09O1e/du/7Br1y4nYgQAAAAAAEAZE3JSauPGjRo4cKBSUlKciAcAAAAAAADHgJCTUt27d9evv/7qRCwAAAAAAAA4RoT8TKkLLrhA//73v7Vs2TK1adNG8fHxAe/37NkzbMEBAAAAAACgbAo5KXXLLbdIkkaPHp3vPcuy5PF4jj4qAAAAAAAAlGkh377n9XoLHUJNSD322GNq3769KlSooBo1aqhXr15auXJlqCEBAAAAAAAgxoSclAqnOXPmqH///vrpp580c+ZMZWdn69xzz9X+/fujGRYAAAAAAAAcFnRSqkePHtqzZ4//9eOPP6709HT/6507d+r4448PaeZffPGF+vXrp1atWqlt27aaPHmy1q1bpwULFoQ0HQAAAAAAAMSWoJ8p9eWXXyorK8v/+tFHH9UVV1yhSpUqSZJycnKO+tY7X9KrSpUqBb6flZUVEENGRoYkKTs7W9nZ2Uc179LAtwxlYVmA0ob2BTiLNgY4h/YFOIs2BoRfsO3JMsaYYAq6XC5t2bJFNWrUkCRVqFBBf/zxhxo1aiRJ2rp1q+rUqVPiB517vV717NlT6enp+uGHHwosM3LkSI0aNSrf+KlTpyolJaVE8wUAAAAAAED4ZGZm6pprrtGePXuUmppaaLmQf33PKf3799eSJUsKTUhJ0v33368hQ4b4X2dkZCgtLU3nnntukQsZK7KzszVz5kx169ZN8fHx0Q4HKFNoX4CzaGOAc2hfgLNoY0D4+e5sK07QSSnLsmRZVr5x4TBgwAB98skn+u6771SvXr1CyyUmJioxMTHf+Pj4+DK18yhrywOUJrQvwFm0McA5tC/AWbQxIHyCbUtBJ6WMMerXr58/KXTw4EHdfvvtKleunCQFPOsplGneeeedev/99/Xtt9+qYcOGIU8DAAAAAAAAsSfopFTfvn0DXl933XX5yvTp0yekmffv319Tp07Vhx9+qAoVKmjLli2SpIoVKyo5OTmkaQEAAAAAACB2BJ2UmjRpUthnPmHCBElS165d882rX79+YZ8fAAAAAAAASoeoPug8yB/+AwAAAAAAQBnjinYAAAAAAAAAOPaQlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZQCAAAAAABAxJGUAgAAAAAAQMSRlAIAAAAAAEDEkZRC8BaNlBaPKfi9xWPs9wEAAAAAAIJAUgrBs9zS4uH5E1OLx9jjLXd04gKONYtGkiBG2bZoJNs4AADAMYCkFILXZpjUZrSdgFo0SjLe3IRUm9H2+2XdopHRvVByav5OTbesWjQyuvUVawniRSNjZ/taNJI2FopFI51ZrljbxsNt0Ui2w0UjYydWKfbiPdYtGsn6QtndDhaNLJvLFYpFI6mDGEJSCqFpM0xqNkhaMlJ66/BFQ8M+Uqv7ox1ZZET7Qsmp+Ud7uWJNtOsrb4LYF0NpThBHu75CQRsLjVPLFWvbeLixHcZWrFLsxXusY31BKrvbQVldrlBQB7HFlALPPfecqV+/vklMTDSnnnqq+fnnn4P63J49e4wks2fPHocjjIxDhw6ZDz74wBw6dCjaoRQuc4sxHzQ0ZooCh+nVjPn5VmM2zzTGk23MHyOMWTS64GksGm2/7yQn579otL3Mvukf+dppTs0/2ssVrBKu22LbV6jTLQ319ceIw23Qis68Y62+ghUrbcyp/VwJ123OwhHmgw8+MDkLR4SnvrxeY769yJ7WVHfp3V6cWg+xsh06yYlYY/T8IGrniKVlP+OEWGkLodRVaahXJ0Sg3Yb9GBZtsbJ9GxN7x1EELdh8TVy0k2LvvPOOhgwZohdffFEdOnTQ+PHj1b17d61cuVI1atSIdnjIK3uv9G0Paf8a+7UVJ5kcyZ0iZe2Q/nzZHhKrSeUaSLt+tW/xO2FE7jTyftMdqkUj7ax2Qd+QLx4jGY90wsjDsR3OjkuB5Y9m/j5thkk5++3pLBllz7fNqMh9c99mmJSz73D2f6Qkr9T4Vun4+wLLLRoZfH1JUuMbpD1LjliuCPZICDZep9ZtsdMdJR1KlzLXS5kbpOSaUo0u0auv3YukjR8dfmHsP3uWSgd3SEnVSjbNRSPD2MZGSQe359ZXYhWp+r8O19doe99xNPUVSqyhav2QdGBj+Ndtm2FS1rbw1UFU28Juaf/hdZtUQ6p+htxLR+kiueRa6j36+tr/j/TzzdKWr+3XxmP/3fentPsPqXLb0Ke5aKQz24xT68E3rcXD7d7Jxisdf3+YtsPtgdt365Glb18vSS3vlvavDTzeHe3x1snzg1ZDpb2rw7fvWDTSmW02lOlGbT8zOvRYQ9VmmOTJDGxjzQbax4C8QonBiXhDWQehlHUiVqem62S7bf2QdHCL3EtHqadcspZ67btCjmY7CEWw0w11/s3ukPauLP3n9ZJz+4Om/xc7dRBK2TIo6kmpcePG6ZZbbtENN9wgSXrxxRf16aef6rXXXtPQoUOjHB38PIek7y+Tdv9mv242SDplfO7OomFfyZUgbZhhJ6iydtjlloyUNn0qdXhZ2vDx0d16EcoBqeXddgyLh0u7f5fq9ZL2rpKWPnJ0OyPPQWnl09KfE+zXvgultVMkb7ZU/wqpYmvJssK/I2p2h7R+hrRumrRtzuE3vfafv16W1rwuVWknVesoVT/dPtFa/qT9fkH11fgWacVT0o559pC5IbeMb7nSF0nb50nVO4ZaU8EvV6gHpOPvlQ7tssel/yHVOlva9bv01yv5120o828zTJI5PN1FUsXjpY0f29tPYjW7LhePyD8dKbe+Dm6WMlZJqc2CqJwS8mZLy56wkxre7MMjXZK80rp3pG2zpfYvSmmXhD7tYNeBMfaB/sBme/zOX6RKbey2nr5ISqgsLX20iPrKyf17YKud4HMqVim47aDNCHv/9s80ad27ucl337pNqGjXtys+9Fgl6eA2adEIezuV8tSBx34vqQRfwuRNWmSul6qeKu2cb+8PwtYW/jjcFj453BaqSsvHFrpuXfLKSLIqHm9vJ5YV2jIZI/31qvTb3VLO3twvP2TZMa35nz3UPFtqMUTa+bNdJtwX14tGhlZfxpt7vGlxl7R1tl1HR3O82bPCbluSPX1JWvEfe50cd4VU72IpoVLwse5ZYR8/1r1rfwEh5W7fq5+391/HXSHVOENyxTl3Ml3oehht11lab+nXgdKOH+0EpK+t+I53/0yVkmpKDa+X4lKKqMBC5G03e1dJtc6Rdv4qrX6uZO2m9YP2+l43TVr/vn18knLrdsMMyeWW0i6XUpsGP90jv4RpMTSwzNFss6Hs65sPkA5uscfv+lWqf5W9n1n51NHvZ0zO4enOl467Uto6S/r7Nan18NxpOJWMyFhlnwOtef3wsh7evlY9I/0zRap6Wu75lPHYF7TBxODEfibvNrvvb6nyidLmL+yh9nlSXDlp+Ti7TFw5e9zi4XYbqnOBtGOu9M/bUst/29urk7FGpA7+stfNrl/t40VJtsM2I6T0xfb+cN00e18gyfLtZ1Y9bW8H1TrmDqFsB07UV3HlmvaXVr+Ue15/eJkk5e6Pts6yt5+6F0hWCZ7i48RyeXOkRv3s85jFw+3jeoNrpS3fSH9PlFqPCH5/0PQOafWLeepgdf462PattPFkqc75pacOQi1bBkU1KXXo0CEtWLBA99+f+zwil8ulc845R/PmzYtiZAhgvNLPN0lbZtqvm9xmJ6SkwINEm9HSJZulrd8ePjmbYZ+c7ZovfX6SXa7FPSU/Qc87L5Mj1TxLWvmMfcJXpb29E/vsRHun5jsplKQN79uDJFXrZO/4QmWMfUD/4377G3y/w8mAvaukpQ/bQ2oL6bjLpaxd9ol+3til0HZEvw+Vlj8hlWsoLR2Te9KU93PGI7mTJc8B+wRkx4/Siv/a78dXPLyD/8lOHK582n7fcudeHOedVmIN+8LEdwG4fro9VD3NTvTV6yUtediZnXGbYfZnFw+367jWWdLfk+3tLrWFtPFD+6Lh4Lbcz6x/zx5889r0md2Do/rhE4jiTuhbDZO2/ZB78NpxeL+zfrq0Ps9y+ZKskn1RnlxPSkmTDm61t29ffa2eYB8M615kXzDXOENaPCp833ykL5F+6iftWpA7ruW/pZPGSr/cLv35kl0/318q1b9GOuUZO95g5W1j2Xukml3taW78RKrU1k54ffQ/6cAGO0Hrs+kTe/A5tDv3/6RaUsrh+jqw2d4WfZaMthNsDa+Tmt8lVWpVshPUnEw7CbfuHWnFuPwnqMVdBFf/l7T2Dftk3/+ZeMlky79uFwySVj0nnTjWTgYEm2zJOSCtHC8tfcxOshxpyShp2eP2BXaLu+wEUDB10GaEnbDITrfb+V+v5LZpy21frGTvsS+ojmwLBbXFVsOkbd8V0BbytDFJytqZ+39itTzrdou0a76dkJKkH3pL1TtJJ/1XqtYhuLrav/5w76iv7Ncpx0mZ63LX58+32Qk3WdLWb+whsZrdPr3ZUts8+5PC9jG+ZNvBrfaxYN00O+Ec9DZzeLrH3y9t+jxPff1sv5/3eJPW2+45E6qD26XFI+225zuJ9h1rvNn2fm7TZ3aCtFZ3yZ1gH28Li7XmWdJnJ9gXYP7lcx0+nhyebtZ2e35/viQlVpfSLpO8WdLfkwqfbklPpvMm8favlSq2lP56TcpYbr+/fnpgfcSVt3sH+2LOWCnNv11a9KDU5P+kZv3t/W4w+w3PIXu72f+35EqS1r5pD5LkSpS2/2C3v+qnS1U7FL5ci0bZX7pVbmcfl/K2i7hydm9q375j90J7+ONBqfJJ9vlBdrp9PC6uvvLs51wHt6uyp55cC2dJq58JfZstqi3U6XF4X/+RHeO2OdInU+w26cnM/dzGj3J76MaVt5Owy5609zFV2hUfQ4t7pHXTpe0/2u3G90Xnxo/twWfJaOnPF+19S0o9e134vjBqPkja/KV9vhVyMiJHqnmmfZzIOz9f/RmP/Tdrp/0ly6ZPD7/nso9li4dL6Qul5oOlv1+3L5gb3WRPc9sPdtmaZ0r7b8o9l2k5RFo7teAvRYurr2YD7fn498mWtGaytCZP3L7kVEGOfG/5k/bxKLlubt1WO9yDOWO5fd6y7l37i4ej3b58bTxzg9Ri0FHWweTA49Ka13OTiZbLPv84tPNw8uj04o/5NbpIn75t70vyxmE8MnLZiSnLbR9b8m6blltKqi3/Fzath9vXBsseC0N9HT7/zVxvP6v378n5t68jt620S+x1um2O3THAd82RV2LVw/unw/v6bd/aQ4Vm9jlHwz7SsrEOntd7c5erdjd7X7/5C6liK/s49udL9nVH3uubvG1POrw/eCn3fCPv/qDhddLyp6Ttvjp4oYA6OHye4KuDrbPsIbW5fe7Z8PoI1cEGe14FrVsp//ptNVRa+1buNEt6/RwjLGOMidbMN23apLp16+rHH39Ux465PTHuvfdezZkzRz///HNA+aysLGVlZflfZ2RkKC0tTTt27FBqamrE4nZKdna2Zs6cqW7duik+voTfxjvAteh+uVf+V0aWvMddLW+HyfnLLHtEMh55Ww3PHenNlrVtttzf9/R/82DcyfI2u0veFvfYJzShytop90/XyLVtdrFFjbucvQPbu0qWcjdzY7ll6vWWt9lgmSrt5Fo6WrLc8h7/YL5puJY9Imv/WiljmVy75tufj6sgK2evPMcPl7fVQ3ItHi73isflrdBM1v61sryHcueVUE3WoR3yNP4/eduOlWv5E3Ivf1ielg/J2zLwdjv/e83vlinfWK4VT8q1f01AGW/ldlJCZbm2fi1PqxHyHv+gXMsekXvpKHmaDJCpfKKsnT/LtfMnac/S3G98CqqfhKoyVTvIVD1NpmpHWdtmyb38sdzp/jZQ7r9elLHcsg5fGJmUBjKpLeTa8oW/XN66ci8dVfj444fJ26CPXMsflXvNJHlrniNTobmsAxukzA2yMjfIytpa7HqVJONKstftvr9kyeReCB9ZLrmeTHyqXBnLlF3/Rs3e1kFnVf5ScZtmyJtcV9bBbbJMduBnrDjJeOzpWm55TnlJSkmTSa5rJ6MOfzMfsLwtH5Dr11vlXvt64LQqnSRv+YZyb5gRVH0Vui16c+T+/kJZ276VJa+MK0mW96A8x4+Qt1WeaS4ZLffyh2Vk2fEn1ZKpfoZMastCt++Adpu5Xq4N78la9YxcBzbkK5+vfhNryKTUk7V7oR2X5Zbn1Nek5DSZlHpSch37JOHI5W1xn1w/95F7Q+DFp7fmuTJJNeT+580itqPh8tbtKdfOn2Xt/EnW5k9l5UlCG0lKqiOTYq8vO4560q5f5N7wnjzNBst73NVyL7xbrh0/BC6PO1mm9vkycsm9Ybo/VvePl8u1OfcEyVv9DHnajpVr0yeF7zuWPiwr/Q9Z6b/Lylxnfy6ptlwHNxddB7XOk0msLvc/b+Svg6WPyL1slLxVT5d1YKOszNwEua8NFNoWUurLxJWXK2OpPI1ukbf53XItfkDuDTPkTaotK2u7LH+PlMOfcSXY+3F/W3g5ty2k1LOT4XnWzaEWw/TVuuN1fvILcu/8PneZ0q6Up83Dcq19veD6MkauHy+Xa/MXsswhGVeSTI0z5NryVaHbgbdqR1l7lsjKk+jzVu8iT/tX5Fo1Xu4/X5CnzkVSpbayMjdIBzbk/s3ZFzh7uezlOryt2NtOmrRzntzrp8nT/G55618n15Lhcm/62L9fz1fHceWlnP2Bx5vkuvI26S9vo5vsfXdRx5slo2TtnCdr16+ycjL84z1N+st70lO5y169i72+MpblzufwftpT9xJ5j39Q7j/ulWvbrMD4rDiZmt1k3Alyb/wwd1+/dIzcy8bIW+lkWZlrA9uTu5wsz355GvSV9+Rn5Fr+ZPHHsJYPytvoRjveNa/JW7ObTGpLWZnrpQMb7fVwcEuBxydjxclUOvHwcamDrF2/yr366dxYF4+Qe8VjMvGVZGWn259xJcikHi9X+sLC9xv1r5Vlxcva+KGs7NykeVHtxsiSUlvKuJLkSv9NniZ3yNTpKdfiB+XavSCwbGJ1eeteInkPyr32f7nxLnpQ7pVPyluusV23/iSjZJJqyzq4WZ4mA+RtOkCu5Y/LvXayvDXOlqnQVJZ/m90oK2t7/rpKrJlnP5dmJxp2/2ofb5oOkvf4B+RaMlLuvyYcbgsn2NPL3JB73PXszzfdgpiEalJKXSl9sb2vL6i+rHiZSm0lyy3Xrp/laTpQJu0KuZaNkWvLlzJxqQHbdd66U9YO/7Fch49vxcaUVEveur385zEq18g+vyjoPOTwNu6rcx9v+WZy7VvlP5b6t5dGN0sVWtjHmJ0/yzqwvqAQQmJciVL5xjLJ9aSUejLJdWVS0mRt/84+5jW/R6bWuXItf1yubbNk3MmyPAfyT0e+bdYlc9xVRc7TWvf24fVl2cfjA5uLPC/0z8NySynHBcSqlMP7xHVvy9P8Hnmb3inXsjFy//2qfdxKbRlwPqeDmwK294A6OLy9mpR6Msn1jqiDbofrYHYQdVDYuV/dw+d+y+VpeJO8Te+Q+/chcm2fE1jOlShT61wZV7zcG2boUIth+nx9O52ftkAJK8bI0/AmKdW3Hfwk68DGQuorXirfME992ece1vYf5F43VZ4W98rb+Fa5lj2au0+s0NzerjI32vV2cGvAsaMkTFx5mSrtD7eJjvb8Vz6Zuz/64165V42XcSXK8trX0yahikxqK7l2fF+C8/rh9r5+6cNyr3lV3lrdD+/r8xxzD27Otx0UXIdx/m3C2jHPv59RnliDr4NT/fsFa8dcuVeMLaYOqh6ug+9KWAc32OdnAXWQ93gXXB3kW5bDCdIj5x1rMjIyVK1aNe3Zs6fIfE1MJaVGjhypUaNG5ZvO1KlTlZJSgi7cKFaj7I/V5tBESdJvCXdqffzZIX2+2aF31DL7LXnllku5DfKgVVnL46/RurizFMyvH5TzblTj7I+VljNLccqT9JGl9XFn6oBVVQesavbgsv/PUTk1y56mltlvyaM4uZWj/VYNlTO5PW12uFrpgFVFaZ7vtTz+aq1KuNL/XuusiWqck/tNWo6StNvVVNW9i/OV9S3nyvje2ueqpzo5c1XD87vcCrzIK4l0V2NtdHfSprjTVS9njlpmv1Xo/POOjzOZquxdrcqeFWqR/fbhEz5LvycM1C53c+23avt7exT0+bzjd7haqYJ3nRJlXwB6FC+3svVn3MVamniDmh96Sy2y39HauHO03X2iks0OJXt32H/NTpX3rle8ij/J9E3bpezDJxyWVsdf6l+/B61qOuCqpkOqkG/d/hl3kfa4G6uKZ6Uqe1cq1btWriBOvg5albXL1Vy7Xc21y91c1T0L1SJ7mn+6R9ZJMPW129VEFbzr/NtqtpIVrwNaFX+ZlidcX+zn846v4F2vjgdHKNnYF4pb3Kdon1VH2Va5fHH5ppHs3a6q3hWqYHITS755Hzmv1XEX66CrqurmzFUVb+63hrknfZbWxJ1v17/L186q6qBVVV4r3j+do6mvDCtNFcxG/8nyQVVUkvZoRfyV+jv+IrXOekXHeeZov1VTCSZD8cp/kpo35lB4lKCt7nbaGNdJW93t1Dj7owJjbXHoTTXPnh6wL9vjaqCK3rX5yrbNelYNcr7xv860qmm3q6nqeuYVUQf1DteBfUg+aFVSkknXivgrtdXdXq0Pvaqq3hUBsecoSVvc7WUkpXm+96+Dv+Iu1B5XY1X2rlQV7wqletcFdSFywKpyuB200C5Xc9Xw/FbitnB81iQ1zfnQv048ile6q4mqepcHlE3y7lCng8NU3tgXirtczfV74kDVzflORq5Ct3FLXv0Vf7GOy/lajbM/VorJf9FelOIuaoKxz6qjXe7D+w5Xc9Xy/KyW2W/Lqzi5lKNsJfn3ezlK0j9x58jIUpOcjwPryxidkvWk6np+9E/7gFVZyWZ3kfuIzXEdVSdnrurmzA1o63l55dZ2d1ttdHfSlrgOapj9aZFtcUX8ldrlaqG6nrmqnfOTErSvwOmGg1dxspQjS5JXLs1NGqN0VxN5rcR8y1pQrBvc/1KK2R6w35Kkde6uWpjYXydmPa/jPN8qRwkB5w0Hrcra5O4ot7JUP+cb//a9Ju48ZbiO8x9DypstRcafpVRtiuuoTXGdtNPVSk2zpxcZ76r4S5Vp1VKdnLmq5l0c1PEpr3Bss0VP19LauO65x1vX4XMqq4q8VmK+ff3auHO036p7eD+zUklmd3GzkpFLe1z1tdvVQrvcdrupl/NdvmPI2vjzlezdoSSTex6R7N2hep7vCr1wP6iK2u1uLrfJUg3vH1oZf4X+iu+pUw7+RzW8C/3lcpSg9XFnyas4Nc75JKjjcJJ3p72cnhVqnPOxP4G236pT5PKWM5uOal359pv2fqaFKntWqmnO+0Xuk49cjrxlV8dfpkSz265PX9167fqt7fmpyC/5SsJ7+KL66OogQemuxoXWwZ9xPZV++NyvinelUr1rimxbHsVpm/tkbYrrpC3u9mqU/XHQ52NJXnt/U8WzUo1yPjnqJFJBsbkO7xOL275825aRpUUJt2mXq7kyXMf5r6mK239udbdTee8GlTP2l8FeueSSV3/Hna/Fibep2aG31TL7ba2JO1fb3Scp2exQ0uF2mGx2qoL3HyUos6DQ8sm7HRhZ+jP+Yh3Me91mVVWWVUmyXAVut2vjz/PPN28cvv2Bvw7cLZRhpYVQByergne9//whtw56aHHirUfUwYn+/VDS4TgqeNeFUAduWfIEtW7zrl+P4vRJuelFli3tMjMzdc0115TupNShQ4eUkpKi6dOnq1evXv7xffv2VXp6uj788MOA8vSUiixr/bty/3SdLBl5Wo/J961ocY7MLvu+5Tfxlf3fVJqKrWUqniBToWnB357/eptcO+ZK+/707/xNUi1ZB7fIuBJkeQ8VmkHON3//N2C3ysrZJ2v9NH/PAJNQRdahXfK0fEDeZgPl/r6XXLvs24yMXDINb5Cn9Qi5/nq5yF5VAb1OsvfI2vSxXOuny9r8WcgHZWO5lXPeEql849x5FNOrK19vtTz1UFR9BTXd5vfI9c8UuVY/LSvPfeqhnsAYSaZa54Bv33zflimlnlx/vmxvJ0XFW9i6zVs2Z5+sXQsOf8M1z78OjCx5m/zf4W/iT5NS6vuTc0FNN9j6anKHXH+/ItfqFwJ6gPl6MXmrdZY57sqAZVd85dxveo8fJrmT5FoyXJbxyLgS5Wn3gkz964K7dcxzUK6lo+Ra+ZQ/IeE57mp5O7wu1x/3yb3qKXmT0+TK8w2wkSVT7V9SfAW5Nn9W8jZWkvqqf51cfz4v15pJ+XqyHCnvN2Ha/4/c66bmxtr8bpl6vaUD62Vlbszz9/C3dpn/+L9l9pz2P5naPQJ6bRYb66FdsrJ2yrVuqh2LFSfL5MjT4n55G1wn99zL5Nq74nCcFeRtca+8zQbKteLJIOrgWrlWPyfXmsmF9mAw7nIydS6QN623TK3ucq0cV/w6yN4ra9f83G/9t3ye2xaaDsjtaZCcdlRt4chjmGvZI/a3wPv/8vduNe4UWZ5MeY4fJlOuvtwL+svyZslYbnnbPCJvs0FBfVERwJsja+P7/uOVkaSKrf3fVutwjwRfLyjXP2/JvfzR3G2m2WCZupfk31YObLBfH8w9+fe2+Le/F48SqweswwLrq87Fcu37U1bGUnv55ZKp0EyuvSvkOX6ETM2ucv10nVyHv4E3yXXlaT1a1r6/JVdc8Pv6PUvl2jBdrmWP+k/SPae8JFO3p5RQpcD1Vex0vdmyts2yj2FrXw/9GCbJVOvk//bb7kFZ1//X9derci8bXeh+JthYrZ0/ybXqaVkb3s/tla3A45JJrCFvvUtl0nrLVOtk90oqbvs+uE3Wrp/tb+13/ixrx/e5+44zPpWp3sV+9lZJ6jZru6yNH9p1u22W/0LFVO8qpdTN1+PCtW6a3ePh8IWap/k98qZdnqc3VZ5eVZkbpP1/+6epim1k8vRwzDtt1z9TAttCSff1xtj7Vt8+ZudPsnYv8NeXt/VIu91UOSVwfxvkfiZvWX+s9XpLyXXteaX/HtBLPe826NsOTFIteZv8n7yNbpESqzl2PlVo2Sb9Zepc4O+plru+DveoyN5zOGaXvCc+addXpbYF9zYOsq6CKVtgrE0HydTrladX3cbDvXo22L10D27x16+p0iFwu/L3OK0n19+v2T3U/HVwh0ztC/zLbR3YGNhz73BPutw66ChT6YTQ6iAn0972ds6zt41Nn+S221MnytS50L7l3bfsxR3DgtkOmt4pU+eiPOcZR6zjQztz66ta5wLauN1zzPXXK8We/wa7HQa1fR//oL0fWvW0XDtzH5tTovP6qqcF9E7Pvx0Uvq8/crlKtD8oaR20fEDWxg/kWjVerl2/HGUddMxzvnH4uHd43xtsHQS7bLEk2J5Scvx3AItx6qmnmgEDBvhfezweU7duXfPYY48V+9lgf2IwVkTt534LsmWWMW8l2D+b+Ut/+6e5Q1HYT276xs86z5h3Kx/+OXvlzscYYzyHjFkzxZj3age+/+1Fxsy7Ibif9gxm/P4Nxvx+nzHTKgXOJ+8w6zxjdi8ObdkLqwtfff7+gDFZ6QUPvz8QWLY0/vS312PMhk+M+frMwLp6K96YDxoY81VnY3642pjf7jVmxbPGrHvfmJ9vD265gok31HWe572cKXHFlglpusHIOWjMX5OM+bRN4duZb3g7xZiPmhnzQaPA8R81tbfXktg+z5iPm+eZnnXEfC17na141pjMTcFvM07VV9ZuY5aONeb9eoEx/tjHmFUTjNm10BhPTsHzKm7eR7bFo2kHO+YbM/OMQtalZW/zmVtKNu2sXcYsfdyYGXXyTNNlzLr3jMnen395StAWiqyDEq7bQo9hXq8xGz425uMWBdfXjHrGpC8PomKKEOy6dWKbKba+Rhmz6StjZnUvvO2/FW/M4jGB69epOjja6YbjGObEsWnv38b8OviI84r/M2bL7Nx9RlHzOtp2UxIhbF9B/1x9uNtCadjPFBdrzgFjts01Ztl/jPnusiP2nZYxf022j8VHI5RtNtz7maOpq1DHh+s4Gs06CDXWw4K6Dot2fTmx7zTGPlf8/vLA/edUlzEz6hrzxWnGfNfbmF/vMmbZf435Z5oxP90Svf2MU3Ww7Ud7OQurg+8vt+tg+Thj/nk3/HXg5LJFUbD5GkUonkK9/fbbJjEx0UyePNksW7bM3HrrraZSpUpmy5biT+ZJSoXBHyPyb+i7/jBmWqrdCD5uGXgydzTT9Vk02n7/4E77BPKt+NzG//HxduP37wzi7JPKPStC22EFM3+fQ3vtC/IPGwWexGz6MvTlLmhe0doRleQAXpLpT43PvfAKJo6jPSCFsm7zfL7YE/pQpxuqRaNyt+kpMubz9nai9bMTjZlerfCL1alxoSeFj5SdacyCuwOn+9W/jFnxdGCyy6k2VhJ/jAjvCaoTB3qv1066ftQ0T926jElfWvJp5lVcHZSwLRRbByVct8UewzyHjFn5fOD2PtVtjCe74PLBcuKkN5TphlJfuxcbM+/G3HXq22YyN4eyxAXPx4kTWSeOYU4em/IdlwqYllPtpqSxFjXdPOMC2tfRbrNO7uvD3W5Ksr34j7dFbAehCCUGJ/YzoayDUMo6tU+Mdh2EEmsexR7Dol1fTu47807H127+GFl0uWjsZ8pyHTi9bFESM0kpY4x59tlnzXHHHWcSEhLMqaeean766aegPkdSqhBHc0Dat9aYGXl6KP0x/OhiCUbGamO+uzT/xfjXZxtzYHtuOccvgkcGJg1K80lMMJysr9Jw4htinEGd0DslmPrKzrTbwpZZue0hnN/K+w+0RWzfTrexYIX7BNXpA70veRSuC6CCYjvaWCNwshP0MczfmyYM9eXUPsbp+vrt3ujUgVPTjfa+vqB5hWNdRbtu89RVvvZ1NNtspNZBceODURqSiKUlKRRuTsUa7Too4TSLPYZFu744ry/bdVBazsHDLKaSUiVFUqoQJW1Evw8NvL3i9/uPLo5Qbf3e7ibpuxiPpFg6iYm2aB+QQhHKCb1Tjvag5sSFVaROeEuiNFzUhCJWLoQj0BYdufWhOE4tVyxtM6WhDqK9r3fq4r401O1hYb1gdkq0Y3BqO3BKtOsrFKWoLTg1zbDesRJL6zaWzuudQh04Jth8TVQfdH60MjIyVLFixeIfnBUjsrOz9dlnn6lHjx5H/6DzxWOkxcOlNqOlNsNyXzcbJKVdLO1fLx3YYP/N3CDt+lU6mOeXZlrcLZ38n6OLoaQxuxIk76Hc2CM13yPnV9j4Y92ikfaDiAuqk8VjJOORThgZ4aCKF9b2FYpFI4OvLye2xVjbvheNjJ3ty6m6XTQyduogj2LbWKxti06gDpyxaGRMtplQRO0YFksWjSzz2wGcc8y2sUUjaTeLRlIHDgk2XxNX6DuIbb5GtXi4tHiE5PvZ0lVP20NRXPHRS0gdmUSTnD9BN56CLwR8r43H2fnHmqJ2ylxM5RdKfTmxLcba9h1L25dTdRtLdRCKWNsWnUAdOKOsthmEhu0ACB3thjooBUhKlWWtHzyc2MnTGc6dIpWzfxo74O+276W1b+b2Ulo8JnKNsKBviPMm1fK+dgI7IpQWTmyLbN/OoW5DQ31RBwAAAEcgKVWW/fJ/ga+Pv19q+4hkWYHjF4+xE1LR6KUk8c0xAAAAAADHIJJSZdXiMdJfL9v/H3e5VLGNnWhyJxf/HItI9lKS+OYYAAAAAIBjEEmpssiXaEqsLmVtl+peLDW89vB7RySa6KUEAAAAAACigKRUWWQ8UvNB0sqn7V8SqNvDHl9QooleSgAAAAAAIApISpVFJ4yUlv/X/r9GFymhcu57JJoAAAAAAEAp4Ip2AHDIhg/tv/V6RTUMAAAAAACAgpCUKosO7pB2zLX/r9czurEAAAAAAAAUgKRUWbTpE8l4pconSuXqRzsaAAAAAACAfEhKlUW+W/fqXhzdOAAAAAAAAApBUqqsyTkgbf7K/r8eSSkAAAAAAFA6kZQqa7Z8LXkypZTj7Nv3AAAAAAAASiGSUmXNRt+v7vWULCu6sQAAAAAAABSCpFRZ4vVIGz+2/+fWPQAAAAAAUIqRlCpLdv4sHdwmxVeUanSJdjQAAAAAAACFIilVlvh+da9OD8kVH91YAAAAAAAAikBSqizxP0+KW/cAAAAAAEDpRlKqrMhYaQ+ueKnO+dGOBgAAAAAAoEgkpcoK3617Nc6U4lOjGwsAAAAAAEAxSEqVFRu4dQ8AAAAAAMQOklJlwYGt0o559v/1ekY3FgAAAAAAgCCQlCoLNn0iyUhV2kkp9aIdDQAAAAAAQLFISpUF6z+w/9bl1j0AAAAAABAbSErFupz90tav7f95nhQAAAAAAIgRJKVi3eavJM9BqVwDqVKbaEcDAAAAAAAQFJJSsS7vr+5ZVnRjAQAAAAAACBJJqVjmzTn8kHNx6x4AAAAAAIgpJKVi2Y4fpaydUkJlqXrnaEcDAAAAAAAQNJJSscx3616dCyRXXHRjAQAAAAAACAFJqVhlTODzpAAAAAAAAGIISalYtWeZtO8vyZUg1e4e7WgAAAAAAABCQlIqViwaKS0ek/t64+FeUjXPllaMt98HAAAAAACIESSlYoXllhYPz01M+W7dc8XZ4y139GIDAAAAAAAIEU/HjhVthtl/Fw+XsjOknb/Yrzd+LLUZnfs+AAAAAABADCApFUvyJqb840hIAQAAAACA2MPte7GmzTBJlv2/5SYhBQAAAAAAYhJJqVizeIwkYyekjCfw4ecAAAAAAAAxgqRULFk8xr51r81o6eoc+2/eh58DAAAAAADECJ4pFSvyJqR8t+wd+YwpbuUDAAAAAAAxgqRUrDCegh9q7nttPJGPCQAAAAAAoIRISsWKE0YW/h49pAAAAAAAQIzhmVIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIg4klIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIg4klIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIg4klIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIi4uGgHcDSMMZKkjIyMKEcSHtnZ2crMzFRGRobi4+OjHQ5QptC+AGfRxgDn0L4AZ9HGgPDz5Wl8eZvCxHRSau/evZKktLS0KEcCAAAAAACAvPbu3auKFSsW+r5liktblWJer1ebNm1ShQoVZFlWtMM5ahkZGUpLS9P69euVmpoa7XCAMoX2BTiLNgY4h/YFOIs2BoSfMUZ79+5VnTp15HIV/uSomO4p5XK5VK9evWiHEXapqansDAGH0L4AZ9HGAOfQvgBn0caA8Cqqh5QPDzoHAAAAAABAxJGUAgAAAAAAQMSRlCpFEhMTNWLECCUmJkY7FKDMoX0BzqKNAc6hfQHOoo0B0RPTDzoHAAAAAABAbKKnFAAAAAAAACKOpBQAAAAAAAAijqQUAAAAAAAAIo6kVCnx/PPPq0GDBkpKSlKHDh30yy+/RDskIOY89thjat++vSpUqKAaNWqoV69eWrlyZUCZgwcPqn///qpatarKly+vyy67TFu3bo1SxEBse/zxx2VZlgYPHuwfRxsDjs7GjRt13XXXqWrVqkpOTlabNm3066+/+t83xmj48OGqXbu2kpOTdc4552j16tVRjBiIDR6PR8OGDVPDhg2VnJysxo0ba8yYMcr7iGXaFxB5JKVKgXfeeUdDhgzRiBEj9Ntvv6lt27bq3r27tm3bFu3QgJgyZ84c9e/fXz/99JNmzpyp7OxsnXvuudq/f7+/zF133aWPP/5Y7777rubMmaNNmzbp0ksvjWLUQGyaP3++XnrpJZ1wwgkB42ljQMnt3r1bnTp1Unx8vD7//HMtW7ZM//3vf1W5cmV/mbFjx+qZZ57Riy++qJ9//lnlypVT9+7ddfDgwShGDpR+TzzxhCZMmKDnnntOy5cv1xNPPKGxY8fq2Wef9ZehfQGRx6/vlQIdOnRQ+/bt9dxzz0mSvF6v0tLSdOedd2ro0KFRjg6IXdu3b1eNGjU0Z84cnXHGGdqzZ4+qV6+uqVOnqnfv3pKkFStWqGXLlpo3b55OO+20KEcMxIZ9+/bp5JNP1gsvvKCHH35YJ554osaPH08bA47S0KFDNXfuXH3//fcFvm+MUZ06dXT33XfrnnvukSTt2bNHNWvW1OTJk3XVVVdFMlwgplx44YWqWbOmJk6c6B932WWXKTk5WW+++SbtC4gSekpF2aFDh7RgwQKdc845/nEul0vnnHOO5s2bF8XIgNi3Z88eSVKVKlUkSQsWLFB2dnZAe2vRooWOO+442hsQgv79++uCCy4IaEsSbQw4Wh999JFOOeUUXX755apRo4ZOOukkvfLKK/7316xZoy1btgS0sYoVK6pDhw60MaAYp59+ur755hutWrVKkvTHH3/ohx9+0Pnnny+J9gVES1y0AzjW7dixQx6PRzVr1gwYX7NmTa1YsSJKUQGxz+v1avDgwerUqZNat24tSdqyZYsSEhJUqVKlgLI1a9bUli1bohAlEHvefvtt/fbbb5o/f36+92hjwNH5+++/NWHCBA0ZMkQPPPCA5s+fr4EDByohIUF9+/b1t6OCzhtpY0DRhg4dqoyMDLVo0UJut1sej0ePPPKIrr32WkmifQFRQlIKQJnUv39/LVmyRD/88EO0QwHKjPXr12vQoEGaOXOmkpKSoh0OUOZ4vV6dcsopevTRRyVJJ510kpYsWaIXX3xRffv2jXJ0QGybNm2apkyZoqlTp6pVq1ZauHChBg8erDp16tC+gCji9r0oq1atmtxud75fJtq6datq1aoVpaiA2DZgwAB98sknmj17turVq+cfX6tWLR06dEjp6ekB5WlvQHAWLFigbdu26eSTT1ZcXJzi4uI0Z84cPfPMM4qLi1PNmjVpY8BRqF27to4//viAcS1bttS6deskyd+OOG8EQvfvf/9bQ4cO1VVXXaU2bdro+uuv11133aXHHntMEu0LiBaSUlGWkJCgdu3a6ZtvvvGP83q9+uabb9SxY8coRgbEHmOMBgwYoPfff1+zZs1Sw4YNA95v166d4uPjA9rbypUrtW7dOtobEISzzz5bixcv1sKFC/3DKaecomuvvdb/P20MKLlOnTpp5cqVAeNWrVql+vXrS5IaNmyoWrVqBbSxjIwM/fzzz7QxoBiZmZlyuQIvf91ut7xeryTaFxAt3L5XCgwZMkR9+/bVKaecolNPPVXjx4/X/v37dcMNN0Q7NCCm9O/fX1OnTtWHH36oChUq+O//r1ixopKTk1WxYkXddNNNGjJkiKpUqaLU1FTdeeed6tixI78KBgShQoUK/me0+ZQrV05Vq1b1j6eNASV311136fTTT9ejjz6qK664Qr/88otefvllvfzyy5Iky7I0ePBgPfzww2ratKkaNmyoYcOGqU6dOurVq1d0gwdKuYsuukiPPPKIjjvuOLVq1Uq///67xo0bpxtvvFES7QuIFpJSpcCVV16p7du3a/jw4dqyZYtOPPFEffHFF/kesgegaBMmTJAkde3aNWD8pEmT1K9fP0nSU089JZfLpcsuu0xZWVnq3r27XnjhhQhHCpRdtDGg5Nq3b6/3339f999/v0aPHq2GDRtq/Pjx/gcxS9K9996r/fv369Zbb1V6err+9a9/6YsvvuA5b0Axnn32WQ0bNkx33HGHtm3bpjp16ui2227T8OHD/WVoX0DkWcYYE+0gAAAAAAAAcGzhmVIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIg4klIAAAAAAACIOJJSAAAAAAAAiDiSUgAAAAAAAIg4klIAAAAAAACIOJJSAAAAAAAAiDiSUgAAICrWrl0ry7K0cOHCaIcSskjGfv311+vRRx8NunzXrl01ePBg5wIqpU477TS999570Q4DAACEgKQUAAAoMcuyihxGjhwZ7RDziaWkzR9//KHPPvtMAwcO9CfCihomT56sGTNmaMyYMRGP1ePx6PHHH1eLFi2UnJysKlWqqEOHDnr11Vf9ZZys+4ceekhDhw6V1+t1ZPoAACD84qIdAAAAiF2bN2/2///OO+9o+PDhWrlypX/c/7d3/zFNXmscwL9tGdLZ2SVTVDIBN1FoBtIGyAgzGE2oorhsCokT6kDQCAwYumAiuHUTExeQEU0MaKLzR5RkxoC/GDFTU0RwTKAiBAlRMRtRCf5IFVlsz/3D2/f6XlqVqxbuvd9P0qTvec95z3Oe8Ad5cs5bjUYzGmH9z9i+fTsSExOh0WigVqtl+S4pKUFtbS1Onz4ttWm1WqjV6tEIFWazGRUVFdixYwciIiLw4MEDNDc34+7dux6Zf+HChUhPT8epU6ewaNEij8xJREREr4Y7pYiIiOg/NmXKFOmj1WqhUCika19fX2zbtg3vv/8+xo0bh/DwcNTW1rp9lt1uR1paGoKDg9Hb2wsAqK6uhsFggI+PDz744AOYzWY8efJEGqNQKLB792589tlnePvttxEUFISampoRrSEwMBBbtmxBWloa3nnnHfj7+6OyslLW5+LFi9Dr9fDx8UFERARaWlqGPae9vR0LFy6ERqPB5MmTkZKSgv7+fgDA2bNn4e3tDYvFIvX/8ccf4evri1u3brnNxy+//IKEhAQAgEqlkuVbo9HAy8tL1qZWq4ftRgoMDMTmzZthMpmg0WgQEBCAmpoa3LlzB59++ik0Gg3CwsLQ3Nwsm7++vh5z5syBWq3GtGnTkJOTg4cPH7rNY01NDTIzM5GYmIjp06dj9uzZWLVqFdavXw8A+PLLL3Hu3DmUl5dLO7uuX7/+wtwBT3dYZWdnIzs7G1qtFhMnTkRRURGEEFIflUqF+Ph4HD582G2MRERENLawKEVERERvRHl5OUpLS1FSUgKr1Qqj0YglS5agu7t7WN+hoSEkJiaitbUVFosF/v7+sFgsMJlMyM3NRUdHByoqKrB3714UFxfLxprNZiQlJcFqtSI+Ph4rVqzAwMDAiGItLS2Vik2ZmZlYu3attOPLZrNh8eLF0Ol0+OOPP/Ddd99JhRane/fuYd68edDr9WhubkZtbS1u3bqFpKQkAP86tpaSkoL79++jpaUFRUVF2L17NyZPnuwyJqvVivv37yMiImJEa3GlrKwMMTExaGlpwaJFi5CSkgKTyYTk5GRcunQJH374IUwmk1Tk6enpwYIFC7B06VJYrVZUVVWhvr4e2dnZbueYMmUKfvvtN9y5c8fl/fLyckRHRyMjIwN9fX3o6+vDtGnTXpg7p59//hleXl64ePEiysvLsW3bNtnRQACIioqSFf6IiIhojBNEREREr8GePXuEVquVrv38/ERxcbGsT2RkpMjMzBRCCHHt2jUBQFgsFjF//nzxySefiHv37kl958+fL7Zs2SIbv3//fjF16lTpGoAoLCyUrm02mwAgTp065TbO2NhYkZubK10HBASI5ORk6drhcAhfX1+xc+dOIYQQFRUV4r333hODg4NSn507dwoAoqWlRQghxA8//CDi4uJk89y8eVMAEF1dXUIIIYaGhkR4eLhISkoSOp1OZGRkuI1RCCGOHj0qVCqVcDgcLu9/++23Yvbs2SNeX19fnwAgioqKpLYLFy4IAKKvr08IIcSqVavE6tWrZc+1WCxCqVTK8vCsK1euiJCQEKFUKkVoaKhYs2aNOHny5HNjE+LlchcbGytCQkJkuSgoKBAhISGycdXV1UKpVAq73e4yRiIiIhpbuFOKiIiIXrsHDx7gr7/+QkxMjKw9JiYGnZ2dsrbly5fj4cOHqKurg1arldrb2trw/fffQ6PRSB/nLptHjx5J/cLCwqTv48ePx4QJE3D79u0RxfvsM5xHEJ3P6OzsRFhYGHx8fKQ+0dHRsvFtbW04c+aMLNbg4GAAT3cdAYC3tzcOHjyII0eO4PHjxygrK3tuTIODgxg3bhwUCsWI1vKi9Tl3ZoWGhg5rc665ra0Ne/fula3HaDTC4XDg2rVrLufQ6XRob29HY2Mj0tLScPv2bSQkJCA9Pf25sb1M7oCnv673bC6io6PR3d0Nu90utanVajgcDgwNDb1UXoiIiGh08UXnRERENKri4+Nx4MABXLhwAfPmzZPabTYbzGYzPv/882Fjni0QvfXWW7J7CoVixL/A9qrPsNlsSEhIwNatW4fdmzp1qvS9oaEBADAwMICBgQGMHz/e7TMnTpyIR48e4e+//4a3t/dLx+LKs+tzFnZctTnXbLPZsGbNGuTk5Ax7lr+/v9t5lEolIiMjERkZiby8PBw4cAApKSnYuHEjpk+f7nLMy+buZThzOloveyciIqKRYVGKiIiIXrsJEybAz88P58+fR2xsrNR+/vx5REVFyfquXbsWH330EZYsWYITJ05I/Q0GA7q6ujBjxgyPxv7vQkJCsH//fjx+/FgqhjU2Nsr6GAwGHDlyBIGBgfDycv3vVU9PD77++mvs2rULVVVVWLlyJU6fPg2l0vXG9fDwcABAR0eH9N1TDAYDOjo6Xjn3Op0OAKQXpHt7e8t2NjnnelHuAKCpqUl23djYiKCgIKhUKqmtvb0der3+lWImIiIiz+HxPSIiInojvvnmG2zduhVVVVXo6urChg0b0Nraitzc3GF9v/rqK2zevBmLFy9GfX09AGDTpk3Yt28fzGYzrly5gs7OThw+fBiFhYUeXccXX3wBhUKBjIwMdHR04OTJkygpKZH1ycrKwsDAAJYvX47ff/8dPT09+PXXX5Gamgq73Q673Y7k5GQYjUakpqZiz549sFqtKC0tdTvvpEmTYDAYpHx4UkFBARoaGpCdnY3W1lZ0d3ejurr6uS86X7ZsGcrKytDU1IQbN27g7NmzyMrKwsyZM6XjeIGBgWhqasL169fR398Ph8Pxwtw59fb2Ij8/H11dXTh06BC2b98+7G/JYrEgLi7uzSSFiIiIXjsWpYiIiOiNyMnJQX5+PtatW4fQ0FDU1taipqYGQUFBLvvn5eXBbDYjPj4eDQ0NMBqNOH78OOrq6hAZGYmPP/4YZWVlCAgI8Og6NBoNjh07hsuXL0Ov12Pjxo3Djpo5d4XZ7XbExcUhNDQUeXl5ePfdd6FUKlFcXIwbN26goqICwNNjaZWVlSgsLERbW5vbudPT03Hw4ME3uj5XwsLCcO7cOVy9ehVz5syBXq/Hpk2b4Ofn53aM0WjEsWPHkJCQgJkzZ2LlypUIDg5GXV2dtANq/fr1UKlU0Ol0mDRpEnp7e1+YOyeTyYTBwUFERUUhKysLubm5WL16tXT/zz//RENDA1JTU99cYoiIiOi1Ugjxz9/+JSIiIqIxZXBwELNmzUJVVdWwl6v/P5k7dy7Cw8Px008/ue1TUFCAu3fvorKy0nOBERER0SvhO6WIiIiIxii1Wo19+/ahv79/tEMZ83x9fZGfnz/aYRAREdEIsChFRERENIbNnTt3tEP4r7Bu3brRDoGIiIhGiMf3iIiIiIiIiIjI4/iicyIiIiIiIiIi8jgWpYiIiIiIiIiIyONYlCIiIiIiIiIiIo9jUYqIiIiIiIiIiDyORSkiIiIiIiIiIvI4FqWIiIiIiIiIiMjjWJQiIiIiIiIiIiKPY1GKiIiIiIiIiIg8jkUpIiIiIiIiIiLyuH8A4sKCcrbudk4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entropy_and_difference(entropies, batch_index=0):\n",
    "    \"\"\"\n",
    "    Plot entropy and its first difference for a single batch sample.\n",
    "\n",
    "    Parameters:\n",
    "    - entropies: torch.Tensor of shape [B, L]\n",
    "    - batch_index: which sequence in the batch to plot\n",
    "    \"\"\"\n",
    "    entropy_values = entropies[batch_index].detach().cpu().float().numpy()\n",
    "    diff_values = entropy_values[1:] - entropy_values[:-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot entropy\n",
    "    plt.plot(entropy_values, marker='o', label=\"Entropy\", color='blue')\n",
    "\n",
    "    # Plot entropy difference (offset by +1)\n",
    "    plt.plot(range(1, len(entropy_values)), diff_values, marker='x', label=\"Δ Entropy\", color='orange')\n",
    "\n",
    "    plt.title(f\"Entropy and Δ Entropy over Time (Batch {batch_index})\")\n",
    "    plt.xlabel(\"Token Index (Time Step)\")\n",
    "    plt.ylabel(\"Entropy / Δ Entropy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_entropy_and_difference(e, batch_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from typing import Optional, List, Tuple, Dict, Union\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
    "from chronos import ChronosModel, ChronosConfig\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class EntropyModel:\n",
    "    \"\"\"\n",
    "    A wrapper for Chronos model that calculates entropy for time series tokens\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str = \"amazon/chronos-t5-tiny\",\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the entropy model with a Chronos model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the Chronos model\n",
    "            device: Device to run the model on ('cuda', 'cpu', etc.)\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Load Chronos model\n",
    "        config = AutoConfig.from_pretrained(model_path)\n",
    "        chronos_config = ChronosConfig(**config.chronos_config)\n",
    "        pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        self.model = ChronosModel(config=chronos_config, model=pretrained_model)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Store the chronos config for tokenization\n",
    "        self.chronos_config = chronos_config\n",
    "        \n",
    "    def calculate_entropies(\n",
    "        self, \n",
    "        time_series: torch.Tensor,\n",
    "        batch_size: int = 1\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the entropy for each position in the time series.\n",
    "        \n",
    "        Args:\n",
    "            time_series: Time series tensor of shape [batch_size, seq_len]\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of entropy values of shape [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        if time_series.dim() == 1:\n",
    "            time_series = time_series.unsqueeze(0)  # Add batch dimension if not present\n",
    "            \n",
    "        batch_size, seq_len = time_series.shape\n",
    "        all_entropies = torch.zeros((batch_size, seq_len), device=self.device)\n",
    "        \n",
    "        # For each position in the sequence\n",
    "        for i in range(1, seq_len):  # Start from 1 as we need at least one context token\n",
    "            # Process context up to position i (excluding position i)\n",
    "            contexts = time_series[:, :i]\n",
    "            \n",
    "            # Tokenize inputs\n",
    "            tokenizer = self.chronos_config.create_tokenizer()\n",
    "            token_ids, attention_mask, scale = tokenizer.context_input_transform(contexts.to('cpu'))\n",
    "            \n",
    "            # For seq2seq models, decoder_input_ids starts with decoder_start_token_id\n",
    "            decoder_start_token_id = self.model.model.config.decoder_start_token_id\n",
    "            decoder_input_ids = torch.full(\n",
    "                (token_ids.shape[0], 1),\n",
    "                decoder_start_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=token_ids.device,\n",
    "            )\n",
    "            \n",
    "            # Get model prediction for the next token (position i)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.model(\n",
    "                    input_ids=token_ids.to(self.device),\n",
    "                    attention_mask=attention_mask.to(self.device),\n",
    "                    decoder_input_ids=decoder_input_ids.to(self.device),\n",
    "                    output_hidden_states=False,\n",
    "                    output_attentions=False,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                \n",
    "                # Get logits for the next position\n",
    "                next_position_logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "                \n",
    "                # Convert logits to probabilities\n",
    "                next_token_probs = F.softmax(next_position_logits, dim=-1)\n",
    "                \n",
    "                # Calculate entropy: -sum(p * log(p))\n",
    "                epsilon = 1e-10\n",
    "                entropy = -torch.sum(next_token_probs * torch.log2(next_token_probs + epsilon), dim=-1)\n",
    "                \n",
    "                # Store the entropy value for position i\n",
    "                all_entropies[:, i] = entropy\n",
    "                \n",
    "        return all_entropies\n",
    "\n",
    "    def tokenize_time_series(self, time_series: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenize the time series using the Chronos tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            time_series: Time series tensor\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with token_ids, attention_mask, and scale\n",
    "        \"\"\"\n",
    "        tokenizer = self.chronos_config.create_tokenizer()\n",
    "        token_ids, attention_mask, scale = tokenizer.context_input_transform(time_series.to('cpu'))\n",
    "        \n",
    "        return {\n",
    "            \"token_ids\": token_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"scale\": scale\n",
    "        }\n",
    "\n",
    "\n",
    "class EntropyPatcher:\n",
    "    \"\"\"\n",
    "    Patches time series data based on entropy values\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        threshold: float = 3.0, \n",
    "        min_len: int = 3, \n",
    "        max_len: int = 8, \n",
    "        mode: str = 'global'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the patcher.\n",
    "        \n",
    "        Args:\n",
    "            threshold: Threshold for entropy-based patching\n",
    "            min_len: Minimum length of a patch\n",
    "            max_len: Maximum length of a patch\n",
    "            mode: Patching mode ('global' or 'relative')\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode not in ['global', 'relative']:\n",
    "            raise ValueError(f\"Mode must be 'global' or 'relative', got {mode}\")\n",
    "    \n",
    "    def patch(\n",
    "        self, \n",
    "        token_ids: torch.Tensor, \n",
    "        entropies: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create patches based on entropy values.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Tensor of token IDs [batch_size, seq_len]\n",
    "            entropies: Tensor of entropy values [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - token_ids: Original token IDs [batch_size, seq_len]\n",
    "                - patch_ids: Patch IDs for each token [batch_size, seq_len]\n",
    "                - patch_lengths: Length of each patch [batch_size, num_patches]\n",
    "        \"\"\"\n",
    "        if token_ids.dim() == 1:\n",
    "            token_ids = token_ids.unsqueeze(0)  # Add batch dimension if not present\n",
    "            \n",
    "        if entropies.dim() == 1:\n",
    "            entropies = entropies.unsqueeze(0)  # Add batch dimension if not present\n",
    "            \n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        device = token_ids.device\n",
    "        \n",
    "        # Initialize outputs\n",
    "        patch_ids = torch.zeros_like(token_ids, dtype=torch.long)\n",
    "        patch_lengths_list = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Get patches for this sequence\n",
    "            patches = self._get_patches(entropies[b])\n",
    "            \n",
    "            # Fill in patch_ids and collect patch lengths\n",
    "            patch_lengths_b = []\n",
    "            for patch_idx, (start, length) in enumerate(patches):\n",
    "                patch_ids[b, start:start+length] = patch_idx\n",
    "                patch_lengths_b.append(length)\n",
    "            \n",
    "            patch_lengths_list.append(patch_lengths_b)\n",
    "        \n",
    "        # Create patch_lengths tensor with padding\n",
    "        max_num_patches = max(len(p) for p in patch_lengths_list)\n",
    "        patch_lengths = torch.zeros((batch_size, max_num_patches), dtype=torch.long, device=device)\n",
    "        \n",
    "        for b, lengths in enumerate(patch_lengths_list):\n",
    "            patch_lengths[b, :len(lengths)] = torch.tensor(lengths, dtype=torch.long, device=device)\n",
    "        \n",
    "        return {\n",
    "            \"token_ids\": token_ids,\n",
    "            \"patch_ids\": patch_ids,\n",
    "            \"patch_lengths\": patch_lengths\n",
    "        }\n",
    "        \n",
    "    def _get_patches(self, entropies: torch.Tensor) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Create patches for a single sequence based on entropy values.\n",
    "        First patch always has length 1, subsequent patches follow regular logic.\n",
    "        \n",
    "        Args:\n",
    "            entropies: Entropy values for a single sequence\n",
    "            \n",
    "        Returns:\n",
    "            List of (start_position, length) tuples\n",
    "        \"\"\"\n",
    "        patches = []\n",
    "        L = len(entropies)\n",
    "        \n",
    "        # Always make the first patch exactly length 1\n",
    "        if L > 0:\n",
    "            patches.append((0, 1))\n",
    "            start = 1  # Start the next patch from position 1\n",
    "        else:\n",
    "            return patches  # Empty sequence\n",
    "        \n",
    "        # Process the rest of the sequence with regular logic\n",
    "        while start < L:\n",
    "            end = start + self.min_len\n",
    "            \n",
    "            # Find the end of this patch\n",
    "            while end < min(L, start + self.max_len):\n",
    "                # Check whether to break the patch - only if not at the end of sequence\n",
    "                should_break = False\n",
    "                \n",
    "                if end < L:  # Only check for break if not at the end\n",
    "                    if self.mode == 'global' and entropies[end] > self.threshold:\n",
    "                        should_break = True\n",
    "                    elif self.mode == 'relative' and end > 0 and (entropies[end] - entropies[end-1]) > self.threshold:\n",
    "                        should_break = True\n",
    "                \n",
    "                if should_break:\n",
    "                    break\n",
    "                \n",
    "                end += 1\n",
    "            \n",
    "            # Add the patch\n",
    "            patches.append((start, end - start))\n",
    "            \n",
    "            # Move to the next position\n",
    "            start = end\n",
    "            \n",
    "        return patches\n",
    "\n",
    "\n",
    "class TimeSeriesPatcher:\n",
    "    \"\"\"\n",
    "    Combines EntropyModel and EntropyPatcher to process time series data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"amazon/chronos-t5-tiny\",\n",
    "        threshold: float = 3.0,\n",
    "        min_len: int = 3,\n",
    "        max_len: int = 8,\n",
    "        mode: str = 'global',\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the TimeSeriesPatcher.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the Chronos model\n",
    "            threshold: Threshold for entropy-based patching\n",
    "            min_len: Minimum length of a patch\n",
    "            max_len: Maximum length of a patch\n",
    "            mode: Patching mode ('global' or 'relative')\n",
    "            device: Device to run the model on ('cuda', 'cpu', etc.)\n",
    "        \"\"\"\n",
    "        self.entropy_model = EntropyModel(model_path, device)\n",
    "        self.patcher = EntropyPatcher(threshold, min_len, max_len, mode)\n",
    "    \n",
    "    def process(\n",
    "        self, \n",
    "        time_series: Union[torch.Tensor, List[float]],\n",
    "        return_entropies: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process time series data: tokenize, calculate entropies, and create patches.\n",
    "        \n",
    "        Args:\n",
    "            time_series: Time series data (tensor or list)\n",
    "            return_entropies: Whether to include entropy values in the output\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - token_ids: Tokenized time series\n",
    "                - patch_ids: Patch IDs for each token\n",
    "                - patch_lengths: Length of each patch\n",
    "                - entropies: (optional) Entropy values\n",
    "        \"\"\"\n",
    "        # Convert to tensor if necessary\n",
    "        if not isinstance(time_series, torch.Tensor):\n",
    "            time_series = torch.tensor(time_series, dtype=torch.float)\n",
    "            \n",
    "        if time_series.dim() == 1:\n",
    "            time_series = time_series.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "        # Move to the correct device\n",
    "        time_series = time_series.to(self.entropy_model.device)\n",
    "        \n",
    "        # Calculate entropies\n",
    "        entropies = self.entropy_model.calculate_entropies(time_series)\n",
    "        \n",
    "        # Tokenize the time series\n",
    "        tokenization = self.entropy_model.tokenize_time_series(time_series.to('cpu'))\n",
    "        token_ids = tokenization[\"token_ids\"]\n",
    "        \n",
    "        # Create patches\n",
    "        patching_result = self.patcher.patch(token_ids, entropies)\n",
    "        \n",
    "        # Add entropies to the output if requested\n",
    "        if return_entropies:\n",
    "            patching_result[\"entropies\"] = entropies\n",
    "            \n",
    "        return patching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be15932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import logging\n",
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# from bytelatent.transformer import LMTransformer, LMTransformerArgs\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# def build_entropy_model():\n",
    "#     \"\"\"\n",
    "#     Build an entropy model for time series patching.\n",
    "#     \"\"\"\n",
    "#     # Define model parameters\n",
    "#     args = LMTransformerArgs(\n",
    "#         dim=128,                  # Embedding dimension\n",
    "#         n_layers=4,               # Number of transformer layers\n",
    "#         n_heads=4,                # Number of attention heads\n",
    "#         max_seqlen=512,          # Maximum sequence length\n",
    "#         ffn_dim_multiplier=4,     # Multiplier for FFN dimension\n",
    "#         vocab_size=4096,           # For byte-level tokens (256 + small offset)\n",
    "#         attn_bias_type=\"local_block_causal\",  # Use causal attention with local blocks\n",
    "#         attn_impl=\"xformers\",     # Using xformers attention implementation\n",
    "#         sliding_window=512,       # Sliding window size for attention\n",
    "#         weight_tying=False,       # Whether to tie input and output embeddings\n",
    "#         norm_eps=1e-5,            # Epsilon for normalization\n",
    "#         seed=42                   # Random seed\n",
    "#     )\n",
    "    \n",
    "#     # Create the model\n",
    "#     model = LMTransformer(args)\n",
    "    \n",
    "#     # Initialize weights\n",
    "#     model.init_weights()\n",
    "    \n",
    "#     return model, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67917927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_entropy_model(\n",
    "#     model,\n",
    "#     train_dataloader,\n",
    "#     val_dataloader=None,\n",
    "#     epochs=10,\n",
    "#     lr=5e-5,\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     checkpoint_dir=\"./entropy_model_checkpoints\",\n",
    "#     save_every=1,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Train the entropy model on tokenized time series data.\n",
    "    \n",
    "#     Args:\n",
    "#         model: LMTransformer model\n",
    "#         train_dataloader: DataLoader for training data\n",
    "#         val_dataloader: Optional DataLoader for validation\n",
    "#         epochs: Number of training epochs\n",
    "#         lr: Learning rate\n",
    "#         device: Device to train on\n",
    "#         checkpoint_dir: Directory to save checkpoints\n",
    "#         save_every: Save checkpoint every N epochs\n",
    "#     \"\"\"\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#     os.makedirs(os.path.join(checkpoint_dir, \"consolidated\"), exist_ok=True)\n",
    "    \n",
    "#     # Move model to device\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Setup optimizer and scheduler\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#         optimizer, T_max=epochs * len(train_dataloader)\n",
    "#     )\n",
    "    \n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         epoch_loss = 0\n",
    "#         # for step, batch in enumerate(train_dataloader):\n",
    "#         for step, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_dataloader):\n",
    "#             x = batch_x.float().squeeze(-1)\n",
    "#             y = batch_y.float().squeeze(-1)\n",
    "            \n",
    "#             # Tokenize input sequence\n",
    "#             token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            \n",
    "#             # Tokenize target sequence\n",
    "#             target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             logits = model(token_ids.to(device), patch_lengths)\n",
    "        \n",
    "#             loss = F.cross_entropy(\n",
    "#                             logits.reshape(-1, logits.size(-1)),\n",
    "#                             target_token_ids.reshape(-1).to(device),\n",
    "#                             ignore_index=PAD_ID\n",
    "#                         )\n",
    "            \n",
    "#             # Backward pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "            \n",
    "#             # Log progress\n",
    "#             if step % 100 == 0:\n",
    "#                 logger.info(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "#         avg_loss = epoch_loss / len(train_dataloader)\n",
    "#         logger.info(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         if val_dataloader is not None:\n",
    "#             val_loss = evaluate(model, val_dataloader, device)\n",
    "#             logger.info(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "#         # Save checkpoint\n",
    "#         if (epoch + 1) % save_every == 0:\n",
    "#             save_checkpoint(model, optimizer, checkpoint_dir, epoch)\n",
    "    \n",
    "#     # Save final model\n",
    "#     save_checkpoint(model, optimizer, checkpoint_dir, epochs)\n",
    "    \n",
    "#     # Save consolidated weights for inference\n",
    "#     consolidated_path = os.path.join(checkpoint_dir, \"consolidated/consolidated.pth\")\n",
    "#     torch.save({\"model\": model.state_dict()}, consolidated_path)\n",
    "    \n",
    "#     logger.info(f\"Model saved to {checkpoint_dir}\")\n",
    "#     return model\n",
    "\n",
    "# def evaluate(model, dataloader, device):\n",
    "#     \"\"\"Evaluate model on the provided dataloader\"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             tokens = batch[\"input_ids\"].to(device)\n",
    "#             inputs = tokens[:, :-1]\n",
    "#             targets = tokens[:, 1:]\n",
    "#             loss = model(inputs, target=targets)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     model.train()\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# def save_checkpoint(model, optimizer, checkpoint_dir, epoch):\n",
    "#     \"\"\"Save model checkpoint\"\"\"\n",
    "#     checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    \n",
    "#     # Save model state\n",
    "#     torch.save(\n",
    "#         {\n",
    "#             \"model\": model.state_dict(),\n",
    "#             \"optimizer\": optimizer.state_dict(),\n",
    "#             \"epoch\": epoch,\n",
    "#         },\n",
    "#         checkpoint_path\n",
    "#     )\n",
    "    \n",
    "#     # Save model parameters for loading\n",
    "#     args_dict = model.config.__dict__ if hasattr(model, \"config\") else vars(model.args)\n",
    "    \n",
    "#     params = {\n",
    "#         \"entropy_model\": {\n",
    "#             \"dim\": args_dict[\"dim\"],\n",
    "#             \"n_layers\": args_dict[\"n_layers\"],\n",
    "#             \"n_heads\": args_dict[\"n_heads\"],\n",
    "#             \"max_seqlen\": args_dict[\"max_seqlen\"],\n",
    "#             \"ffn_dim_multiplier\": args_dict.get(\"ffn_dim_multiplier\", 4),\n",
    "#             \"vocab_size\": args_dict[\"vocab_size\"],\n",
    "#             \"attn_bias_type\": args_dict[\"attn_bias_type\"],\n",
    "#             \"attn_impl\": args_dict[\"attn_impl\"],\n",
    "#             \"sliding_window\": args_dict.get(\"sliding_window\", 512),\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     with open(os.path.join(checkpoint_dir, \"params.json\"), \"w\") as f:\n",
    "#         json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # Set up logging\n",
    "#     logging.basicConfig(\n",
    "#         level=logging.INFO,\n",
    "#         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#     )\n",
    "    \n",
    "#     # Set random seed for reproducibility\n",
    "#     torch.manual_seed(42)\n",
    "    \n",
    "#     # Build model\n",
    "#     logger.info(\"Building model...\")\n",
    "#     model, _ = build_entropy_model()\n",
    "    \n",
    "#     # Create your time series dataset\n",
    "#     # This is where you'd load your actual time series data\n",
    "#     logger.info(\"Preparing data...\")\n",
    "    \n",
    "#     # Example: Generate synthetic time series data\n",
    "#     import numpy as np\n",
    "    \n",
    "    \n",
    "#     # Train model\n",
    "#     logger.info(\"Training model...\")\n",
    "#     checkpoint_dir = \"./entropy_model\"\n",
    "#     model = train_entropy_model(\n",
    "#         model,\n",
    "#         train_loader,\n",
    "#         validate_loader,\n",
    "#         epochs=5,  # You may need more epochs for real data\n",
    "#         lr=5e-5,\n",
    "#         checkpoint_dir=checkpoint_dir\n",
    "#     )\n",
    "    \n",
    "#     logger.info(f\"Training complete! Model saved to {checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b2cbc",
   "metadata": {},
   "source": [
    "## TimeGPT 2 for Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-layer norm (GPT-2 style)\n",
    "        attn_output = self.attention(self.layer_norm1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(self.layer_norm2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :].transpose(0, 1)\n",
    "\n",
    "class TimeSeriesGPT2(nn.Module):\n",
    "    def __init__(self, vocab_size=2048, d_model=768, n_heads=12, n_layers=12, \n",
    "                 d_ff=3072, max_seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights (GPT-2 style)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal mask to prevent looking at future tokens\"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        return mask == 0\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # Create position indices\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        pos_embeds = self.position_embedding(position_ids)\n",
    "        \n",
    "        hidden_states = self.dropout(token_embeds + pos_embeds)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(input_ids.device)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            hidden_states = block(hidden_states, causal_mask)\n",
    "        \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        logits = self.output_projection(hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift labels for next token prediction\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.vocab_size), \n",
    "                          shift_labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'loss': loss,\n",
    "            'hidden_states': hidden_states\n",
    "        }\n",
    "    \n",
    "    def generate(self, input_ids, max_length=100, temperature=1.0, top_k=50, top_p=0.9):\n",
    "        \"\"\"Generate tokens autoregressively\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                if input_ids.size(1) >= self.max_seq_len:\n",
    "                    break\n",
    "                    \n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                # Top-k filtering\n",
    "                if top_k > 0:\n",
    "                    values, indices = torch.topk(logits, top_k)\n",
    "                    logits[logits < values[:, [-1]]] = -float('inf')\n",
    "                \n",
    "                # Top-p filtering\n",
    "                if top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                    logits[indices_to_remove] = -float('inf')\n",
    "                \n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                \n",
    "        return input_ids\n",
    "\n",
    "# # Training utilities\n",
    "# class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, tokenized_sequences, seq_length=512):\n",
    "#         self.sequences = tokenized_sequences\n",
    "#         self.seq_length = seq_length\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         sequence = self.sequences[idx]\n",
    "        \n",
    "#         # Ensure sequence length\n",
    "#         if len(sequence) > self.seq_length:\n",
    "#             start_idx = torch.randint(0, len(sequence) - self.seq_length, (1,)).item()\n",
    "#             sequence = sequence[start_idx:start_idx + self.seq_length]\n",
    "#         else:\n",
    "#             # Pad if necessary\n",
    "#             padding = [0] * (self.seq_length - len(sequence))\n",
    "#             sequence = sequence + padding\n",
    "            \n",
    "#         return {\n",
    "#             'input_ids': torch.tensor(sequence, dtype=torch.long),\n",
    "#             'labels': torch.tensor(sequence, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, tokenizer, epochs=10, lr=1e-4, device='cuda'):\n",
    "#     \"\"\"Training loop for the time series GPT-2 model\"\"\"\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "#             x = batch_x.float().squeeze(-1)\n",
    "#             y = batch_y.float().squeeze(-1)\n",
    "#             # Tokenize input sequence\n",
    "#             input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            \n",
    "#             # Tokenize target sequence\n",
    "#             labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "#             # input_ids = batch['input_ids'].to(device)\n",
    "#             # labels = batch['labels'].to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids.cuda(), labels.cuda())\n",
    "#             loss = outputs['loss']\n",
    "#             loss.backward()\n",
    "            \n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             if i % 100 == 0:\n",
    "#                 print(f'Epoch {epoch}, Batch {i}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "#         scheduler.step()\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "#         # Validation\n",
    "#         if val_loader:\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(val_loader):\n",
    "#                     x = batch_x.float().squeeze(-1)\n",
    "#                     y = batch_y.float().squeeze(-1)\n",
    "#                     # Tokenize input sequence\n",
    "#                     input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "#                     # Tokenize target sequence\n",
    "#                     labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "#                     outputs = model(input_ids.cuda(), labels.cuda())\n",
    "#                     val_loss += outputs['loss'].item()\n",
    "            \n",
    "#             avg_val_loss = val_loss / len(val_loader)\n",
    "#             print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "#             model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "e_model = TimeSeriesGPT2(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    d_ff=128 * 2,\n",
    "    max_seq_len=seq_len,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in e_model.parameters())} parameters\")\n",
    "\n",
    "# Example of generating sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "e_model.to(device)\n",
    "\n",
    "# Sample input (batch_size=1, seq_len=10)\n",
    "input_tokens = torch.randint(0, 2048, (1, 10)).to(device)\n",
    "\n",
    "# Generate next tokens\n",
    "generated = e_model.generate(input_tokens, max_length=50, temperature=0.8)\n",
    "print(f\"Input length: {input_tokens.size(1)}\")\n",
    "print(f\"Generated length: {generated.size(1)}\")\n",
    "print(f\"Generated tokens: {generated[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c68142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    # Tokenize input sequence\n",
    "    input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "    \n",
    "    # Tokenize target sequence\n",
    "    labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "    # input_ids = batch['input_ids'].to(device)\n",
    "    # labels = batch['labels'].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f98bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(e_model, train_loader, validate_loader, tokenizer, epochs=5, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf27261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_model_directory(base_path=\"entropy_model\"):\n",
    "    \"\"\"Create model directory with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = os.path.join(base_path, f\"run_{timestamp}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    os.makedirs(os.path.join(model_dir, \"checkpoints\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(model_dir, \"plots\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(model_dir, \"logs\"), exist_ok=True)\n",
    "    \n",
    "    return model_dir\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    \"\"\"Get current learning rate\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, scheduler, epoch, loss, model_dir, is_best=False):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save regular checkpoint\n",
    "    checkpoint_path = os.path.join(model_dir, \"checkpoints\", f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_path = os.path.join(model_dir, \"checkpoints\", \"best_model.pt\")\n",
    "        torch.save(checkpoint, best_path)\n",
    "        \n",
    "    # Save latest model (for resuming)\n",
    "    latest_path = os.path.join(model_dir, \"checkpoints\", \"latest_model.pt\")\n",
    "    torch.save(checkpoint, latest_path)\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def plot_losses(train_losses, val_losses, learning_rates, model_dir):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Training and Validation Loss\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    if val_losses:\n",
    "        ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Learning Rate Schedule\n",
    "    ax2.plot(epochs, learning_rates, 'g-', linewidth=2)\n",
    "    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Loss (Log Scale)\n",
    "    ax3.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    if val_losses:\n",
    "        ax3.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax3.set_title('Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss (log)')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Training Progress\n",
    "    if len(train_losses) > 1:\n",
    "        loss_improvement = [(train_losses[i-1] - train_losses[i]) / train_losses[i-1] * 100 \n",
    "                           for i in range(1, len(train_losses))]\n",
    "        ax4.plot(epochs[1:], loss_improvement, 'purple', linewidth=2)\n",
    "        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax4.set_title('Loss Improvement (%)', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Improvement (%)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = os.path.join(model_dir, \"plots\", \"training_curves.png\")\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(model_dir, \"plots\", \"training_curves.pdf\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plot_path\n",
    "\n",
    "def save_training_config(config, model_dir):\n",
    "    \"\"\"Save training configuration\"\"\"\n",
    "    config_path = os.path.join(model_dir, \"training_config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    return config_path\n",
    "\n",
    "def sophisticated_train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    tokenizer,\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_epochs=5,\n",
    "    gradient_clip=1.0,\n",
    "    save_every=10,\n",
    "    patience=20,\n",
    "    min_delta=1e-4,\n",
    "    device='cuda',\n",
    "    model_name=\"timeseries_gpt2\",\n",
    "    use_tensorboard=True,\n",
    "    accumulation_steps=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Sophisticated training function with advanced features\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT-2 model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        tokenizer: Tokenizer for data transformation\n",
    "        epochs: Number of training epochs\n",
    "        lr: Initial learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        warmup_epochs: Number of warmup epochs\n",
    "        gradient_clip: Gradient clipping value\n",
    "        save_every: Save checkpoint every N epochs\n",
    "        patience: Early stopping patience\n",
    "        min_delta: Minimum improvement for early stopping\n",
    "        device: Training device\n",
    "        model_name: Name for the model\n",
    "        use_tensorboard: Whether to use tensorboard logging\n",
    "        accumulation_steps: Gradient accumulation steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create model directory\n",
    "    model_dir = create_model_directory()\n",
    "    print(f\"📁 Model directory created: {model_dir}\")\n",
    "    \n",
    "    # Setup tensorboard\n",
    "    if use_tensorboard:\n",
    "        writer = SummaryWriter(os.path.join(model_dir, \"logs\"))\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "    print(f\"🔢 Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Setup training configuration\n",
    "    config = {\n",
    "        'model_name': model_name,\n",
    "        'epochs': epochs,\n",
    "        'initial_lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'warmup_epochs': warmup_epochs,\n",
    "        'gradient_clip': gradient_clip,\n",
    "        'patience': patience,\n",
    "        'min_delta': min_delta,\n",
    "        'accumulation_steps': accumulation_steps,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'device': device,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save configuration\n",
    "    save_training_config(config, model_dir)\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Setup optimizer with different learning rates for different parts\n",
    "    param_groups = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if 'embedding' in n.lower()],\n",
    "            'lr': lr * 0.5,  # Lower LR for embeddings\n",
    "            'weight_decay': weight_decay * 0.1\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if 'embedding' not in n.lower()],\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(param_groups, eps=1e-8, betas=(0.9, 0.95))\n",
    "    \n",
    "    # Setup learning rate scheduler with warmup\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Training tracking variables\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"🚀 Starting training for {epochs} epochs...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            try:\n",
    "                # Data preprocessing\n",
    "                x = batch_x.float().squeeze(-1)\n",
    "                y = batch_y.float().squeeze(-1)\n",
    "                \n",
    "                # Tokenize sequences\n",
    "                input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "                labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "                \n",
    "                # Move to device\n",
    "                input_ids = input_ids.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, labels)\n",
    "                loss = outputs['loss'] / accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                total_train_loss += loss.item() * accumulation_steps\n",
    "                num_train_batches += 1\n",
    "                \n",
    "                # Log batch progress\n",
    "                if i % 100 == 0:\n",
    "                    current_lr = get_lr(optimizer)\n",
    "                    print(f\"Epoch {epoch+1:3d} | Batch {i:4d} | Loss: {loss.item()*accumulation_steps:.4f} | LR: {current_lr:.2e}\")\n",
    "                \n",
    "                # Tensorboard logging\n",
    "                if use_tensorboard and i % 50 == 0:\n",
    "                    global_step = epoch * len(train_loader) + i\n",
    "                    writer.add_scalar('Batch/TrainLoss', loss.item() * accumulation_steps, global_step)\n",
    "                    writer.add_scalar('Batch/LearningRate', get_lr(optimizer), global_step)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Handle remaining gradients\n",
    "        if num_train_batches % accumulation_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / max(num_train_batches, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss = 0\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            num_val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(val_loader):\n",
    "                    try:\n",
    "                        x = batch_x.float().squeeze(-1)\n",
    "                        y = batch_y.float().squeeze(-1)\n",
    "                        \n",
    "                        input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "                        labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "                        \n",
    "                        input_ids = input_ids.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        \n",
    "                        outputs = model(input_ids, labels)\n",
    "                        val_loss += outputs['loss'].item()\n",
    "                        num_val_batches += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Validation error in batch {i}: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_val_loss = val_loss / max(num_val_batches, 1)\n",
    "            val_losses.append(avg_val_loss)\n",
    "        else:\n",
    "            avg_val_loss = avg_train_loss\n",
    "            val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = get_lr(optimizer)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(\"─\" * 80)\n",
    "        print(f\"📊 Epoch {epoch+1:3d}/{epochs} Summary:\")\n",
    "        print(f\"   ├─ Train Loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"   ├─ Val Loss:   {avg_val_loss:.6f}\")\n",
    "        print(f\"   ├─ Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"   ├─ Epoch Time: {epoch_time:.1f}s\")\n",
    "        print(f\"   └─ Total Time: {(time.time() - start_time)/3600:.2f}h\")\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        if use_tensorboard:\n",
    "            writer.add_scalar('Epoch/TrainLoss', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Epoch/ValLoss', avg_val_loss, epoch)\n",
    "            writer.add_scalar('Epoch/LearningRate', current_lr, epoch)\n",
    "            writer.add_scalar('Epoch/EpochTime', epoch_time, epoch)\n",
    "        \n",
    "        # Model checkpointing\n",
    "        is_best = avg_val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            print(f\"🎉 New best validation loss: {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % save_every == 0 or is_best:\n",
    "            checkpoint_path = save_model_checkpoint(\n",
    "                model, optimizer, scheduler, epoch + 1, avg_val_loss, model_dir, is_best\n",
    "            )\n",
    "            if is_best:\n",
    "                print(f\"💾 Best model saved to: {checkpoint_path}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"🛑 Early stopping triggered after {patience} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Final model save\n",
    "    final_checkpoint = save_model_checkpoint(\n",
    "        model, optimizer, scheduler, epoch + 1, avg_val_loss, model_dir, is_best=False\n",
    "    )\n",
    "    \n",
    "    # Plot and save training curves\n",
    "    plot_path = plot_losses(train_losses, val_losses, learning_rates, model_dir)\n",
    "    \n",
    "    # Close tensorboard writer\n",
    "    if use_tensorboard:\n",
    "        writer.close()\n",
    "    \n",
    "    # Training summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n🏁 Training completed!\")\n",
    "    print(f\"📊 Final Results:\")\n",
    "    print(f\"   ├─ Best Validation Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ├─ Final Train Loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"   ├─ Total Training Time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"   ├─ Model Directory: {model_dir}\")\n",
    "    print(f\"   └─ Training Curves: {plot_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'model_dir': model_dir,\n",
    "        'final_epoch': epoch + 1,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8de748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage function\n",
    "def train_with_your_data(model, train_loader, val_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    Convenience function to train with your existing data setup\n",
    "    \"\"\"\n",
    "    return sophisticated_train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        epochs=15,\n",
    "        lr=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_epochs=2,\n",
    "        gradient_clip=1.0,\n",
    "        save_every=5,\n",
    "        patience=6,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_name=\"ETT_TimeSeries_GPT2\",\n",
    "        use_tensorboard=True,\n",
    "        accumulation_steps=2\n",
    "    )\n",
    "\n",
    "train_with_your_data(e_model, train_loader, validate_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b79d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152abe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    if i == 21:\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        # Tokenize input sequence\n",
    "        input_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "        \n",
    "        # Tokenize target sequence\n",
    "        labels, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "        # input_ids = batch['input_ids'].to(device)\n",
    "        # labels = batch['labels'].to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0750548",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8368aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = e_model.generate(input_ids[4:5, :452].cuda(), max_length=60, temperature=0.8)\n",
    "inputs = input_ids[:, :452].cpu().numpy()\n",
    "preds = gens[:, 452:].cpu().numpy()\n",
    "actual = input_ids[:, 452:].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_forecast_comparison(inputs, preds, actual, sample_indices=[0, 1, 2, 3], \n",
    "                           figsize=(16, 12), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot 4 time series lines showing inputs, predictions, and actual values\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences (context), shape: [batch_size, 452]\n",
    "        preds: Predicted sequences, shape: [4, 60] \n",
    "        actual: Actual sequences, shape: [batch_size, 60]\n",
    "        sample_indices: Which samples to plot (4 samples)\n",
    "        figsize: Figure size\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each of the 4 samples\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get data for this sample\n",
    "        input_seq = inputs[sample_idx] if sample_idx < len(inputs) else inputs[0]\n",
    "        pred_seq = preds[i]  # preds is already [4, 60]\n",
    "        actual_seq = actual[sample_idx] if sample_idx < len(actual) else actual[0]\n",
    "        \n",
    "        # Create time indices\n",
    "        input_time = np.arange(len(input_seq))\n",
    "        pred_time = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        actual_time = np.arange(len(input_seq), len(input_seq) + len(actual_seq))\n",
    "        \n",
    "        # Plot lines\n",
    "        ax.plot(input_time, input_seq, color=colors[0], linewidth=2.5, \n",
    "                label='Input Context (452 tokens)', alpha=0.8)\n",
    "        ax.plot(pred_time, pred_seq, color=colors[1], linewidth=2.5, \n",
    "                label='Predicted (60 tokens)', linestyle='--', alpha=0.9)\n",
    "        ax.plot(actual_time, actual_seq, color=colors[2], linewidth=2.5, \n",
    "                label='Actual (60 tokens)', alpha=0.9)\n",
    "        \n",
    "        # Add vertical line at forecast start\n",
    "        ax.axvline(x=len(input_seq), color='black', linestyle=':', \n",
    "                   alpha=0.6, linewidth=1.5, label='Forecast Start')\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f'Sample {sample_idx + 1}: Forecast vs Actual', \n",
    "                     fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Time Steps', fontsize=12)\n",
    "        ax.set_ylabel('Token Values', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper left', fontsize=10)\n",
    "        \n",
    "        # Add some statistics\n",
    "        if len(pred_seq) == len(actual_seq):\n",
    "            mae = np.mean(np.abs(pred_seq - actual_seq))\n",
    "            rmse = np.sqrt(np.mean((pred_seq - actual_seq) ** 2))\n",
    "            \n",
    "            # Add text box with metrics\n",
    "            textstr = f'MAE: {mae:.2f}\\nRMSE: {rmse:.2f}'\n",
    "            props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('Time Series Forecasting: Input Context → Predictions vs Actual', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"💾 Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_all_four_in_one(inputs, preds, actual, sample_indices=[0, 1, 2, 3], \n",
    "                        figsize=(20, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot all 4 samples in a single plot for easy comparison\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences, shape: [batch_size, 452]\n",
    "        preds: Predicted sequences, shape: [4, 60]\n",
    "        actual: Actual sequences, shape: [batch_size, 60]\n",
    "        sample_indices: Which samples to plot\n",
    "        figsize: Figure size\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot each sample with offset for visibility\n",
    "    offset_step = 50  # Vertical offset between samples\n",
    "    \n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        # Get data\n",
    "        input_seq = inputs[sample_idx] if sample_idx < len(inputs) else inputs[0]\n",
    "        pred_seq = preds[i]\n",
    "        actual_seq = actual[sample_idx] if sample_idx < len(actual) else actual[0]\n",
    "        \n",
    "        # Apply offset\n",
    "        offset = i * offset_step\n",
    "        input_seq_offset = input_seq + offset\n",
    "        pred_seq_offset = pred_seq + offset\n",
    "        actual_seq_offset = actual_seq + offset\n",
    "        \n",
    "        # Time indices\n",
    "        input_time = np.arange(len(input_seq))\n",
    "        pred_time = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        actual_time = np.arange(len(input_seq), len(input_seq) + len(actual_seq))\n",
    "        \n",
    "        # Plot with sample-specific colors\n",
    "        sample_color = colors[i]\n",
    "        \n",
    "        # Input context\n",
    "        ax.plot(input_time, input_seq_offset, color=sample_color, \n",
    "                linewidth=2, alpha=0.7, label=f'Sample {sample_idx + 1} - Input')\n",
    "        \n",
    "        # Predictions\n",
    "        ax.plot(pred_time, pred_seq_offset, color=sample_color, \n",
    "                linewidth=2.5, linestyle='--', alpha=0.9, \n",
    "                label=f'Sample {sample_idx + 1} - Predicted')\n",
    "        \n",
    "        # Actual values\n",
    "        ax.plot(actual_time, actual_seq_offset, color=sample_color, \n",
    "                linewidth=2.5, linestyle='-', alpha=0.9, \n",
    "                label=f'Sample {sample_idx + 1} - Actual')\n",
    "        \n",
    "        # Forecast start line\n",
    "        ax.axvline(x=len(input_seq), color=sample_color, linestyle=':', \n",
    "                   alpha=0.5, linewidth=1.5)\n",
    "        \n",
    "        # Add sample label\n",
    "        ax.text(10, offset + np.mean(input_seq), f'Sample {sample_idx + 1}', \n",
    "                fontsize=12, fontweight='bold', color=sample_color,\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Overall forecast start line\n",
    "    ax.axvline(x=452, color='black', linestyle=':', linewidth=2, alpha=0.8,\n",
    "               label='Forecast Start')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title('All 4 Samples: Input Context → Predictions vs Actual', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Time Steps', fontsize=14)\n",
    "    ax.set_ylabel('Token Values (with offset)', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"💾 Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_overlay_comparison(inputs, preds, actual, sample_indices=[0, 1, 2, 3], \n",
    "                           figsize=(16, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot all samples overlaid for direct comparison\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequences, shape: [batch_size, 452]\n",
    "        preds: Predicted sequences, shape: [4, 60]\n",
    "        actual: Actual sequences, shape: [batch_size, 60]\n",
    "        sample_indices: Which samples to plot\n",
    "        figsize: Figure size\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: Full sequences\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        input_seq = inputs[sample_idx] if sample_idx < len(inputs) else inputs[0]\n",
    "        pred_seq = preds[i]\n",
    "        actual_seq = actual[sample_idx] if sample_idx < len(actual) else actual[0]\n",
    "        \n",
    "        input_time = np.arange(len(input_seq))\n",
    "        pred_time = np.arange(len(input_seq), len(input_seq) + len(pred_seq))\n",
    "        actual_time = np.arange(len(input_seq), len(input_seq) + len(actual_seq))\n",
    "        \n",
    "        sample_color = colors[i]\n",
    "        \n",
    "        # Plot input context\n",
    "        ax1.plot(input_time, input_seq, color=sample_color, alpha=0.6, \n",
    "                linewidth=1.5, label=f'Sample {sample_idx + 1} Input')\n",
    "        \n",
    "        # Plot predictions and actual\n",
    "        ax1.plot(pred_time, pred_seq, color=sample_color, linestyle='--', \n",
    "                linewidth=2, alpha=0.8, label=f'Sample {sample_idx + 1} Pred')\n",
    "        ax1.plot(actual_time, actual_seq, color=sample_color, linestyle='-', \n",
    "                linewidth=2, alpha=0.9, label=f'Sample {sample_idx + 1} Actual')\n",
    "    \n",
    "    ax1.axvline(x=452, color='black', linestyle=':', linewidth=2, alpha=0.8)\n",
    "    ax1.set_title('Full Sequences: All Samples Overlaid', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Time Steps')\n",
    "    ax1.set_ylabel('Token Values')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Forecast region only (zoomed)\n",
    "    for i, sample_idx in enumerate(sample_indices):\n",
    "        pred_seq = preds[i]\n",
    "        actual_seq = actual[sample_idx] if sample_idx < len(actual) else actual[0]\n",
    "        \n",
    "        pred_time = np.arange(len(pred_seq))\n",
    "        actual_time = np.arange(len(actual_seq))\n",
    "        \n",
    "        sample_color = colors[i]\n",
    "        \n",
    "        ax2.plot(pred_time, pred_seq, color=sample_color, linestyle='--', \n",
    "                linewidth=2.5, alpha=0.8, marker='o', markersize=3,\n",
    "                label=f'Sample {sample_idx + 1} Predicted')\n",
    "        ax2.plot(actual_time, actual_seq, color=sample_color, linestyle='-', \n",
    "                linewidth=2.5, alpha=0.9, marker='s', markersize=3,\n",
    "                label=f'Sample {sample_idx + 1} Actual')\n",
    "    \n",
    "    ax2.set_title('Forecast Region (60 tokens): Predictions vs Actual', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Forecast Steps (0-59)')\n",
    "    ax2.set_ylabel('Token Values')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"💾 Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage with your data\n",
    "def plot_your_data(inputs, preds, actual):\n",
    "    \"\"\"\n",
    "    Convenience function to plot your specific data\n",
    "    \n",
    "    Args:\n",
    "        inputs: shape [batch_size, 452]\n",
    "        preds: shape [4, 60] \n",
    "        actual: shape [batch_size, 60]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 Creating forecast visualization plots...\")\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    print(f\"Predictions shape: {preds.shape}\")\n",
    "    print(f\"Actual shape: {actual.shape}\")\n",
    "    \n",
    "    # Plot 1: Individual subplots for each sample\n",
    "    print(\"\\n1. Individual sample plots...\")\n",
    "    fig1 = plot_forecast_comparison(inputs, preds, actual, \n",
    "                                   save_path=\"forecast_individual.png\")\n",
    "    \n",
    "    # Plot 2: All samples in one plot with offset\n",
    "    print(\"\\n2. All samples with offset...\")\n",
    "    fig2 = plot_all_four_in_one(inputs, preds, actual,\n",
    "                               save_path=\"forecast_all_offset.png\")\n",
    "    \n",
    "    # Plot 3: Overlaid comparison\n",
    "    print(\"\\n3. Overlaid comparison...\")\n",
    "    fig3 = plot_overlay_comparison(inputs, preds, actual,\n",
    "                                  save_path=\"forecast_overlay.png\")\n",
    "    \n",
    "    return fig1, fig2, fig3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_your_data(inputs, preds, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf76ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85042e2e",
   "metadata": {},
   "source": [
    "## Training Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194c67f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 718/718 [00:31<00:00, 23.03it/s, loss=2.3185, avg_loss=3.3825, lr=0.000489]\n",
      "Epoch 1 Validation: 100%|██████████| 180/180 [00:07<00:00, 24.27it/s, loss=1.9351, avg_loss=2.0913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 Results:\n",
      "Training Loss: 3.3825 (Time: 31.26s)\n",
      "Validation Loss: 0.0405, (Time: 7.48s)\n",
      "New best model saved with validation loss: 0.0405\n",
      "Checkpoint saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 718/718 [00:31<00:00, 22.69it/s, loss=1.9890, avg_loss=2.1079, lr=0.000457]\n",
      "Epoch 2 Validation: 100%|██████████| 180/180 [00:07<00:00, 24.08it/s, loss=1.9402, avg_loss=1.9154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 Results:\n",
      "Training Loss: 2.1079 (Time: 31.72s)\n",
      "Validation Loss: 0.0314, (Time: 7.52s)\n",
      "New best model saved with validation loss: 0.0314\n",
      "Checkpoint saved at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 718/718 [00:31<00:00, 22.72it/s, loss=1.6389, avg_loss=1.8210, lr=0.000407]\n",
      "Epoch 3 Validation: 100%|██████████| 180/180 [00:07<00:00, 24.26it/s, loss=2.0215, avg_loss=1.8987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 Results:\n",
      "Training Loss: 1.8210 (Time: 31.68s)\n",
      "Validation Loss: 0.0321, (Time: 7.47s)\n",
      "Checkpoint saved at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 718/718 [00:31<00:00, 22.85it/s, loss=1.5845, avg_loss=1.6211, lr=0.000345]\n",
      "Epoch 4 Validation: 100%|██████████| 180/180 [00:07<00:00, 24.31it/s, loss=1.7967, avg_loss=1.9494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 Results:\n",
      "Training Loss: 1.6211 (Time: 31.50s)\n",
      "Validation Loss: 0.0321, (Time: 7.46s)\n",
      "Checkpoint saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training:   4%|▍         | 30/718 [00:01<00:31, 21.72it/s, loss=1.4456, avg_loss=1.4607, lr=0.000342]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     loss = loss / grad_accumulation_steps\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     total_loss += loss.item() * grad_accumulation_steps\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/blt_250513/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "total_steps = epochs * num_batches\n",
    "min_lr = learning_rate * min_lr_factor\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Training\", position=0, leave=True)\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in progress_bar:\n",
    "        iteration = epoch * num_batches + i\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = get_lr(\n",
    "            iteration,\n",
    "            total_steps,\n",
    "            warmup_steps,\n",
    "            learning_rate,\n",
    "            min_lr,\n",
    "            decay_lr\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for micro_step in range(grad_accumulation_steps):\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "            # with ctx:\n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to(device), patch_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                target_token_ids.reshape(-1).to(device),\n",
    "                ignore_index=PAD_ID\n",
    "            )\n",
    "            \n",
    "            # Scale loss\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "                \n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "        # Update weights with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update training metrics\n",
    "        epoch_loss += total_loss\n",
    "        avg_epoch_loss = epoch_loss / (i + 1)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{total_loss:.4f}\",\n",
    "            'avg_loss': f\"{avg_epoch_loss:.4f}\",\n",
    "            'lr': f\"{lr:.6f}\"\n",
    "        })\n",
    "    \n",
    "    # Calculate training time and average loss\n",
    "    train_time = time.time() - t1\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    t1 = time.time()\n",
    "    model.eval()\n",
    "    val_loss = validate(model, validate_loader, tokenizer, patch_lengths, device, desc=f\"Epoch {epoch+1} Validation\")\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"Training Loss: {train_avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, (Time: {val_time:.2f}s)\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss\n",
    "        }, os.path.join(output_dir, f'best_model_{seq_len}.pth'))\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if save_every > 0 and (epoch + 1) % save_every == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss\n",
    "        }, os.path.join(output_dir, f'checkpoint_{seq_len}_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206880c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6902d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model on test set...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.031404678)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and evaluate best model\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "checkpoint = torch.load(os.path.join(output_dir, f\"best_model_{seq_len}.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3112db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteLatentTransformer(\n",
       "  (local_encoder): LocalEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wk): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (w3): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (w2): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=256, out_features=512, bias=False)\n",
       "    (tok_embeddings): Embedding(4096, 256)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-1): 2 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wk): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_transformer): GlobalTransformer(\n",
       "    (rope_embeddings): RotaryEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=1536, out_features=512, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (local_decoder): LocalDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wk): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (w3): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (w2): Linear(in_features=768, out_features=256, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (rope): RotaryEmbedding()\n",
       "    (patch_embedding_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (cross_attn_layers): ModuleList(\n",
       "      (0-1): 2 x CrossAttention(\n",
       "        (cross_attn_norm_q): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_norm_kv): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (wq): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wk): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=4096, bias=False)\n",
       "  )\n",
       "  (encoder_hash_tok_embedding): ModuleList(\n",
       "    (0-4): 5 x Embedding(32768, 256)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75ca81",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d49f357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b88f532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 96]), torch.Size([64, 12]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape, patch_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eb9000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "    patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "    # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "    target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94655cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 96])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_ids.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b95d906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 64, 96])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.output_transform(token_ids, tokenizer_state).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1791d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 96])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c96754",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f56daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from contextlib import nullcontext\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18f07fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_len = 96\n",
    "# Create a new config with prediction_length=1\n",
    "test_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=seq_len,\n",
    "    prediction_length=pred_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, test_tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ce81803",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = batch_size\n",
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': 'ETTm2',\n",
    "    'data' : 'ETTm2',\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': 'ETTm2.csv',\n",
    "    'features': 'S',\n",
    "    'seq_len': seq_len,\n",
    "    'label_len': 0,\n",
    "    'pred_len': pred_len\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=eval_batch_size,\n",
    "    freq='m',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "len(test_loader.dataset) # = 11425\n",
    "\n",
    "l = torch.full((eval_batch_size,12), 8).to('cuda')\n",
    "l[:,0] = 1\n",
    "l[:,-1] = 10\n",
    "l[:,2] = 10\n",
    "l[:,1] = 11\n",
    "patch_lengths = l\n",
    "patch_lengths.shape, patch_lengths[0].sum().item()\n",
    "assert patch_lengths[0].sum().item() == token_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eac60d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(96, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_lengths[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4130b0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 96]), torch.Size([64, 96]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    if i == 0:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22dede25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - MSE: 0.5401, MAE: 0.5711\n",
      "Batch 1 - MSE: 0.3527, MAE: 0.4435\n",
      "Batch 2 - MSE: 0.4262, MAE: 0.4742\n",
      "Batch 3 - MSE: 0.3886, MAE: 0.4761\n",
      "Batch 4 - MSE: 0.2402, MAE: 0.3789\n",
      "Batch 5 - MSE: 0.3546, MAE: 0.4738\n",
      "Batch 6 - MSE: 0.4302, MAE: 0.5199\n",
      "Batch 7 - MSE: 0.2311, MAE: 0.3715\n",
      "Batch 8 - MSE: 0.4588, MAE: 0.5095\n",
      "Batch 9 - MSE: 0.5251, MAE: 0.5383\n",
      "Batch 10 - MSE: 0.1738, MAE: 0.2612\n",
      "Batch 11 - MSE: 0.4199, MAE: 0.4031\n",
      "Batch 12 - MSE: 1.0674, MAE: 0.8104\n",
      "Batch 13 - MSE: 0.5144, MAE: 0.5168\n",
      "Batch 14 - MSE: 0.4498, MAE: 0.5149\n",
      "Batch 15 - MSE: 0.3721, MAE: 0.4577\n",
      "Batch 16 - MSE: 0.2099, MAE: 0.3735\n",
      "Batch 17 - MSE: 0.4545, MAE: 0.5191\n",
      "Batch 18 - MSE: 0.2453, MAE: 0.3331\n",
      "Batch 19 - MSE: 0.3096, MAE: 0.3971\n",
      "Batch 20 - MSE: 0.3807, MAE: 0.4044\n",
      "Batch 21 - MSE: 0.1086, MAE: 0.2274\n",
      "Batch 22 - MSE: 0.2648, MAE: 0.3683\n",
      "Batch 23 - MSE: 0.4029, MAE: 0.4874\n",
      "Batch 24 - MSE: 0.2812, MAE: 0.4093\n",
      "Batch 25 - MSE: 0.1716, MAE: 0.3124\n",
      "Batch 26 - MSE: 0.1267, MAE: 0.2694\n",
      "Batch 27 - MSE: 0.1155, MAE: 0.2575\n",
      "Batch 28 - MSE: 0.0816, MAE: 0.2045\n",
      "Batch 29 - MSE: 0.0610, MAE: 0.1256\n",
      "Batch 30 - MSE: 0.0461, MAE: 0.1714\n",
      "Batch 31 - MSE: 0.0329, MAE: 0.1337\n",
      "Batch 32 - MSE: 0.0936, MAE: 0.2213\n",
      "Batch 33 - MSE: 0.2662, MAE: 0.3632\n",
      "Batch 34 - MSE: 0.0784, MAE: 0.1745\n",
      "Batch 35 - MSE: 0.0160, MAE: 0.0739\n",
      "Batch 36 - MSE: 0.1946, MAE: 0.3036\n",
      "Batch 37 - MSE: 0.5562, MAE: 0.6154\n",
      "Batch 38 - MSE: 0.2740, MAE: 0.4197\n",
      "Batch 39 - MSE: 0.0726, MAE: 0.1959\n",
      "Batch 40 - MSE: 0.1854, MAE: 0.3398\n",
      "Batch 41 - MSE: 0.2046, MAE: 0.2457\n",
      "Batch 42 - MSE: 0.0779, MAE: 0.1888\n",
      "Batch 43 - MSE: 0.3506, MAE: 0.4460\n",
      "Batch 44 - MSE: 0.3690, MAE: 0.3826\n",
      "Batch 45 - MSE: 0.3051, MAE: 0.4577\n",
      "Batch 46 - MSE: 0.2255, MAE: 0.3695\n",
      "Batch 47 - MSE: 0.3539, MAE: 0.3618\n",
      "Batch 48 - MSE: 0.2397, MAE: 0.3553\n",
      "Batch 49 - MSE: 0.3050, MAE: 0.4085\n",
      "Batch 50 - MSE: 0.2656, MAE: 0.3639\n",
      "Batch 51 - MSE: 0.1306, MAE: 0.2323\n",
      "Batch 52 - MSE: 0.0703, MAE: 0.1965\n",
      "Batch 53 - MSE: 0.0391, MAE: 0.1437\n",
      "Batch 54 - MSE: 0.1302, MAE: 0.2908\n",
      "Batch 55 - MSE: 0.1237, MAE: 0.2987\n",
      "Batch 56 - MSE: 0.0924, MAE: 0.2417\n",
      "Batch 57 - MSE: 0.0793, MAE: 0.2275\n",
      "Batch 58 - MSE: 0.0747, MAE: 0.2029\n",
      "Batch 59 - MSE: 0.1721, MAE: 0.3028\n",
      "Batch 60 - MSE: 0.1280, MAE: 0.2644\n",
      "Batch 61 - MSE: 0.1318, MAE: 0.2763\n",
      "Batch 62 - MSE: 0.1442, MAE: 0.3118\n",
      "Batch 63 - MSE: 0.3378, MAE: 0.4534\n",
      "Batch 64 - MSE: 0.1419, MAE: 0.2783\n",
      "Batch 65 - MSE: 0.0923, MAE: 0.2342\n",
      "Batch 66 - MSE: 0.1142, MAE: 0.2707\n",
      "Batch 67 - MSE: 0.1753, MAE: 0.3331\n",
      "Batch 68 - MSE: 0.1808, MAE: 0.3506\n",
      "Batch 69 - MSE: 0.1496, MAE: 0.2639\n",
      "Batch 70 - MSE: 0.5353, MAE: 0.5641\n",
      "Batch 71 - MSE: 0.3504, MAE: 0.4407\n",
      "Batch 72 - MSE: 0.2767, MAE: 0.3835\n",
      "Batch 73 - MSE: 0.1626, MAE: 0.3061\n",
      "Batch 74 - MSE: 0.1040, MAE: 0.2558\n",
      "Batch 75 - MSE: 0.0668, MAE: 0.2015\n",
      "Batch 76 - MSE: 0.0407, MAE: 0.1510\n",
      "Batch 77 - MSE: 0.0411, MAE: 0.1511\n",
      "Batch 78 - MSE: 0.0407, MAE: 0.1566\n",
      "Batch 79 - MSE: 0.0517, MAE: 0.1481\n",
      "Batch 80 - MSE: 0.2311, MAE: 0.3783\n",
      "Batch 81 - MSE: 0.3232, MAE: 0.3598\n",
      "Batch 82 - MSE: 0.2025, MAE: 0.3273\n",
      "Batch 83 - MSE: 0.2670, MAE: 0.3581\n",
      "Batch 84 - MSE: 0.2491, MAE: 0.3765\n",
      "Batch 85 - MSE: 0.2129, MAE: 0.3383\n",
      "Batch 86 - MSE: 0.2658, MAE: 0.3637\n",
      "Batch 87 - MSE: 0.1898, MAE: 0.2954\n",
      "Batch 88 - MSE: 0.4562, MAE: 0.4851\n",
      "Batch 89 - MSE: 0.3579, MAE: 0.3425\n",
      "Batch 90 - MSE: 0.1677, MAE: 0.2912\n",
      "Batch 91 - MSE: 0.1715, MAE: 0.2796\n",
      "Batch 92 - MSE: 0.3771, MAE: 0.4925\n",
      "Batch 93 - MSE: 0.1862, MAE: 0.3364\n",
      "Batch 94 - MSE: 0.2931, MAE: 0.4078\n",
      "Batch 95 - MSE: 0.2575, MAE: 0.3125\n",
      "Batch 96 - MSE: 0.0890, MAE: 0.2150\n",
      "Batch 97 - MSE: 0.0438, MAE: 0.1639\n",
      "Batch 98 - MSE: 0.0407, MAE: 0.1492\n",
      "Batch 99 - MSE: 0.0624, MAE: 0.2094\n",
      "Batch 100 - MSE: 0.0654, MAE: 0.1703\n",
      "Batch 101 - MSE: 0.2233, MAE: 0.4201\n",
      "Batch 102 - MSE: 0.1455, MAE: 0.2700\n",
      "Batch 103 - MSE: 0.2037, MAE: 0.3447\n",
      "Batch 104 - MSE: 0.2194, MAE: 0.3864\n",
      "Batch 105 - MSE: 0.1149, MAE: 0.2684\n",
      "Batch 106 - MSE: 0.4041, MAE: 0.5231\n",
      "Batch 107 - MSE: 0.7475, MAE: 0.7612\n",
      "Batch 108 - MSE: 0.1325, MAE: 0.2639\n",
      "Batch 109 - MSE: 0.1009, MAE: 0.2430\n",
      "Batch 110 - MSE: 0.0704, MAE: 0.1982\n",
      "Batch 111 - MSE: 0.0545, MAE: 0.1815\n",
      "Batch 112 - MSE: 0.0426, MAE: 0.1617\n",
      "Batch 113 - MSE: 0.0418, MAE: 0.1538\n",
      "Batch 114 - MSE: 0.0779, MAE: 0.2145\n",
      "Batch 115 - MSE: 0.2065, MAE: 0.3341\n",
      "Batch 116 - MSE: 0.1615, MAE: 0.2967\n",
      "Batch 117 - MSE: 0.2146, MAE: 0.3746\n",
      "Batch 118 - MSE: 0.2122, MAE: 0.3560\n",
      "Batch 119 - MSE: 0.1805, MAE: 0.3361\n",
      "Batch 120 - MSE: 0.1656, MAE: 0.3243\n",
      "Batch 121 - MSE: 0.1161, MAE: 0.2672\n",
      "Batch 122 - MSE: 0.2999, MAE: 0.4042\n",
      "Batch 123 - MSE: 0.3180, MAE: 0.4208\n",
      "Batch 124 - MSE: 0.4897, MAE: 0.5406\n",
      "Batch 125 - MSE: 0.4221, MAE: 0.4271\n",
      "Batch 126 - MSE: 0.3712, MAE: 0.4244\n",
      "Batch 127 - MSE: 0.1361, MAE: 0.2852\n",
      "Batch 128 - MSE: 0.0814, MAE: 0.2331\n",
      "Batch 129 - MSE: 0.1057, MAE: 0.1836\n",
      "Batch 130 - MSE: 0.0463, MAE: 0.1715\n",
      "Batch 131 - MSE: 0.0366, MAE: 0.1508\n",
      "Batch 132 - MSE: 0.0350, MAE: 0.1457\n",
      "Batch 133 - MSE: 0.0519, MAE: 0.1667\n",
      "Batch 134 - MSE: 0.1268, MAE: 0.2819\n",
      "Batch 135 - MSE: 0.0560, MAE: 0.1758\n",
      "Batch 136 - MSE: 0.0975, MAE: 0.2060\n",
      "Batch 137 - MSE: 0.1897, MAE: 0.3544\n",
      "Batch 138 - MSE: 0.2260, MAE: 0.4005\n",
      "Batch 139 - MSE: 0.1939, MAE: 0.3575\n",
      "Batch 140 - MSE: 0.0784, MAE: 0.2106\n",
      "Batch 141 - MSE: 0.0797, MAE: 0.2258\n",
      "Batch 142 - MSE: 0.0457, MAE: 0.1584\n",
      "Batch 143 - MSE: 0.0597, MAE: 0.1770\n",
      "Batch 144 - MSE: 0.0509, MAE: 0.1724\n",
      "Batch 145 - MSE: 0.0775, MAE: 0.2122\n",
      "Batch 146 - MSE: 0.1500, MAE: 0.3046\n",
      "Batch 147 - MSE: 0.1052, MAE: 0.2466\n",
      "Batch 148 - MSE: 0.0780, MAE: 0.1866\n",
      "Batch 149 - MSE: 0.2337, MAE: 0.3377\n",
      "Batch 150 - MSE: 0.4797, MAE: 0.4922\n",
      "Batch 151 - MSE: 0.3862, MAE: 0.4615\n",
      "Batch 152 - MSE: 0.1310, MAE: 0.2789\n",
      "Batch 153 - MSE: 0.2913, MAE: 0.4464\n",
      "Batch 154 - MSE: 0.1494, MAE: 0.3024\n",
      "Batch 155 - MSE: 0.2207, MAE: 0.3345\n",
      "Batch 156 - MSE: 0.2084, MAE: 0.3380\n",
      "Batch 157 - MSE: 0.2131, MAE: 0.3459\n",
      "Batch 158 - MSE: 0.1541, MAE: 0.2900\n",
      "Batch 159 - MSE: 0.1420, MAE: 0.2842\n",
      "Batch 160 - MSE: 0.2309, MAE: 0.3606\n",
      "Batch 161 - MSE: 0.1595, MAE: 0.2757\n",
      "Batch 162 - MSE: 0.1655, MAE: 0.3126\n",
      "Batch 163 - MSE: 0.1100, MAE: 0.2482\n",
      "Batch 164 - MSE: 0.2437, MAE: 0.4111\n",
      "Batch 165 - MSE: 0.1195, MAE: 0.2598\n",
      "Batch 166 - MSE: 0.2075, MAE: 0.3477\n",
      "Batch 167 - MSE: 0.2897, MAE: 0.3905\n",
      "Batch 168 - MSE: 0.3503, MAE: 0.4335\n",
      "Batch 169 - MSE: 0.5661, MAE: 0.5622\n",
      "Batch 170 - MSE: 0.1155, MAE: 0.2355\n",
      "Batch 171 - MSE: 0.3614, MAE: 0.4695\n",
      "Batch 172 - MSE: 0.1575, MAE: 0.2980\n",
      "Batch 173 - MSE: 0.1189, MAE: 0.2297\n",
      "Batch 174 - MSE: 0.1286, MAE: 0.2606\n",
      "Batch 175 - MSE: 0.0635, MAE: 0.1806\n",
      "Batch 176 - MSE: 0.1267, MAE: 0.2849\n",
      "Batch 177 - MSE: 0.0361, MAE: 0.1365\n",
      "\n",
      "Overall Results (96) (96):\n",
      "Average MSE: 0.2099\n",
      "Average MAE: 0.3208\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "temperature = 1.0\n",
    "top_k = 5\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = pred_len \n",
    "B, L = token_ids.shape\n",
    "_, num_patches = patch_lengths.shape\n",
    "\n",
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    # Move the batch to the same device as the model\n",
    "    try:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        # patch_lengths = patch_lengths[0].unsqueeze(0).to(device)\n",
    "        # tokenizer_state = tokenizer_state[0].unsqueeze(0).to(device)\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_ids = token_ids_start\n",
    "            forecast = torch.zeros((B, max_new_tokens), dtype=torch.long).to(device)\n",
    "            for _ in range(max_new_tokens):\n",
    "                # print(_)\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids.to(device), patch_lengths)\n",
    "                next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # Apply top-k filtering if specified\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    v, __ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                    next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # Apply softmax to convert to probabilities\n",
    "                probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                # print(next_token.shape, forecast.shape, _)\n",
    "                forecast[:, _] = next_token.squeeze(-1)\n",
    "                # Update token sequence and patch lengths\n",
    "                all_tokens = torch.cat([token_ids.to(device), next_token], dim=1)\n",
    "                look_back = max(pred_len, all_tokens.shape[1])\n",
    "                token_ids = all_tokens[:, -1*seq_len:]  # Keep last seq_len tokens\n",
    "\n",
    "            \n",
    "            # Extract the forecasted and actual tokens\n",
    "            actual = target_token_ids\n",
    "\n",
    "            \n",
    "            # Convert tokens back to values using inverse transform\n",
    "            actual_values = tokenizer.output_transform(actual.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "            forecast_values = tokenizer.output_transform(forecast.to('cpu').unsqueeze(1), tokenizer_state.to('cpu'))\n",
    "        \n",
    "            # Ensure they have the same shape by trimming if necessary\n",
    "            # min_len = min(actual_values.shape[1], forecast_values.shape[1])\n",
    "            # actual_values = actual_values[:, :min_len]\n",
    "            # forecast_values = forecast_values[:, :min_len]\n",
    "            \n",
    "            # Calculate MSE and MAE\n",
    "            mse = torch.mean((actual_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(actual_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print for the current batch\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results ({seq_len}) ({pred_len}):\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(f\"forecast_metrics_{seq_len}_{pred_len}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2309b59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset_name\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65592502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics across all batches\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "\n",
    "print(f\"\\nOverall Results:\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Optionally, you can save the metrics to a file\n",
    "import json\n",
    "with open(\"forecast_metrics.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.output_transform(actual.to('cpu'), tokenizer_state.to('cpu')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74407ee9",
   "metadata": {},
   "source": [
    "# Deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # must be BEFORE torch/TF import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from bytelatent.model.blt import ByteLatentTransformerArgs, ByteLatentTransformer\n",
    "from chronos import MeanScaleUniformBins, ChronosConfig\n",
    "from utils.train_utils import get_lr, validate\n",
    "from data_provider.data_factory import data_provider\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "torch.cuda.set_device(0)   # 0 here means “the first visible GPU”, i.e. physical #3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from bytelatent.tokenizers.constants import PAD_ID\n",
    "\n",
    "## Training Args\n",
    "vocab_size = 4096\n",
    "batch_size = 128\n",
    "seq_len = 512\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-2\n",
    "epochs = 10\n",
    "grad_accumulation_steps = 1\n",
    "clip_grad = 1.0\n",
    "seed = 42\n",
    "warmup_steps = 0\n",
    "min_lr_factor = 0.1\n",
    "decay_lr = True\n",
    "compile = True\n",
    "output_dir = \"output\"\n",
    "save_every = 1\n",
    "compile = False\n",
    "\n",
    "# Model Args\n",
    "dim = 128\n",
    "vocab_size=vocab_size\n",
    "max_length=seq_len\n",
    "local_attention_window_len=seq_len\n",
    "max_seqlen=seq_len\n",
    "max_encoder_seq_length=seq_len\n",
    "\n",
    "dim_local_encoder=64\n",
    "dim_local_decoder=64\n",
    "n_layers_global = 4\n",
    "n_layers_local = 4\n",
    "n_layers_global=4\n",
    "n_layers_local_decoder=2\n",
    "n_layers_local_encoder=2\n",
    "cross_attn_k=2\n",
    "cross_attn_nheads=4\n",
    "patch_size=8\n",
    "\n",
    "patching_mode=\"static\"\n",
    "tie_local_encoder_decoder_logits=False\n",
    "patch_in_forward=False\n",
    "pad_to_max_length=True\n",
    "patching_threshold=3.1439168453216553\n",
    "\n",
    "encoder_hash_byte_group_size=[4,5,6]\n",
    "encoder_hash_byte_group_vocab=2**7\n",
    "encoder_hash_byte_group_nb_functions=1\n",
    "encoder_enable_byte_ngrams=False\n",
    "\n",
    "cross_attn_encoder=True\n",
    "cross_attn_decoder=True\n",
    "cross_attn_window_encoder=None\n",
    "cross_attn_window_decoder=None\n",
    "cross_attn_all_layers_decoder=True\n",
    "cross_attn_all_layers_encoder=True\n",
    "cross_attn_use_flex_attention=False\n",
    "cross_attn_init_by_pooling=True\n",
    "\n",
    "log_patch_lengths=True\n",
    "non_linearity=\"swiglu\"\n",
    "use_rope=True\n",
    "recompute_fc1_out=False\n",
    "recompute_fc3_out=False\n",
    "recompute_attn=False\n",
    "custom_bwd=False\n",
    "layer_ckpt=\"none\"\n",
    "use_local_encoder_transformer=True\n",
    "init_use_gaussian=True\n",
    "init_use_depth=\"current\"\n",
    "attn_impl=\"sdpa\"\n",
    "attn_bias_type=\"causal\"\n",
    "alpha_depth=\"disabled\"\n",
    "\n",
    "downsampling_by_pooling=\"max\"\n",
    "\n",
    "\n",
    "model_args = ByteLatentTransformerArgs(\n",
    "    seed=seed,\n",
    "    vocab_size=vocab_size,\n",
    "    dim=dim, \n",
    "    n_layers_global=n_layers_global,  \n",
    "    n_layers_local_decoder=n_layers_local_decoder, \n",
    "    n_layers_local_encoder=n_layers_local_encoder,  \n",
    "    patch_size=patch_size,\n",
    "    patching_mode=patching_mode,\n",
    "    tie_local_encoder_decoder_logits=tie_local_encoder_decoder_logits,\n",
    "    patch_in_forward=patch_in_forward,\n",
    "    max_encoder_seq_length=max_encoder_seq_length,\n",
    "    pad_to_max_length=pad_to_max_length,\n",
    "    patching_threshold=patching_threshold,\n",
    "    encoder_hash_byte_group_size=encoder_hash_byte_group_size,\n",
    "    encoder_hash_byte_group_vocab=encoder_hash_byte_group_vocab,\n",
    "    encoder_hash_byte_group_nb_functions=encoder_hash_byte_group_nb_functions,\n",
    "    encoder_enable_byte_ngrams=encoder_enable_byte_ngrams,\n",
    "    cross_attn_encoder=cross_attn_encoder,\n",
    "    cross_attn_decoder=cross_attn_decoder,\n",
    "    cross_attn_window_encoder=cross_attn_window_encoder,\n",
    "    cross_attn_window_decoder=cross_attn_window_decoder,\n",
    "    dim_local_encoder=dim_local_encoder,\n",
    "    dim_local_decoder=dim_local_decoder,\n",
    "    cross_attn_k=cross_attn_k,   \n",
    "    cross_attn_nheads=cross_attn_nheads,  \n",
    "    cross_attn_all_layers_decoder=cross_attn_all_layers_decoder,\n",
    "    cross_attn_all_layers_encoder=cross_attn_all_layers_encoder,\n",
    "    cross_attn_use_flex_attention=cross_attn_use_flex_attention,\n",
    "    cross_attn_init_by_pooling=cross_attn_init_by_pooling,\n",
    "    log_patch_lengths=log_patch_lengths,\n",
    "    non_linearity=non_linearity,\n",
    "    use_rope=use_rope,\n",
    "    recompute_fc1_out=recompute_fc1_out,\n",
    "    recompute_fc3_out=recompute_fc3_out,\n",
    "    recompute_attn=recompute_attn,\n",
    "    custom_bwd=custom_bwd,\n",
    "    layer_ckpt=layer_ckpt,\n",
    "    use_local_encoder_transformer=use_local_encoder_transformer,\n",
    "    init_use_gaussian=init_use_gaussian,\n",
    "    init_use_depth=init_use_depth,\n",
    "    attn_impl=attn_impl,\n",
    "    attn_bias_type=attn_bias_type,\n",
    "    alpha_depth=alpha_depth,\n",
    "    max_length=max_length,\n",
    "    local_attention_window_len=local_attention_window_len,\n",
    "    max_seqlen=max_seqlen,\n",
    "    downsampling_by_pooling=downsampling_by_pooling,\n",
    ")\n",
    "\n",
    "model = ByteLatentTransformer(model_args)\n",
    "model = model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# n of params in model in millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in model: {count_parameters(model) / 1e6:.2f}M\")\n",
    "\n",
    "\n",
    "# 336 > 42, 8\n",
    "# 96 > 12, 8\n",
    "# 512 > 64, 8\n",
    "\n",
    "l = torch.full((batch_size,64), 8).to('cuda')\n",
    "l[:,0] = 1\n",
    "l[:,-1] = 10\n",
    "l[:,2] = 10\n",
    "l[:,1] = 11\n",
    "patch_lengths = l\n",
    "patch_lengths.shape, patch_lengths[0].sum().item()\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=5e-4, \n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.95)  # Use better beta values from first code\n",
    "    )\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "torch.manual_seed(model_args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(model_args.seed)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "print(f\"Using precision: {dtype}\")\n",
    "\n",
    "\n",
    "# Create a new config with prediction_length=1\n",
    "train_tokenizer_config = ChronosConfig(\n",
    "    tokenizer_class='MeanScaleUniformBins',\n",
    "    tokenizer_kwargs={'low_limit': -15.0, 'high_limit': 15.0},\n",
    "    context_length=seq_len,\n",
    "    prediction_length=seq_len,   \n",
    "    n_tokens=vocab_size,\n",
    "    n_special_tokens=4,\n",
    "    pad_token_id=-1,\n",
    "    eos_token_id=0,\n",
    "    use_eos_token=False,\n",
    "    model_type='causal',\n",
    "    num_samples=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Create a new tokenizer with the updated config\n",
    "tokenizer = MeanScaleUniformBins(-15, 15, train_tokenizer_config)\n",
    "\n",
    "config = {\n",
    "    'task_name': 'pretrain_long_term_forecast',\n",
    "    'dataset': 'ETTm1',\n",
    "    'data' : 'ETTm1',\n",
    "    'embed' : 'timeF',\n",
    "    'root_path': 'dataset/ETT-small/',\n",
    "    'data_path': 'ETTm1.csv',\n",
    "    'features': 'S',\n",
    "    'seq_len': seq_len,\n",
    "    'label_len': seq_len - 1,\n",
    "    'pred_len': 1\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size=batch_size,\n",
    "    freq='m',\n",
    "    num_workers=2,\n",
    "    subsample_pct=None,\n",
    "    fix_seed=42,\n",
    "    target='OT',\n",
    "    shuffle=True,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "train_dataset, train_loader = data_provider(args, config, flag='train')\n",
    "validate_dataset, validate_loader = data_provider(args, config, flag='val')\n",
    "test_dataset, test_loader = data_provider(args, config, flag='test')\n",
    "\n",
    "\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "total_steps = epochs * num_batches\n",
    "min_lr = learning_rate * min_lr_factor\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Training\", position=0, leave=True)\n",
    "    \n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in progress_bar:\n",
    "        iteration = epoch * num_batches + i\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = get_lr(\n",
    "            iteration,\n",
    "            total_steps,\n",
    "            warmup_steps,\n",
    "            learning_rate,\n",
    "            min_lr,\n",
    "            decay_lr\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for micro_step in range(grad_accumulation_steps):\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "            # with ctx:\n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to(device), patch_lengths)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                target_token_ids.reshape(-1).to(device),\n",
    "                ignore_index=PAD_ID\n",
    "            )\n",
    "            \n",
    "            # Scale loss\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "                \n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "        # Update weights with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update training metrics\n",
    "        epoch_loss += total_loss\n",
    "        avg_epoch_loss = epoch_loss / (i + 1)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{total_loss:.4f}\",\n",
    "            'avg_loss': f\"{avg_epoch_loss:.4f}\",\n",
    "            'lr': f\"{lr:.6f}\"\n",
    "        })\n",
    "    \n",
    "    # Calculate training time and average loss\n",
    "    train_time = time.time() - t1\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    t1 = time.time()\n",
    "    model.eval()\n",
    "    val_loss, val_rmse = validate(model, validate_loader, tokenizer, patch_lengths, device, desc=f\"Epoch {epoch+1} Validation\")\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"Training Loss: {train_avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, RMSE: {val_rmse:.4f} (Time: {val_time:.2f}s)\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss,\n",
    "            'val_rmse': val_rmse,\n",
    "        }, os.path.join(output_dir, f'best_model_{seq_len}.pth'))\n",
    "        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if save_every > 0 and (epoch + 1) % save_every == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss,\n",
    "            'val_rmse': val_rmse,\n",
    "        }, os.path.join(output_dir, f'checkpoint_{seq_len}_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate the result tensor\n",
    "result = torch.zeros(32, 192)\n",
    "\n",
    "# In your loop (192 iterations)\n",
    "for i in range(192):\n",
    "    # Your code that produces tensor of shape [32, 1]\n",
    "    current_tensor = next_token  # shape [32, 1]\n",
    "    \n",
    "    # Fill the i-th column\n",
    "    result[:, i] = current_tensor.squeeze(-1)  # or current_tensor[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e678140",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape, current_tensor.shape, actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((B, max_new_tokens), dtype=torch.long).to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example: Create sample tensors (replace with your actual data)\n",
    "# Assuming your tensors are of shape (1, 96)\n",
    "actual_tensor = torch.randn(1, 96).cumsum(dim=1)  # Sample actual data\n",
    "forecast_tensor = actual_tensor + torch.randn(1, 96) * 0.3  # Sample forecast with some noise\n",
    "\n",
    "# Convert tensors to numpy for plotting\n",
    "actual_data = actual.cpu().squeeze().numpy()  # Remove the batch dimension\n",
    "forecast_data = forecast.cpu().squeeze().numpy()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_data, label='Actual', linewidth=2, color='blue')\n",
    "plt.plot(forecast_data, label='Forecast', linewidth=2, color='red', linestyle='--')\n",
    "\n",
    "plt.title('Actual vs Forecast Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optional: Add some statistics\n",
    "mae = np.mean(np.abs(actual_data - forecast_data))\n",
    "rmse = np.sqrt(np.mean((actual_data - forecast_data)**2))\n",
    "plt.text(0.02, 0.98, f'MAE: {mae:.3f}\\nRMSE: {rmse:.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If you want to use your actual tensors, replace the sample data creation with:\n",
    "# actual_data = your_actual_tensor.squeeze().numpy()\n",
    "# forecast_data = your_forecast_tensor.squeeze().nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "temperature = 1.0\n",
    "top_k = 50\n",
    "max_patch_size = 8  # Maximum tokens per patch\n",
    "max_new_tokens = 96\n",
    "pred_len = 96  # Your desired prediction length\n",
    "\n",
    "# Initialize lists to store metrics across batches\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "    if i < 300:  # Only process first 300 batches\n",
    "        # Move tensors to device\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Get input and target sequences\n",
    "        x = batch_x.float().squeeze(-1) if batch_x.dim() > 2 else batch_x.float()\n",
    "        y = batch_y.float().squeeze(-1) if batch_y.dim() > 2 else batch_y.float()\n",
    "        \n",
    "        # Get context length and label length from your config\n",
    "        context_len = config['seq_len']  # 512\n",
    "        label_len = config['label_len']  # 416\n",
    "        \n",
    "        # Tokenize input\n",
    "        token_ids_start, attention_mask, tokenizer_state = tokenizer.context_input_transform(x.to('cpu'))\n",
    "        target_token_ids, target_attention_mask = tokenizer.label_input_transform(y.to('cpu'), tokenizer_state.to('cpu'))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize token ids with context\n",
    "            token_ids = token_ids_start.to(device)\n",
    "            \n",
    "            # Patch lengths should be properly initialized from your tokenizer\n",
    "            # If patch_lengths isn't defined, you might need to extract it from tokenizer_state\n",
    "            # patch_lengths = tokenizer.get_patch_lengths(token_ids).to(device)\n",
    "            \n",
    "            # Generate tokens autoregressively\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Get predictions for the next token\n",
    "                logits = model(token_ids, patch_lengths)\n",
    "                next_token_logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                \n",
    "                # Apply top-k filtering\n",
    "                if top_k > 0:\n",
    "                    v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                    next_token_logits[next_token_logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "                # Apply softmax and sample\n",
    "                probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append new token and update sequence\n",
    "                token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "                \n",
    "                # If using a sliding context window, keep only the last 'context_len' tokens\n",
    "                if token_ids.size(1) > context_len:\n",
    "                    token_ids = token_ids[:, -context_len:]\n",
    "                \n",
    "                # Update patch_lengths for the new token\n",
    "                # patch_lengths = tokenizer.get_patch_lengths(token_ids).to(device)\n",
    "            \n",
    "            # Extract only the prediction part (last pred_len tokens)\n",
    "            forecast_tokens = token_ids[:, -pred_len:]\n",
    "            \n",
    "            # For target, use only the prediction portion from target_token_ids\n",
    "            # This is likely the last pred_len tokens after the label_len overlap\n",
    "            target_tokens = target_token_ids[:, -pred_len:].to(device)\n",
    "            \n",
    "            # Convert tokens back to values\n",
    "            forecast_values = tokenizer.output_transform(forecast_tokens.to('cpu'), tokenizer_state.to('cpu'))\n",
    "            target_values = tokenizer.output_transform(target_tokens.to('cpu'), tokenizer_state.to('cpu'))\n",
    "            \n",
    "            # Ensure same dimensions\n",
    "            min_len = min(target_values.shape[1], forecast_values.shape[1])\n",
    "            target_values = target_values[:, :min_len]\n",
    "            forecast_values = forecast_values[:, :min_len]\n",
    "            \n",
    "            # Calculate MSE and MAE on the actual values\n",
    "            mse = torch.mean((target_values - forecast_values) ** 2)\n",
    "            mae = torch.mean(torch.abs(target_values - forecast_values))\n",
    "            \n",
    "            # Store metrics\n",
    "            all_mse.append(mse.item())\n",
    "            all_mae.append(mae.item())\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Batch {i} - MSE: {mse.item():.4f}, MAE: {mae.item():.4f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_mse = sum(all_mse) / len(all_mse)\n",
    "avg_mae = sum(all_mae) / len(all_mae)\n",
    "print(f\"\\nOverall Results:\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "import json\n",
    "with open(\"forecast_metrics.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"mse_per_batch\": all_mse,\n",
    "        \"mae_per_batch\": all_mae,\n",
    "        \"average_mse\": avg_mse,\n",
    "        \"average_mae\": avg_mae\n",
    "    }, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blt_250513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
