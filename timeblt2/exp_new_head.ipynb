{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1febc3",
   "metadata": {},
   "source": [
    "## Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c795ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # must be BEFORE torch/TF import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from bytelatent.model.blt import ByteLatentTransformerArgs, ByteLatentTransformer\n",
    "from utils.train_utils import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from bytelatent.tokenizers.constants import PAD_ID\n",
    "from utils.eval_utils import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)   # 0 here means \"the first visible GPU\", i.e. physical #3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f33e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Args\n",
    "vocab_size = 4096\n",
    "quant_range = 15\n",
    "batch_size = 256\n",
    "seq_len = 96\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-2\n",
    "epochs = 500  # Increased for early stopping\n",
    "grad_accumulation_steps = 1\n",
    "clip_grad = 1.0\n",
    "seed = 42\n",
    "warmup_steps = 0\n",
    "min_lr_factor = 0.1\n",
    "decay_lr = True\n",
    "compile = True\n",
    "output_dir = \"output\"\n",
    "save_every = 10\n",
    "# eval_every = 100  # Evaluate every 5 epochs\n",
    "patience = 6   # Early stopping patience\n",
    "compile = True\n",
    "dataset_name = 'ETTm1'\n",
    "features = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb36a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features, \n",
    "    seq_len=seq_len, \n",
    "    label_len=0, \n",
    "    pred_len=96, \n",
    "    flag='train', \n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    "    )\n",
    "\n",
    "validate_dataset, validate_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features, \n",
    "    seq_len=seq_len, \n",
    "    label_len=0, \n",
    "    pred_len=96, \n",
    "    flag='val', \n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    "    )\n",
    "\n",
    "test_dataset, test_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features, \n",
    "    seq_len=seq_len, \n",
    "    label_len=0, \n",
    "    pred_len=96, \n",
    "    flag='test', \n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    "    )\n",
    "\n",
    "print(f\"Dataset: {dataset_name}, Features: {features}, Batch Size: {batch_size}, Seq Len: {seq_len}\")\n",
    "\n",
    "# Initialize components\n",
    "tokenizer = build_tokenizer(\n",
    "    quant_range=quant_range,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=seq_len,\n",
    "    prediction_length=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af17dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set model args\n",
    "model_args = ByteLatentTransformerArgs(\n",
    "    seed=42,\n",
    "    vocab_size=vocab_size,                       # Small byte-level vocab\n",
    "    max_length=seq_len,                        # Max full sequence length\n",
    "    max_seqlen=seq_len,\n",
    "    max_encoder_seq_length=seq_len,\n",
    "    local_attention_window_len=seq_len,        # Local window, 128 is sufficient for small models\n",
    "\n",
    "    dim_global=32,                        # Lower than default 512\n",
    "    dim_local_encoder=16,\n",
    "    dim_local_decoder=16,\n",
    "\n",
    "    n_layers_global=4,\n",
    "    n_layers_local_encoder=4,\n",
    "    n_layers_local_decoder=4,\n",
    "\n",
    "    n_heads_global=4,                      # Reduce heads\n",
    "    n_heads_local_encoder=2,\n",
    "    n_heads_local_decoder=2,\n",
    "\n",
    "    patch_size=8,\n",
    "    patch_in_forward=False,                # Patch in forward pass\n",
    "    patching_batch_size=256,\n",
    "    patching_device=\"cuda\",               # Use CPU for patching in small model\n",
    "    patching_mode=\"entropy\",\n",
    "    patching_threshold=3.0,\n",
    "    max_patch_length=16,\n",
    "    monotonicity=True,            # Monotonic patching\n",
    "    pad_to_max_length=True,\n",
    "\n",
    "    cross_attn_encoder=True,\n",
    "    cross_attn_decoder=True,\n",
    "    cross_attn_k=2,\n",
    "    cross_attn_nheads=2,\n",
    "    cross_attn_all_layers_encoder=True,\n",
    "    cross_attn_all_layers_decoder=True,\n",
    "    cross_attn_use_flex_attention=False,\n",
    "    cross_attn_init_by_pooling=True,\n",
    "\n",
    "    encoder_hash_byte_group_size=[6,7,8],   # Fewer hash sizes\n",
    "    encoder_hash_byte_group_vocab=32,\n",
    "    encoder_hash_byte_group_nb_functions=1,\n",
    "    encoder_enable_byte_ngrams=False,\n",
    "\n",
    "    non_linearity=\"swiglu\",\n",
    "    use_rope=True,\n",
    "    attn_impl=\"sdpa\",                      # Efficient PyTorch attention\n",
    "    attn_bias_type=\"causal\",\n",
    "\n",
    "    dropout=0.0,\n",
    "    layer_ckpt=\"none\",                     # No checkpointing in small model\n",
    "    init_use_gaussian=True,\n",
    "    init_use_depth=\"current\",\n",
    "    alpha_depth=\"disabled\",\n",
    "    log_patch_lengths=True,\n",
    "\n",
    "    downsampling_by_pooling=\"max\",         # Efficient downsampling\n",
    "    use_local_encoder_transformer=True,\n",
    "    share_encoder_decoder_emb=True         # Save memory if possible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ByteLatentTransformer(model_args)\n",
    "model = model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# n of params in model in millions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model_param_count = count_parameters(model)\n",
    "print(f\"Number of parameters in model: {model_param_count / 1e6:.2f}M\")\n",
    "\n",
    "patch_lengths = create_static_patch_lengths(batch_size=batch_size, seq_len=seq_len) #torch.full((batch_size, 8), 12).to('cuda')\n",
    "#create_static_patch_lengths(batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=5e-4, \n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.95)  # Use better beta values from first code\n",
    ")\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "torch.manual_seed(model_args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(model_args.seed)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "print(f\"Using precision: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55758c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training function with early stopping, periodic evaluation, and WandB logging\n",
    "\"\"\"\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=1e-6)\n",
    "logger = TrainingLogger(output_dir, dataset_name, enable_wandb=ENABLE_WANDB)\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "total_steps = epochs * num_batches\n",
    "min_lr = learning_rate * min_lr_factor\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüöÄ Starting training with early stopping...\")\n",
    "print(f\"üìù Configuration:\")\n",
    "print(f\"   Max epochs: {epochs}\")\n",
    "print(f\"   Early stopping patience: {patience}\")\n",
    "print(f\"   Save every: {save_every} epochs\")\n",
    "print(f\"   WandB logging: {'Enabled' if ENABLE_WANDB else 'Disabled'}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    t1 = time.time()\n",
    "    epoch_loss = 0\n",
    "    current_lr = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader), \n",
    "        desc=f\"üèÉ Epoch {epoch+1}/{epochs}\", \n",
    "        position=0, \n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for i, (batch_x, batch_y, _, _) in progress_bar:\n",
    "        iteration = epoch * num_batches + i\n",
    "        x = batch_x.float().squeeze(-1)\n",
    "        y = batch_y.float().squeeze(-1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        lr = get_lr(iteration, total_steps, warmup_steps, learning_rate, min_lr, decay_lr)\n",
    "        current_lr = lr\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for micro_step in range(grad_accumulation_steps):\n",
    "            token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "            target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(token_ids.to(device), patch_lengths)\n",
    "            # MSE Loss\n",
    "            loss = F.mse_loss(logits, y.to(device), reduction='mean')\n",
    "\n",
    "            # # Calculate loss\n",
    "            # loss = F.cross_entropy(\n",
    "            #     logits.reshape(-1, logits.size(-1)),\n",
    "            #     target_token_ids.reshape(-1).to(device),\n",
    "            #     ignore_index=PAD_ID\n",
    "            # )\n",
    "            loss = loss / grad_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item() * grad_accumulation_steps\n",
    "\n",
    "        # Gradient clipping\n",
    "        if clip_grad > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "            # Log gradient norm to wandb periodically\n",
    "            if ENABLE_WANDB and i % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'train/grad_norm': grad_norm,\n",
    "                    'train/step': iteration,\n",
    "                    'train/batch_loss': total_loss\n",
    "                })\n",
    "        else:\n",
    "            grad_norm = 0\n",
    "            \n",
    "        # Update weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += total_loss\n",
    "        batch_losses.append(total_loss)\n",
    "        avg_epoch_loss = epoch_loss / (i + 1)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{total_loss:.4f}\",\n",
    "            'avg_loss': f\"{avg_epoch_loss:.4f}\",\n",
    "            'lr': f\"{lr:.6f}\",\n",
    "            'patience': f\"{early_stopping.counter}/{patience}\"\n",
    "        })\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_time = time.time() - t1\n",
    "    train_avg_loss = epoch_loss / len(train_loader)\n",
    "    train_std_loss = np.std(batch_losses) if len(batch_losses) > 1 else 0\n",
    "    \n",
    "    # Validation phase\n",
    "    print(f\"\\nüîç Running validation for epoch {epoch+1}...\")\n",
    "    t1 = time.time()\n",
    "    model.eval()\n",
    "    val_loss = validate(model, validate_loader, tokenizer, patch_lengths, device, \n",
    "                        desc=f\"Epoch {epoch+1} Validation\")\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"   Training Loss: {train_avg_loss:.6f} ¬± {train_std_loss:.6f} (Time: {train_time:.2f}s)\")\n",
    "    print(f\"   Validation Loss: {val_loss:.6f} (Time: {val_time:.2f}s)\")\n",
    "    print(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_avg_loss,\n",
    "            'model_args': model_args.__dict__\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(output_dir, f'best_model_{dataset_name}_{features}_{seq_len}.pth'))\n",
    "        print(f\"   ‚úÖ New best model saved! (Val Loss: {best_val_loss:.6f})\")\n",
    "        \n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if save_every > 0 and (epoch + 1) % save_every == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f'checkpoint_{seq_len}_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_avg_loss,\n",
    "            'model_args': model_args.__dict__\n",
    "        }, checkpoint_path)\n",
    "        print(f\"   üíæ Checkpoint saved at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs!\")\n",
    "        print(f\"   No improvement for {patience} consecutive epochs\")\n",
    "        print(f\"   Best validation loss: {early_stopping.best_loss:.6f}\")\n",
    "        \n",
    "        if ENABLE_WANDB:\n",
    "            wandb.run.summary['early_stopped'] = True\n",
    "            wandb.run.summary['early_stop_epoch'] = epoch + 1\n",
    "            wandb.run.summary['early_stop_patience'] = patience\n",
    "        break\n",
    "\n",
    "# Training completed\n",
    "print(f\"\\nüéâ Training completed!\")\n",
    "\n",
    "# Print summary\n",
    "# logger.print_summary()\n",
    "\n",
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scaler': scaler.state_dict() if scaler else None,\n",
    "    'val_loss': val_loss,\n",
    "    'train_loss': train_avg_loss,\n",
    "    'early_stopped': early_stopping.counter >= patience,\n",
    "    'model_args': model_args.__dict__,\n",
    "    'final_metrics': {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'total_epochs': epoch + 1,\n",
    "        'total_training_time': time.time()\n",
    "    }\n",
    "}\n",
    "# torch.save(final_checkpoint, os.path.join(output_dir, f'final_model_{dataset_name}_{features}_{seq_len}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5602b43",
   "metadata": {},
   "source": [
    "0.334 with all embeddings in the head with small dimensions.\n",
    "0.339 all 8 len patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195de9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids.shape, y.shape, target_token_ids.shape, target_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate best model\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "checkpoint = torch.load(os.path.join(output_dir, f\"best_model_{dataset_name}_{features}_{seq_len}.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Saved Val loss: {checkpoint['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, test_loader = build_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    features=features,\n",
    "    seq_len=seq_len,\n",
    "    label_len=0,\n",
    "    pred_len=96,\n",
    "    flag='test',\n",
    "    batch_size=batch_size,\n",
    "    pretrain=True\n",
    ")\n",
    "\n",
    "# Initialize components\n",
    "tokenizer = build_tokenizer(\n",
    "    quant_range=quant_range,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=seq_len,\n",
    "    prediction_length=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mse = 0.0\n",
    "model.eval()\n",
    "for i, (batch_x, batch_y, _, _) in enumerate(test_loader):\n",
    "    x = batch_x.float().squeeze(-1)\n",
    "    y = batch_y.float().squeeze(-1)\n",
    "\n",
    "    token_ids, attention_mask, tokenizer_state = tokenizer.context_input_transform(x)\n",
    "    # target_token_ids, target_attention_mask = tokenizer.label_input_transform(y, tokenizer_state)\n",
    "\n",
    "    logits = model(token_ids.to(device), patch_lengths)\n",
    "    loss = F.mse_loss(logits, y.to(device), reduction='mean')\n",
    "    all_mse += loss.item()\n",
    "\n",
    "    # print(f\"Test Loss: {loss.item():.6f}\")\n",
    "    \n",
    "# Final evaluation\n",
    "final_mse = all_mse / len(test_loader)\n",
    "print(f\"\\nFinal Test MSE Loss: {final_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periodic evaluation\n",
    "eval_results = None\n",
    "\n",
    "print(f\"\\nüéØ Running full evaluation at ...\")\n",
    "try:\n",
    "    eval_results = evaluation(\n",
    "        model, \n",
    "        dataset_name, \n",
    "        features,\n",
    "        quant_range,\n",
    "        vocab_size,\n",
    "        input_len=96,\n",
    "        pred_len=96,\n",
    "        eval_batch_size=batch_size,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"   üìà Evaluation completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Evaluation failed: {e}\")\n",
    "    eval_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff8d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blt_250513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
